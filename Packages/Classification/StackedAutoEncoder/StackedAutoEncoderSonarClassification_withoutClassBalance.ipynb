{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "## Laboratorio de Processamento de Sinais - UFRJ\n",
    "\n",
    "#### Autor: Vinícius dos Santos Mello (viniciudsmello@poli.ufrj.br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/vinicius.mello/.virtualenvs/sonarenv/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 6.50882720947e-05 seconds\n",
      "Time to read data file: 1.27910494804 seconds\n",
      "Qtd event of 0 is 12939\n",
      "Qtd event of 1 is 29352\n",
      "Qtd event of 2 is 11510\n",
      "Qtd event of 3 is 23760\n",
      "\n",
      "Biggest class is 1 with 29352 events\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Functions import TrainParameters as trnparams\n",
    "from Functions import TrainFunctions\n",
    "\n",
    "import multiprocessing \n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "analysis_name = 'StackedAutoEncoder'\n",
    "data_path = os.getenv('OUTPUTDATAPATH')\n",
    "results_path = os.getenv('PACKAGE_NAME')\n",
    "\n",
    "base_results_path = '%s/%s'%(results_path,analysis_name)\n",
    "pict_results_path = '%s/pictures_files'%(base_results_path)\n",
    "files_results_path = '%s/output_files'%(base_results_path)\n",
    "\n",
    "# For multiprocessing purpose\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "database = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "spectrum_bins_left = 400\n",
    "development_flag = False\n",
    "development_events = 400\n",
    "\n",
    "if not os.path.exists('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                      (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left)):\n",
    "    print 'No Files in %s/%s\\n'%(data_path,database)\n",
    "else:\n",
    "    #Read lofar data\n",
    "    [data,trgt,class_labels] = joblib.load('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                                           (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left))\n",
    "\n",
    "\n",
    "    m_time = time.time()-m_time\n",
    "    print 'Time to read data file: '+str(m_time)+' seconds'\n",
    "\n",
    "    # correct format\n",
    "    all_data = data\n",
    "    all_trgt = trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))\n",
    "    \n",
    "    # Process data\n",
    "    # unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "    # Same number of events in each class\n",
    "    qtd_events_biggest_class = 0\n",
    "    biggest_class_label = ''\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "            qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "            biggest_class_label = class_label\n",
    "        print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "    print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "#     balanced_data = {}\n",
    "#     balanced_trgt = {}\n",
    "\n",
    "#     from Functions import DataHandler as dh\n",
    "#     m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "#     for iclass, class_label in enumerate(class_labels):\n",
    "#         if development_flag:\n",
    "#             class_events = all_data[all_trgt==iclass,:]\n",
    "#             if len(balanced_data) == 0:\n",
    "#                 balanced_data = class_events[0:development_events,:]\n",
    "#                 balanced_trgt = (iclass)*np.ones(development_events)\n",
    "#             else:\n",
    "#                 balanced_data = np.append(balanced_data,\n",
    "#                                           class_events[0:development_events,:], \n",
    "#                                           axis=0)\n",
    "#                 balanced_trgt = np.append(balanced_trgt,(iclass)*np.ones(development_events))\n",
    "#         else:\n",
    "#             if len(balanced_data) == 0:\n",
    "#                 class_events = all_data[all_trgt==iclass,:]\n",
    "#                 balanced_data = m_datahandler.CreateEventsForClass(\n",
    "#                     class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "#                 balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "#             else:\n",
    "#                 class_events = all_data[all_trgt==iclass,:]\n",
    "#                 created_events = (m_datahandler.CreateEventsForClass(all_data[all_trgt==iclass,:],\n",
    "#                                                                      qtd_events_biggest_class-\n",
    "#                                                                      (len(class_events))))\n",
    "#                 balanced_data = np.append(balanced_data,created_events,axis=0)\n",
    "#                 balanced_trgt = np.append(balanced_trgt,\n",
    "#                                           (iclass)*np.ones(created_events.shape[0]),axis=0)\n",
    "        \n",
    "#     all_data = balanced_data\n",
    "#     all_trgt = balanced_trgt\n",
    "\n",
    "#     # turn targets in sparse mode\n",
    "#     from keras.utils import np_utils\n",
    "#     trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77561, 400)\n",
      "(77561, 400)\n"
     ]
    }
   ],
   "source": [
    "print data.shape\n",
    "print all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_inits_mapstd_norm_500_epochs_128_batch_size_tanh_hidden_activation_linear_output_activation\n",
      "CPU times: user 12.8 ms, sys: 0 ns, total: 12.8 ms\n",
      "Wall time: 15.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load parameters\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "os.remove(trn_params_folder)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='linear',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=128,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "n_folds = 2\n",
    "CVO = trnparams.ClassificationFolds(folder=results_path,\n",
    "                                    n_folds=n_folds,\n",
    "                                    trgt=all_trgt,\n",
    "                                    dev=development_flag, verbose=False)\n",
    "print trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento com processamento paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To train on multiple cores sweeping the number of folds\n",
    "def trainFold(ifold):\n",
    "    ineuron = 20\n",
    "    return TrainFunctions.StackedAutoEncoderTrainFunction(data=all_data,\n",
    "                            trgt=all_data,\n",
    "                            ifold=ifold,\n",
    "                            n_folds=n_folds, \n",
    "                            n_neurons=ineuron,\n",
    "                            trn_params=trn_params, \n",
    "                            save_path=results_path,\n",
    "                            dev=development_flag)\n",
    "# To train on multiple cores sweeping the number of neurons\n",
    "def trainNeuron(ineuron):\n",
    "    for ifold in range(len(CVO)):\n",
    "        TrainFunctions.StackedAutoEncoderTrainFunction(data=all_data,\n",
    "                            trgt=all_data,\n",
    "                            ifold=ifold,\n",
    "                            n_folds=n_folds, \n",
    "                            n_neurons=ineuron,\n",
    "                            trn_params=trn_params, \n",
    "                            save_path=results_path,\n",
    "                            dev=development_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 5.01 µs\n",
      "Neuron: 1 - Fold 1 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 350 - Fold 1 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 200 - Fold 1 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 100 - Fold 1 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 300 - Fold 1 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 1 - Fold 2 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 400 - Fold 1 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 400 - Fold 2 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 300 - Fold 2 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 350 - Fold 2 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 200 - Fold 2 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 100 - Fold 2 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 450 - Fold 1 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 450 - Fold 2 of 2 Folds -  Init 1 of 1 Inits\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "p = multiprocessing.Pool(processes=num_processes+1)\n",
    "start_time = time.time()\n",
    "folds = range(len(CVO))\n",
    "neurons = [0,100,200] + [300,350,400,450]\n",
    "neurons.sort() # Train from 0 to 750\n",
    "# To train on multiple cores sweeping the number of folds\n",
    "#results = p.map(trainFold, folds)\n",
    "\n",
    "# To train on multiple cores sweeping the number of neurons\n",
    "results = p.map(trainNeuron, neurons)\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if not dev:\n",
    "    file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    if os.path.exists(file_name):\n",
    "        if trn_params.params['verbose']:\n",
    "            print 'File %s exists'%(file_name)\n",
    "        return 0\n",
    "else:\n",
    "    file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    if os.path.exists(file_name):\n",
    "        if trn_params.params['verbose']:\n",
    "            print 'File %s exists'%(file_name)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9112233056e-05\n"
     ]
    }
   ],
   "source": [
    "print end_time/3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análises com variação de Neurônios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x1_neurons_fold_0_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x100_neurons_fold_0_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x200_neurons_fold_0_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x250_neurons_fold_0_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x300_neurons_fold_0_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x350_neurons_fold_0_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x400_neurons_fold_0_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x450_neurons_fold_0_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x1_neurons_fold_1_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x100_neurons_fold_1_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x200_neurons_fold_1_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x250_neurons_fold_1_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x300_neurons_fold_1_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x350_neurons_fold_1_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x400_neurons_fold_1_model.h5\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/StackedAutoEncoder/RawData_2_folds_1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation_400x450_neurons_fold_1_model.h5\n",
      "Topology (1_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAJpCAYAAAAQWG6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x52f0f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Neuron variation x MSE\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "current_analysis = 'mse'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name, current_analysis)\n",
    "\n",
    "#os.remove(analysis_file_name)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "\n",
    "    if not os.path.exists(trn_params_folder):\n",
    "        trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                             hidden_activation='relu',\n",
    "                                                             output_activation='linear',\n",
    "                                                             n_epochs=500,\n",
    "                                                             patience=30,\n",
    "                                                             batch_size=256,\n",
    "                                                             verbose=False)\n",
    "        trn_params.save(trn_params_folder)\n",
    "    else:\n",
    "        trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "        trn_params.load(trn_params_folder)\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "    n_folds = 2\n",
    "    CVO = trnparams.ClassificationFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\n",
    "\n",
    "    neurons_mat = neurons\n",
    "    neurons_mat.sort()\n",
    "    \n",
    "    mse_mat = np.zeros([n_folds,len(neurons_mat)])\n",
    "\n",
    "    for ifold in range(len(CVO)):\n",
    "        train_id, test_id = CVO[ifold]\n",
    "\n",
    "        # normalize data based in train set\n",
    "        if trn_params.params['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "        norm_data = scaler.transform(all_data[test_id,:])\n",
    "\n",
    "        for ineuron,neuron_value in enumerate(neurons_mat):     \n",
    "            if neuron_value == 0:\n",
    "                neuron_value = 1\n",
    "            model_str = '%s/%s/%s_%i_folds_%s_400x%i_neurons'%(save_path,analysis_str,\n",
    "                                                       model_prefix_str,\n",
    "                                                       n_folds,\n",
    "                                                       params_str,\n",
    "                                                       neuron_value)\n",
    "\n",
    "            if not development_flag:        \n",
    "                file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "            else:\n",
    "                file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "            print file_name\n",
    "            if not os.path.exists(file_name):\n",
    "                def trainFold(ifold):\n",
    "                    ineuron = neuron_value\n",
    "                    return TrainFunctions.StackedAutoEncoderTrainFunction(data=all_data,\n",
    "                                            trgt=all_data,\n",
    "                                            ifold=ifold,\n",
    "                                            n_folds=n_folds, \n",
    "                                            n_neurons=ineuron,\n",
    "                                            trn_params=trn_params, \n",
    "                                            save_path=results_path,\n",
    "                                            dev=development_flag)\n",
    "                \n",
    "                p = multiprocessing.Pool(processes=num_processes)\n",
    "                folds = range(len(CVO))\n",
    "                results = p.map(trainFold, folds)\n",
    "                p.close()\n",
    "                p.join()\n",
    "                \n",
    "            model = load_model(file_name)\n",
    "            output = model.predict(norm_data)\n",
    "            mse = metrics.mean_squared_error(norm_data, output)\n",
    "            mse_mat[ifold,ineuron] = mse\n",
    "            \n",
    "    joblib.dump([mse_mat,neurons_mat],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [mse_mat,neurons_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10),nrows=1, ncols=1)\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "ax.errorbar(neurons_mat,np.mean(mse_mat,axis=0),\n",
    "            np.std(mse_mat,axis=0),fmt='o-',\n",
    "            color='k',alpha=0.7,linewidth=2.5)\n",
    "\n",
    "ax.set_title('MSE per Hidden Layer size',fontsize=18,weight='bold')\n",
    "ax.set_xlabel('Neurons Quantity',fontsize=18,weight='bold')\n",
    "ax.set_ylabel('MSE Value',fontsize=18,weight='bold') \n",
    "ax.grid()\n",
    "#Save the figure\n",
    "file_name = pict_results_path+'/'+current_analysis+'_first_layer_'+trn_params.get_params_str()+'.pdf'\n",
    "plt.savefig(file_name)\n",
    "print \"Topology (%s)\"%trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis of Reconstruction Input x Output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "current_analysis = 'reconstruction_inputxoutput'\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "\n",
    "# Number of neurons at first layer\n",
    "ineuron = 400\n",
    "\n",
    "models = {}\n",
    "outputs = {}\n",
    "mean = {}\n",
    "indexes = {}\n",
    "\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data[test_id,:])\n",
    "    if ifold == 0:\n",
    "        diffSquared = np.zeros([len(CVO),norm_data.shape[0],norm_data.shape[1]])\n",
    "    # Get the model file\n",
    "    model_str = '%s/%s/%s_%i_folds_%s_400x%i_neurons'%(save_path,analysis_str,\n",
    "                                               model_prefix_str,\n",
    "                                               n_folds,\n",
    "                                               params_str,\n",
    "                                               ineuron)\n",
    "\n",
    "    if not development_flag:        \n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        def trainFold(ifold):\n",
    "            return TrainFunctions.StackedAutoEncoderTrainFunction(data=all_data,\n",
    "                                    trgt=all_data,\n",
    "                                    ifold=ifold,\n",
    "                                    n_folds=n_folds, \n",
    "                                    n_neurons=ineuron,\n",
    "                                    trn_params=trn_params, \n",
    "                                    save_path=results_path,\n",
    "                                    dev=development_flag)\n",
    "        \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    models[ifold]  = load_model(file_name)\n",
    "    outputs[ifold] = models[ifold].predict(norm_data) \n",
    "\n",
    "    diffSquared[ifold] = np.power((norm_data - outputs[ifold]), 2) \n",
    "\n",
    "mean = np.mean(np.mean(diffSquared, axis=0), axis=0)\n",
    "indexes = np.argsort(mean)[::-1]\n",
    "\n",
    "for ifold in range(len(CVO)):\n",
    "    points = norm_data.shape[0]\n",
    "    # Number of dimensions to analyse (even number is better!)\n",
    "    num_dim = 4\n",
    "    fig, m_ax = plt.subplots(figsize=(20,20),nrows=2, ncols=2)\n",
    "    for choose_index in range(num_dim):\n",
    "        ax = plt.subplot(2,2,choose_index+1)\n",
    "        ax.plot(norm_data[:,indexes[choose_index]][:points], outputs[ifold][:,indexes[choose_index]][:points],\"b.\")\n",
    "        plt.tight_layout()\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "        ax.set_title('Input x Output - Dim %i'%(indexes[choose_index]),fontsize=18, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid()    \n",
    "        #Save the figure\n",
    "        file_name = pict_results_path+'/'+current_analysis+'_first_layer_%i_neurons_%i_fold_'%(ineuron,ifold)+trn_params.get_params_str()+'.pdf'\n",
    "        plt.savefig(file_name)\n",
    "print \"Topology (%s)\"%trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise LOFAR para reconstrução da 1ª Camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data)\n",
    "\n",
    "    ineuron = 400\n",
    "\n",
    "    # Get the model file\n",
    "    model_str = '%s/%s/%s_%i_folds_%s_400x%i_neurons'%(save_path,analysis_str,\n",
    "                                               model_prefix_str,\n",
    "                                               n_folds,\n",
    "                                               params_str,\n",
    "                                               ineuron)\n",
    "\n",
    "    if not development_flag:        \n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    model = load_model(file_name)\n",
    "    all_output = model.predict(norm_data)\n",
    "\n",
    "    m_fontsize = 15\n",
    "\n",
    "    fig, subplot_array = plt.subplots(nrows=2, ncols=2,figsize=(20,10))\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        ax = plt.subplot(2,2,iclass+1)\n",
    "        plt.title('Lofar Analysis for Class %s'%(class_label), fontsize= m_fontsize, fontweight=\"bold\")\n",
    "\n",
    "        if iclass > 1:\n",
    "            plt.xlabel('Frequencies (Hz)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        if ((iclass == 0) or (iclass==2)):\n",
    "            plt.ylabel('Time (seconds)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "\n",
    "        plt.imshow(all_output[all_trgt==iclass,:],\n",
    "               cmap=\"jet\",extent=[1, 400, all_output[all_trgt==iclass,:].shape[0],1],\n",
    "               aspect=\"auto\")\n",
    "        plt.xticks(np.linspace(0,400,9),rotation=45)\n",
    "        cbar = plt.colorbar()\n",
    "        plt.clim(-2,9)\n",
    "        #if ((iclass == 1) or (iclass==3)):\n",
    "        cbar.ax.set_ylabel('dB',fontweight='bold') \n",
    "        #Save the figure\n",
    "        file_name = pict_results_path+'/'+current_analysis+'_%i'%ifold+'_fold'+'_%i'%ineuron+'_neurons_'+trn_params.get_params_str()+'.pdf'\n",
    "        plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOFARGram for reconstructed input with lines\n",
    "current_analysis = 'LOFARGram_reconstruction_first_layer'\n",
    "\n",
    "# Choose num of lines to plot\n",
    "points = 20\n",
    "\n",
    "# Choose num of neurons\n",
    "ineuron = 400\n",
    "\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data)\n",
    "\n",
    "    # Get the model file\n",
    "    model_str = '%s/%s/%s_%i_folds_%s_400x%i_neurons'%(save_path,analysis_str,\n",
    "                                               model_prefix_str,\n",
    "                                               n_folds,\n",
    "                                               params_str,\n",
    "                                               ineuron)\n",
    "\n",
    "    if not development_flag:        \n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    model = load_model(file_name)\n",
    "    all_output = model.predict(norm_data)\n",
    "\n",
    "    m_fontsize = 15\n",
    "\n",
    "    fig, subplot_array = plt.subplots(nrows=2, ncols=2,figsize=(20,10))\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        ax = plt.subplot(2,2,iclass+1)\n",
    "        plt.title('Lofar Analysis for Reconstruction of Class %s'%(class_label), fontsize= m_fontsize, fontweight=\"bold\")\n",
    "\n",
    "        if iclass > 1:\n",
    "            plt.xlabel('Frequencies (Hz)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        if ((iclass == 0) or (iclass==2)):\n",
    "            plt.ylabel('Time (seconds)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "\n",
    "        # Plot lines at frequencies with high errors\n",
    "        for index in indexes[:points]:\n",
    "            plt.axvline(index, color='r', alpha=0.7)\n",
    "\n",
    "        plt.imshow(all_output[all_trgt==iclass,:],\n",
    "               cmap=\"jet\",extent=[1, 400, all_output[all_trgt==iclass,:].shape[0],1],\n",
    "               aspect=\"auto\")\n",
    "        plt.xticks(np.linspace(0,400,9),rotation=45)\n",
    "        cbar = plt.colorbar()\n",
    "        plt.clim(-2,9)\n",
    "        #if ((iclass == 1) or (iclass==3)):\n",
    "        cbar.ax.set_ylabel('dB',fontweight='bold') \n",
    "\n",
    "    #Save the figure\n",
    "    file_name = pict_results_path+'/'+current_analysis+'_%i'%points+'_points'+'_%i'%ifold+'_fold'+'_%i'%ineuron+'_neurons_'+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "print \"Topology (%s)\"%trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOFARGram for original input\n",
    "fig, subplot_array = plt.subplots(nrows=2, ncols=2,figsize=(20,10))\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    ax = plt.subplot(2,2,iclass+1)\n",
    "    plt.title('Lofar Analysis for Class %s'%(class_label), fontsize= m_fontsize, fontweight=\"bold\")\n",
    "\n",
    "    if iclass > 1:\n",
    "        plt.xlabel('Frequencies (Hz)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "    if ((iclass == 0) or (iclass==2)):\n",
    "        plt.ylabel('Time (seconds)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "\n",
    "    plt.imshow(norm_data[all_trgt==iclass,:],\n",
    "           cmap=\"jet\",extent=[1, 400, norm_data[all_trgt==iclass,:].shape[0],1],\n",
    "           aspect=\"auto\")\n",
    "    plt.xticks(np.linspace(0,400,9),rotation=45)\n",
    "    cbar = plt.colorbar()\n",
    "    plt.clim(-2,9)\n",
    "    #if ((iclass == 1) or (iclass==3)):\n",
    "    cbar.ax.set_ylabel('dB',fontweight='bold') \n",
    "#Save the figure\n",
    "file_name = pict_results_path+'/LOFARGram_original.pdf'\n",
    "plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Classificação para a 1º Camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# Load Parameters\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "#os.remove(trn_params_folder)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='linear', # For AutoEncoder\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "def SAEClassificationtrainFold(ifold):\n",
    "    return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                          trgt=all_trgt,\n",
    "                                          ifold=ifold,\n",
    "                                          n_folds=n_folds, \n",
    "                                          hidden_neurons=[400],\n",
    "                                          trn_params=trn_params, \n",
    "                                          save_path=results_path,\n",
    "                                          dev=development_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "p = multiprocessing.Pool(processes=num_processes)\n",
    "start_time = time.time()\n",
    "\n",
    "folds = range(len(CVO))\n",
    "# To train on multiple cores sweeping the number of folds\n",
    "results = p.map(SAEClassificationtrainFold, folds)\n",
    "\n",
    "# To train on multiple cores sweeping the number of neurons\n",
    "#results = p.map(trainNeuron, neurons)\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mean Squared Error x Epochs\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_analysis = 'mse'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "prefix_str = 'RawData'\n",
    "save_path = results_path\n",
    "\n",
    "hidden_neurons = [400]\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='linear',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "# plot train updates\n",
    "for ifold in range(len(CVO)):\n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "    plt.rc('font', weight='bold')\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(10,6))\n",
    "    \n",
    "    neurons_str = '400'\n",
    "    for ineuron in hidden_neurons:\n",
    "        neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "    neurons_str = neurons_str + 'x' + str(trgt_sparse.shape[1])\n",
    "    # load train history\n",
    "    topology = 'Classification_(%s)_%s_%i_folds_%s'%(neurons_str, prefix_str, n_folds, params_str)\n",
    "    model_str = '%s/%s/%s'%(save_path,analysis_str,topology)\n",
    "\n",
    "    if not development_flag:        \n",
    "        load_file_name = '%s_fold_%i_trn_desc.jbl'%(model_str,ifold)\n",
    "    else:\n",
    "        load_file_name = '%s_fold_%i_trn_desc_dev.jbl'%(model_str,ifold)\n",
    "        print load_file_name\n",
    "    if not os.path.exists(load_file_name):\n",
    "        def SAEClassificationTrainFold(ifold):\n",
    "            return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                                  trgt=all_trgt,\n",
    "                                                  ifold=ifold,\n",
    "                                                  n_folds=n_folds, \n",
    "                                                  hidden_neurons=hidden_neurons,\n",
    "                                                  trn_params=trn_params, \n",
    "                                                  save_path=results_path,\n",
    "                                                  dev=development_flag)\n",
    "           \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    [trn_desc] = joblib.load(load_file_name)\n",
    "\n",
    "    l1 = plt.plot(trn_desc['epochs'],\n",
    "                  trn_desc['loss'],color=[0,0,1],\n",
    "                  linewidth=2.5,linestyle='solid',label='Train Perf.')\n",
    "    l2 = plt.plot(trn_desc['epochs'],\n",
    "                  trn_desc['val_loss'],color=[1,0,0],\n",
    "                  linewidth=2.5,linestyle='dashed',label='Test Perf.')\n",
    "    cost = ''\n",
    "    cost = 'MSE'\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"# Epochs\",fontsize=18,fontweight='bold')\n",
    "    plt.ylabel(cost,fontsize=18,fontweight='bold')\n",
    "    plt.title(cost+\" vs #Epochs\",fontsize=18,fontweight='bold')\n",
    "    plt.legend()\n",
    "    #Save the figure\n",
    "    file_name = pict_results_path+'/Classification_(%s)_'%neurons_str\\\n",
    "        +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "print 'Topolgy (%s)'%topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "%matplotlib inline  \n",
    "from sklearn import preprocessing\n",
    "\n",
    "current_analysis = 'confusion_matrix'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "prefix_str = 'RawData'\n",
    "save_path = results_path\n",
    "\n",
    "hidden_neurons = [400]\n",
    "analysis_full_data = False\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='tanh',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "    plt.rc('font', weight='bold')\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    if analysis_full_data:\n",
    "        norm_data = scaler.transform(all_data)\n",
    "        norm_trgt = all_trgt\n",
    "    else: \n",
    "        norm_data = scaler.transform(all_data[test_id, :])\n",
    "        norm_trgt = all_trgt[test_id]\n",
    "        \n",
    "    neurons_str = '400'\n",
    "    for ineuron in hidden_neurons:\n",
    "        neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "    neurons_str = neurons_str + 'x' + str(trgt_sparse.shape[1])\n",
    "    # load train history\n",
    "    topology = 'Classification_(%s)_%s_%i_folds_%s'%(neurons_str, prefix_str, n_folds, params_str)\n",
    "    model_str = '%s/%s/%s'%(save_path,analysis_str,topology)\n",
    "\n",
    "    if not development_flag:        \n",
    "        load_file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        load_file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "        print load_file_name\n",
    "    if not os.path.exists(load_file_name):\n",
    "        def SAEClassificationTrainFold(ifold):\n",
    "            return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                                  trgt=all_trgt,\n",
    "                                                  ifold=ifold,\n",
    "                                                  n_folds=n_folds, \n",
    "                                                  hidden_neurons=hidden_neurons,\n",
    "                                                  trn_params=trn_params, \n",
    "                                                  save_path=results_path,\n",
    "                                                  dev=development_flag)\n",
    "           \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(load_file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "    \n",
    "    all_output = np.argmax(output,axis=1)\n",
    "    cm = confusion_matrix(norm_trgt, all_output)\n",
    "    cm_normalized = 100.*cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    im =ax.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Greys,clim=(0.0, 100.0))\n",
    "\n",
    "    width, height = cm_normalized.shape\n",
    "\n",
    "    for x in xrange(width):\n",
    "        for y in xrange(height):\n",
    "            if cm_normalized[x][y] < 50.:\n",
    "                ax.annotate('%1.3f%%'%(cm_normalized[x][y]), xy=(y, x),\n",
    "                            horizontalalignment='center',\n",
    "                            verticalalignment='center')\n",
    "            else:\n",
    "                ax.annotate('%1.3f%%'%(cm_normalized[x][y]), xy=(y, x),\n",
    "                            horizontalalignment='center',\n",
    "                            verticalalignment='center',color='white')\n",
    "\n",
    "    ax.set_title('Confusion Matrix',fontweight='bold',fontsize=15)\n",
    "    fig.colorbar(im)\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    ax.xaxis.set_ticks(tick_marks)\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "\n",
    "    ax.yaxis.set_ticks(tick_marks)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "\n",
    "    ax.set_ylabel('True Label',fontweight='bold',fontsize=15)\n",
    "    ax.set_xlabel('Predicted Label',fontweight='bold',fontsize=15)\n",
    "    \n",
    "    #Save the figure\n",
    "    if analysis_full_data:\n",
    "        file_name = pict_results_path+'/Classification_(%s)_all_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    else: \n",
    "        file_name = pict_results_path+'/Classification_(%s)_test_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "print 'Topology (%s)'%topology\n",
    "if analysis_full_data:\n",
    "    print 'Analysis in All Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "%matplotlib inline  \n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_analysis = 'histogram'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "prefix_str = 'RawData'\n",
    "save_path = results_path\n",
    "\n",
    "# Choose Topology\n",
    "hidden_neurons = [400]\n",
    "analysis_full_data = True\n",
    "neurons_str = str(all_data.shape[1])\n",
    "for ineuron in hidden_neurons:\n",
    "    neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "neurons_str = neurons_str + 'x' + str(trgt_sparse.shape[1])\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='tanh',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "    plt.rc('font', weight='bold')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20),nrows=4, ncols=4)\n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    if analysis_full_data:\n",
    "        norm_data = scaler.transform(all_data)\n",
    "        norm_trgt = all_trgt\n",
    "    else: \n",
    "        norm_data = scaler.transform(all_data[test_id, :])\n",
    "        norm_trgt = all_trgt[test_id]\n",
    "    \n",
    "    # load train history\n",
    "    topology = 'Classification_(%s)_%s_%i_folds_%s'%(neurons_str, prefix_str, n_folds, params_str)\n",
    "    model_str = '%s/%s/%s'%(save_path,analysis_str,topology)\n",
    "\n",
    "    if not development_flag:        \n",
    "        load_file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        load_file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    # Check if the topology has already been trained    \n",
    "    if not os.path.exists(load_file_name):\n",
    "        def SAEClassificationTrainFold(ifold):\n",
    "            return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                                  trgt=all_trgt,\n",
    "                                                  ifold=ifold,\n",
    "                                                  n_folds=n_folds, \n",
    "                                                  hidden_neurons=hidden_neurons,\n",
    "                                                  trn_params=trn_params, \n",
    "                                                  save_path=results_path,\n",
    "                                                  dev=development_flag)\n",
    "           \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(load_file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "   \n",
    "    from sklearn.neighbors import KernelDensity\n",
    "\n",
    "    m_bins = np.linspace(-0.5, 1.5, 50)\n",
    "\n",
    "    m_colors = ['b', 'r', 'g', 'y']\n",
    "    \n",
    "    for i_target in range(trgt_sparse[test_id].shape[1]):\n",
    "        for i_output in range(output.shape[1]):\n",
    "            subplot_id = output.shape[1]*i_target+i_output\n",
    "            m_pts = output[norm_trgt==i_target,i_output]\n",
    "\n",
    "            n, bins, patches = ax[i_target,i_output].hist(m_pts,bins=m_bins,\n",
    "                                                          fc=m_colors[i_target],\n",
    "                                                          alpha=0.8, normed=0)\n",
    "\n",
    "            if i_output == 0:\n",
    "                ax[i_target,i_output].set_ylabel('Target %s'%(class_labels[i_target]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            if i_target == 3:\n",
    "                ax[i_target,i_output].set_xlabel('Output %s'%(class_labels[i_output]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            ax[i_target,i_output].grid()\n",
    "    \n",
    "    #Save the figure\n",
    "    if analysis_full_data:\n",
    "        file_name = pict_results_path+'/Classification_(%s)_all_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    else: \n",
    "        file_name = pict_results_path+'/Classification_(%s)_test_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "print 'Topology (%s)'%topology\n",
    "if analysis_full_data:\n",
    "    print 'Analysis in All Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Kernel Density\n",
    "\n",
    "%matplotlib inline  \n",
    "from sklearn import preprocessing\n",
    "\n",
    "current_analysis = 'kernel_density'\n",
    "\n",
    "hidden_neurons = [400]\n",
    "analysis_full_data = False\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='linear',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "    plt.rc('font', weight='bold')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20),nrows=4, ncols=4)\n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    if analysis_full_data:\n",
    "        norm_data = scaler.transform(all_data)\n",
    "        norm_trgt = all_trgt\n",
    "    else: \n",
    "        norm_data = scaler.transform(all_data[test_id, :])\n",
    "        norm_trgt = all_trgt[test_id]\n",
    "    \n",
    "    neurons_str = '400'\n",
    "    for ineuron in hidden_neurons:\n",
    "        neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "    neurons_str = neurons_str + 'x' + str(trgt_sparse.shape[1])\n",
    "    # load train history\n",
    "    topology = 'Classification_(%s)_%s_%i_folds_%s'%(neurons_str, prefix_str, n_folds, params_str)\n",
    "    model_str = '%s/%s/%s'%(save_path,analysis_str,topology)\n",
    "\n",
    "    if not development_flag:        \n",
    "        load_file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        load_file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "        print load_file_name \n",
    "    # Check if the topology has already been trained    \n",
    "    if not os.path.exists(load_file_name):\n",
    "        def SAEClassificationTrainFold(ifold):\n",
    "            return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                                  trgt=all_trgt,\n",
    "                                                  ifold=ifold,\n",
    "                                                  n_folds=n_folds, \n",
    "                                                  hidden_neurons=hidden_neurons,\n",
    "                                                  trn_params=trn_params, \n",
    "                                                  save_path=results_path,\n",
    "                                                  dev=development_flag)\n",
    "           \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(load_file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "   \n",
    "    from sklearn.neighbors import KernelDensity\n",
    "\n",
    "    m_bins = np.linspace(-0.5, 1.5,100)\n",
    "\n",
    "    m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "    kernel = 'gaussian' # other kernels: 'gaussian', 'tophat', \n",
    "                        #'epanechnikov', 'exponential', 'linear', 'cosine'\n",
    "    for i_target in range(trgt_sparse[test_id].shape[1]):\n",
    "        for i_output in range(output.shape[1]):\n",
    "            subplot_id = output.shape[1]*i_target+i_output\n",
    "            m_pts = output[norm_trgt==i_target,i_output]\n",
    "\n",
    "            kde = KernelDensity(kernel=kernel,algorithm='auto',\n",
    "                                bandwidth=0.5).fit(m_pts[:, np.newaxis])\n",
    "            log_dens_x = kde.score_samples(m_bins[:, np.newaxis])\n",
    "            ax[i_target,i_output].plot(m_bins, np.exp(log_dens_x),\n",
    "                                       color=m_colors[i_target],linewidth=2.0)\n",
    "            if i_output == 0:\n",
    "                ax[i_target,i_output].set_ylabel('Target %s'%(class_labels[i_target]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            if i_target == 3:\n",
    "                ax[i_target,i_output].set_xlabel('Output %s'%(class_labels[i_output]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            ax[i_target,i_output].grid()\n",
    "    \n",
    "    #Save the figure\n",
    "    if analysis_full_data:\n",
    "        file_name = pict_results_path+'/Classification_(%s)_all_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    else: \n",
    "        file_name = pict_results_path+'/Classification_(%s)_test_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "print 'Topology (%s)'%topology\n",
    "if analysis_full_data:\n",
    "    print 'Analysis in All Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "\n",
    "# Choose Topology\n",
    "hidden_neurons = [250]\n",
    "analysis_full_data = True\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='tanh',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=50,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "neurons_str = '400'\n",
    "for ineuron in hidden_neurons:\n",
    "    neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "neurons_str = neurons_str + 'x' + str(trgt_sparse.shape[1])\n",
    "\n",
    "print \"Results for (%s) neurons (%s)\"%(neurons_str, params_str)\n",
    "\n",
    "if analysis_full_data:\n",
    "    print 'Analysis in All Data'\n",
    "    \n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    \n",
    "    if analysis_full_data:\n",
    "        norm_data = scaler.transform(all_data)\n",
    "        norm_trgt = all_trgt\n",
    "        norm_trgt_sparse = trgt_sparse\n",
    "    else: \n",
    "        norm_data = scaler.transform(all_data[test_id, :])\n",
    "        norm_trgt = all_trgt[test_id]\n",
    "        norm_trgt_sparse = trgt_sparse[test_id]\n",
    "    \n",
    "    # load train history\n",
    "    topology = 'Classification_(%s)_%s_%i_folds_%s'%(neurons_str, prefix_str, n_folds, params_str)\n",
    "    model_str = '%s/%s/%s'%(save_path,analysis_str,topology)\n",
    "\n",
    "    if not development_flag:        \n",
    "        load_file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        load_file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "        print load_file_name\n",
    "    # Check if the topology has already been trained    \n",
    "    if not os.path.exists(load_file_name):\n",
    "        def SAEClassificationTrainFold(ifold):\n",
    "            return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                                  trgt=all_trgt,\n",
    "                                                  ifold=ifold,\n",
    "                                                  n_folds=n_folds, \n",
    "                                                  hidden_neurons=hidden_neurons,\n",
    "                                                  trn_params=trn_params, \n",
    "                                                  save_path=results_path,\n",
    "                                                  dev=development_flag)\n",
    "           \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(load_file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "   \n",
    "    num_classes = len(class_labels.keys())\n",
    "    \n",
    "    efficiency = sklearn.metrics.recall_score(norm_trgt_sparse, np.round(output), average=None)\n",
    "    sp_index = np.sum(efficiency)/num_classes * np.power(np.prod(efficiency), 1/num_classes)\n",
    "    sp_index = np.sqrt(sp_index)\n",
    "    \n",
    "    precision = sklearn.metrics.precision_score(norm_trgt_sparse, np.round(output), average=None)\n",
    "    f1_score = sklearn.metrics.f1_score(norm_trgt_sparse, np.round(output), average=None)\n",
    "    print '\\tPrecision\\tEfficiency\\tF1_Score'\n",
    "    for iclass in range(num_classes):\n",
    "        print '%s:\\t%f\\t%f\\t%f\\n'%(class_labels[iclass], precision[iclass], efficiency[iclass],f1_score[iclass])\n",
    "    print 'SP index: %f\\n'%sp_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento da 2ª Camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hidden_neurons = [400]\n",
    "\n",
    "# To train on multiple cores sweeping the number of folds\n",
    "def trainFold(ifold):\n",
    "    ineuron = 20\n",
    "    return TrainFunctions.StackedAutoEncoderTrainFunction(data=all_data,\n",
    "                                trgt=all_data,\n",
    "                                ifold=ifold,\n",
    "                                n_folds=n_folds, \n",
    "                                n_neurons=ineuron,\n",
    "                                trn_params=trn_params, \n",
    "                                save_path=results_path,\n",
    "                                dev=development_flag,\n",
    "                                layer = 2,\n",
    "                                hidden_neurons = hidden_neurons)\n",
    "    \n",
    "# To train on multiple cores sweeping the number of neurons\n",
    "def trainNeuron(ineuron):\n",
    "    for ifold in range(len(CVO)):\n",
    "        TrainFunctions.StackedAutoEncoderTrainFunction(data=all_data,\n",
    "                            trgt=all_data,\n",
    "                            ifold=ifold,\n",
    "                            n_folds=n_folds, \n",
    "                            n_neurons=ineuron,\n",
    "                            trn_params=trn_params, \n",
    "                            save_path=results_path,\n",
    "                            dev=development_flag,\n",
    "                            layer = 2,\n",
    "                            hidden_neurons = hidden_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "p = multiprocessing.Pool(processes=num_processes)\n",
    "start_time = time.time()\n",
    "folds = range(len(CVO))\n",
    "neurons = range(0, 450, 75) # Train from 0 to 300\n",
    "# To train on multiple cores sweeping the number of folds\n",
    "#results = p.map(trainFold, folds)\n",
    "\n",
    "# To train on multiple cores sweeping the number of neurons\n",
    "results = p.map(trainNeuron, neurons)\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Análise da variação do número de neurônios na segunda camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron variation x MSE\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "current_analysis = 'mse'\n",
    "\n",
    "hidden_neurons = [400]\n",
    "neurons_str = str(all_data.shape[1])\n",
    "for ineuron in hidden_neurons:\n",
    "    neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "analysis_file_name='%s/%s/%s_%s_second_layer_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name, current_analysis)\n",
    "\n",
    "#os.remove(analysis_file_name)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "\n",
    "    if not os.path.exists(trn_params_folder):\n",
    "        trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                             hidden_activation='tanh',\n",
    "                                                             output_activation='linear',\n",
    "                                                             n_epochs=500,\n",
    "                                                             patience=30,\n",
    "                                                             batch_size=256,\n",
    "                                                             verbose=False)\n",
    "        trn_params.save(trn_params_folder)\n",
    "    else:\n",
    "        trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "        trn_params.load(trn_params_folder)\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "    n_folds = 2\n",
    "    CVO = trnparams.ClassificationFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\n",
    "\n",
    "    neurons_mat = neurons\n",
    "    neurons_mat.sort()\n",
    "    \n",
    "    mse_mat = np.zeros([n_folds,len(neurons_mat)])\n",
    "\n",
    "    for ifold in range(len(CVO)):\n",
    "        train_id, test_id = CVO[ifold]\n",
    "\n",
    "        # normalize data based in train set\n",
    "        if trn_params.params['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "        norm_data = scaler.transform(all_data[test_id,:])\n",
    "        \n",
    "        for ineuron,neuron_value in enumerate(neurons_mat):     \n",
    "            if neuron_value == 0:\n",
    "                neuron_value = 1\n",
    "            model_str = '%s/%s/%s_%i_folds_%s_%sx%i_neurons'%(save_path,analysis_str,\n",
    "                                                       model_prefix_str,\n",
    "                                                       n_folds,\n",
    "                                                       params_str,neurons_str,\n",
    "                                                       neuron_value)\n",
    "\n",
    "            if not development_flag:        \n",
    "                file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "            else:\n",
    "                file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "            print file_name\n",
    "            if not os.path.exists(file_name):\n",
    "                def trainFold(ifold):\n",
    "                    ineuron = neuron_value\n",
    "                    return TrainFunctions.StackedAutoEncoderTrainFunction(data=all_data,\n",
    "                                            trgt=all_data,\n",
    "                                            ifold=ifold,\n",
    "                                            n_folds=n_folds, \n",
    "                                            n_neurons=ineuron,\n",
    "                                            trn_params=trn_params, \n",
    "                                            save_path=results_path,\n",
    "                                            dev=development_flag,\n",
    "                                            layer=2,\n",
    "                                            hidden_neurons = hidden_neurons)\n",
    "                \n",
    "                p = multiprocessing.Pool(processes=num_processes)\n",
    "                folds = range(len(CVO))\n",
    "                results = p.map(trainFold, folds)\n",
    "                p.close()\n",
    "                p.join()\n",
    "                \n",
    "            model = load_model(file_name)\n",
    "            output = model.predict(norm_data)\n",
    "            mse = metrics.mean_squared_error(norm_data, output)\n",
    "            mse_mat[ifold,ineuron] = mse\n",
    "            \n",
    "    joblib.dump([mse_mat,neurons_mat],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [mse_mat,neurons_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10),nrows=1, ncols=1)\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "ax.errorbar(neurons_mat,np.mean(mse_mat,axis=0),\n",
    "            np.std(mse_mat,axis=0),fmt='o-',\n",
    "            color='k',alpha=0.7,linewidth=2.5)\n",
    "\n",
    "ax.set_title('MSE per Hidden Layer size',fontsize=18,weight='bold')\n",
    "ax.set_xlabel('Neurons Quantity',fontsize=18,weight='bold')\n",
    "ax.set_ylabel('MSE Value',fontsize=18,weight='bold') \n",
    "ax.grid()\n",
    "#Save the figure\n",
    "file_name = pict_results_path+'/'+current_analysis+'_second_layer_'+trn_params.get_params_str()+'.pdf'\n",
    "plt.savefig(file_name)\n",
    "print \"Topology (%s)\"%trn_params.get_params_str()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOFARGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# LOFARGram for reconstructed input\n",
    "current_analysis = 'LOFARGram_reconstruction_second_layer'\n",
    "\n",
    "# Choose num of neurons\n",
    "hidden_neurons = [400, 200]\n",
    "\n",
    "# Create a string like InputDimension x FirstLayerDimension x ... x OutputDimension\n",
    "neurons_str = '400'\n",
    "for ineuron in hidden_neurons:\n",
    "    neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "neurons_str = neurons_str + 'x' + str(all_data.shape[1])\n",
    "\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data)\n",
    "    \n",
    "    # First Layer\n",
    "    previous_model_str = '%s/%s/%s_%i_folds_%s_%i_neurons'%(save_path,analysis_str,\n",
    "                                                            prefix_str,\n",
    "                                                            n_folds,\n",
    "                                                            params_str,\n",
    "                                                            hidden_neurons[0])\n",
    "\n",
    "    if not development_flag:\n",
    "        file_name = '%s_fold_%i_model.h5'%(previous_model_str, ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(previous_model_str, ifold)\n",
    "    # Get the first layer weights\n",
    "    first_layer_model = load_model(file_name)\n",
    "    encoder_first_layer = first_layer_model.layers[0].get_weights()\n",
    "    decoder_first_layer = first_layer_model.layers[2].get_weights()\n",
    "    \n",
    "    # Second Layer\n",
    "    previous_model_str = '%s/%s/%s_%i_folds_%s_%i_neurons'%(save_path,analysis_str,\n",
    "                                                            prefix_str,\n",
    "                                                            n_folds,\n",
    "                                                            params_str,\n",
    "                                                            hidden_neurons[1])\n",
    "\n",
    "    if not development_flag:\n",
    "        file_name = '%s_fold_%i_model.h5'%(previous_model_str, ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(previous_model_str, ifold)\n",
    "    # Get the second layer projection of data\n",
    "    second_layer_model = load_model(file_name)\n",
    "    encoder_second_layer = second_layer_model.layers[0].get_weights()\n",
    "    decoder_second_layer = second_layer_model.layers[2].get_weights()\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Encoder\n",
    "    model.add(Dense(hidden_neurons[0], input_dim=norm_data.shape[1], weights=encoder_first_layer, trainable=False))\n",
    "    model.add(Activation(trn_params.params['hidden_activation']))\n",
    "    model.add(Dense(hidden_neurons[1], weights=encoder_second_layer, trainable=False))\n",
    "    model.add(Activation(trn_params.params['hidden_activation']))\n",
    "    # Decoder\n",
    "    model.add(Dense(hidden_neurons[0], weights=decoder_second_layer, trainable=False))\n",
    "    model.add(Activation(trn_params.params['output_activation']))\n",
    "    model.add(Dense(norm_data.shape[1], weights=decoder_first_layer, trainable=False))\n",
    "    model.add(Activation(trn_params.params['output_activation'], trainable=False))\n",
    "    #model.summary()\n",
    "    all_output = model.predict(norm_data)\n",
    "    \n",
    "    m_fontsize = 15\n",
    "\n",
    "    fig, subplot_array = plt.subplots(nrows=2, ncols=2,figsize=(20,10))\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        ax = plt.subplot(2,2,iclass+1)\n",
    "        plt.title('Lofar Analysis for Class %s'%(class_label), fontsize= m_fontsize, fontweight=\"bold\")\n",
    "\n",
    "        if iclass > 1:\n",
    "            plt.xlabel('Frequencies (Hz)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        if ((iclass == 0) or (iclass==2)):\n",
    "            plt.ylabel('Time (seconds)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "\n",
    "        plt.imshow(all_output[all_trgt==iclass,:],\n",
    "               cmap=\"jet\",extent=[1, 400, all_output[all_trgt==iclass,:].shape[0],1],\n",
    "               aspect=\"auto\")\n",
    "        plt.xticks(np.linspace(0,400,9),rotation=45)\n",
    "        cbar = plt.colorbar()\n",
    "        plt.clim(-2,9)\n",
    "        #if ((iclass == 1) or (iclass==3)):\n",
    "        cbar.ax.set_ylabel('dB',fontweight='bold') \n",
    "        #Save the figure\n",
    "        file_name = pict_results_path+'/'+current_analysis+'_(%s)'%neurons_str+'_%i'%ifold+'_fold'+'_'+trn_params.get_params_str()+'.pdf'\n",
    "        plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Análise de Classificação para segunda camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Load Parameters\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "#os.remove(trn_params_folder)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='linear', # For AutoEncoder\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from Functions import TrainFunctions\n",
    "def SAEClassificationtrainFold(ifold):\n",
    "    return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                          trgt=all_trgt,\n",
    "                                          ifold=ifold,\n",
    "                                          n_folds=n_folds, \n",
    "                                          hidden_neurons=(400,300),\n",
    "                                          trn_params=trn_params, \n",
    "                                          save_path=results_path,\n",
    "                                          dev=development_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "p = multiprocessing.Pool(processes=num_processes)\n",
    "start_time = time.time()\n",
    "\n",
    "folds = range(len(CVO))\n",
    "# To train on multiple cores sweeping the number of folds\n",
    "results = p.map(SAEClassificationtrainFold, folds)\n",
    "\n",
    "# To train on multiple cores sweeping the number of neurons\n",
    "#results = p.map(trainNeuron, neurons)\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Classificação com a segunda camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error x Epochs\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_analysis = 'mse'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "prefix_str = 'RawData'\n",
    "save_path = results_path\n",
    "\n",
    "hidden_neurons = [400,200]\n",
    "\n",
    "# Parameters\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "prefix_str = 'RawData'\n",
    "save_path = results_path\n",
    "\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='linear',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "# plot train updates\n",
    "for ifold in range(len(CVO)):\n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "    plt.rc('font', weight='bold')\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(10,6))\n",
    "    \n",
    "    neurons_str = '400'\n",
    "    for ineuron in hidden_neurons:\n",
    "        neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "    neurons_str = neurons_str + 'x' + str(trgt_sparse.shape[1])\n",
    "    # load train history\n",
    "    model_str = '%s/%s/Classification_(%s)_%s_%i_folds_%s'%(save_path,analysis_str,\n",
    "                                                            neurons_str,\n",
    "                                                            prefix_str, n_folds,\n",
    "                                                            params_str)\n",
    "\n",
    "    if not development_flag:        \n",
    "        load_file_name = '%s_fold_%i_trn_desc.jbl'%(model_str,ifold)\n",
    "    else:\n",
    "        load_file_name = '%s_fold_%i_trn_desc_dev.jbl'%(model_str,ifold)\n",
    "        print load_file_name\n",
    "    if not os.path.exists(load_file_name):\n",
    "        def SAEClassificationTrainFold(ifold):\n",
    "            return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                                  trgt=all_trgt,\n",
    "                                                  ifold=ifold,\n",
    "                                                  n_folds=n_folds, \n",
    "                                                  hidden_neurons=hidden_neurons,\n",
    "                                                  trn_params=trn_params, \n",
    "                                                  save_path=results_path,\n",
    "                                                  dev=development_flag)\n",
    "           \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    [trn_desc] = joblib.load(load_file_name)\n",
    "\n",
    "    l1 = plt.plot(trn_desc['epochs'],\n",
    "                  trn_desc['loss'],color=[0,0,1],\n",
    "                  linewidth=2.5,linestyle='solid',label='Train Perf.')\n",
    "    l2 = plt.plot(trn_desc['epochs'],\n",
    "                  trn_desc['val_loss'],color=[1,0,0],\n",
    "                  linewidth=2.5,linestyle='dashed',label='Test Perf.')\n",
    "    cost = ''\n",
    "    cost = 'MSE'\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"# Epochs\",fontsize=18,fontweight='bold')\n",
    "    plt.ylabel(cost,fontsize=18,fontweight='bold')\n",
    "    plt.title(cost+\" vs #Epochs\",fontsize=18,fontweight='bold')\n",
    "    plt.legend()\n",
    "    #Save the figure\n",
    "    file_name = pict_results_path+'/Classification_(%s)_'%neurons_str\\\n",
    "        +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "%matplotlib inline  \n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_analysis = 'confusion_matrix'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "prefix_str = 'RawData'\n",
    "save_path = results_path\n",
    "\n",
    "\n",
    "hidden_neurons = [400, 200]\n",
    "analysis_full_data = True\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='linear', # for autoencoders\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "    plt.rc('font', weight='bold')\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "    \n",
    "\n",
    "    if analysis_full_data:\n",
    "        norm_data = scaler.transform(all_data)\n",
    "        norm_trgt = all_trgt\n",
    "    else: \n",
    "        norm_data = scaler.transform(all_data[test_id, :])\n",
    "        norm_trgt = all_trgt[test_id]\n",
    "    \n",
    "    neurons_str = str(all_data.shape[1])\n",
    "    for ineuron in hidden_neurons:\n",
    "        neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "    neurons_str = neurons_str + 'x' + str(trgt_sparse.shape[1])\n",
    "    # load train history\n",
    "    topology = 'Classification_(%s)_%s_%i_folds_%s'%(neurons_str, prefix_str, n_folds, params_str)\n",
    "    model_str = '%s/%s/%s'%(save_path,analysis_str,topology)\n",
    "\n",
    "    if not development_flag:        \n",
    "        load_file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        load_file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "        print load_file_name\n",
    "    if not os.path.exists(load_file_name):\n",
    "        def SAEClassificationTrainFold(ifold):\n",
    "            return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                                  trgt=all_trgt,\n",
    "                                                  ifold=ifold,\n",
    "                                                  n_folds=n_folds, \n",
    "                                                  hidden_neurons=hidden_neurons,\n",
    "                                                  trn_params=trn_params, \n",
    "                                                  save_path=results_path,\n",
    "                                                  dev=development_flag)\n",
    "           \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(load_file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "    \n",
    "    all_output = np.argmax(output,axis=1)\n",
    "    cm = confusion_matrix(norm_trgt, all_output)\n",
    "    cm_normalized = 100.*cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    im =ax.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Greys,clim=(0.0, 100.0))\n",
    "\n",
    "    width, height = cm_normalized.shape\n",
    "\n",
    "    for x in xrange(width):\n",
    "        for y in xrange(height):\n",
    "            if cm_normalized[x][y] < 50.:\n",
    "                ax.annotate('%1.3f%%'%(cm_normalized[x][y]), xy=(y, x),\n",
    "                            horizontalalignment='center',\n",
    "                            verticalalignment='center')\n",
    "            else:\n",
    "                ax.annotate('%1.3f%%'%(cm_normalized[x][y]), xy=(y, x),\n",
    "                            horizontalalignment='center',\n",
    "                            verticalalignment='center',color='white')\n",
    "\n",
    "    ax.set_title('Confusion Matrix',fontweight='bold',fontsize=15)\n",
    "    fig.colorbar(im)\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    ax.xaxis.set_ticks(tick_marks)\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "\n",
    "    ax.yaxis.set_ticks(tick_marks)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "\n",
    "    ax.set_ylabel('True Label',fontweight='bold',fontsize=15)\n",
    "    ax.set_xlabel('Predicted Label',fontweight='bold',fontsize=15)\n",
    "    \n",
    "    #Save the figure\n",
    "    if analysis_full_data:\n",
    "        file_name = pict_results_path+'/Classification_(%s)_all_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    else: \n",
    "        file_name = pict_results_path+'/Classification_(%s)_test_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "print 'Topology (%s)'%topology\n",
    "if analysis_full_data:\n",
    "    print 'Analysis in All Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "%matplotlib inline  \n",
    "from sklearn import preprocessing\n",
    "\n",
    "current_analysis = 'histogram'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "prefix_str = 'RawData'\n",
    "save_path = results_path\n",
    "\n",
    "# Choose Topology\n",
    "hidden_neurons = [400,200]\n",
    "analysis_full_data = False\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='linear',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "    plt.rc('font', weight='bold')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20),nrows=4, ncols=4)\n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    if analysis_full_data:\n",
    "        norm_data = scaler.transform(all_data)\n",
    "        norm_trgt = all_trgt\n",
    "    else: \n",
    "        norm_data = scaler.transform(all_data[test_id, :])\n",
    "        norm_trgt = all_trgt[test_id]\n",
    "    \n",
    "    neurons_str = str(all_data.shape[1])\n",
    "    for ineuron in hidden_neurons:\n",
    "        neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "    neurons_str = neurons_str + 'x' + str(trgt_sparse.shape[1])\n",
    "    # load train history\n",
    "    topology = 'Classification_(%s)_%s_%i_folds_%s'%(neurons_str, prefix_str, n_folds, params_str)\n",
    "    model_str = '%s/%s/%s'%(save_path,analysis_str,topology)\n",
    "\n",
    "    if not development_flag:        \n",
    "        load_file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        load_file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    # Check if the topology has already been trained    \n",
    "    if not os.path.exists(load_file_name):\n",
    "        def SAEClassificationTrainFold(ifold):\n",
    "            return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                                  trgt=all_trgt,\n",
    "                                                  ifold=ifold,\n",
    "                                                  n_folds=n_folds, \n",
    "                                                  hidden_neurons=hidden_neurons,\n",
    "                                                  trn_params=trn_params, \n",
    "                                                  save_path=results_path,\n",
    "                                                  dev=development_flag)\n",
    "           \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(load_file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "   \n",
    "    from sklearn.neighbors import KernelDensity\n",
    "\n",
    "    m_bins = np.linspace(-0.5, 1.5, 50)\n",
    "\n",
    "    m_colors = ['b', 'r', 'g', 'y']\n",
    "    \n",
    "    for i_target in range(trgt_sparse[test_id].shape[1]):\n",
    "        for i_output in range(output.shape[1]):\n",
    "            subplot_id = output.shape[1]*i_target+i_output\n",
    "            m_pts = output[norm_trgt==i_target,i_output]\n",
    "\n",
    "            n, bins, patches = ax[i_target,i_output].hist(m_pts,bins=m_bins,\n",
    "                                                          fc=m_colors[i_target],\n",
    "                                                          alpha=0.8, normed=0)\n",
    "\n",
    "            if i_output == 0:\n",
    "                ax[i_target,i_output].set_ylabel('Target %s'%(class_labels[i_target]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            if i_target == 3:\n",
    "                ax[i_target,i_output].set_xlabel('Output %s'%(class_labels[i_output]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            ax[i_target,i_output].grid()\n",
    "    \n",
    "    #Save the figure\n",
    "    if analysis_full_data:\n",
    "        file_name = pict_results_path+'/Classification_(%s)_all_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    else: \n",
    "        file_name = pict_results_path+'/Classification_(%s)_test_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "print 'Topology (%s)'%topology\n",
    "if analysis_full_data:\n",
    "    print 'Analysis in All Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Kernel Density\n",
    "\n",
    "%matplotlib inline  \n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_analysis = 'kernel_density'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "prefix_str = 'RawData'\n",
    "save_path = results_path\n",
    "\n",
    "# Choose Topology\n",
    "hidden_neurons = [400,200]\n",
    "analysis_full_data = True\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='tanh',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=30,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "    plt.rc('font', weight='bold')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20),nrows=4, ncols=4)\n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    if analysis_full_data:\n",
    "        norm_data = scaler.transform(all_data)\n",
    "        norm_trgt = all_trgt\n",
    "    else: \n",
    "        norm_data = scaler.transform(all_data[test_id, :])\n",
    "        norm_trgt = all_trgt[test_id]\n",
    "    \n",
    "    neurons_str = str(all_data.shape[1])\n",
    "    for ineuron in hidden_neurons:\n",
    "        neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "    neurons_str = neurons_str + 'x' + str(trgt_sparse.shape[1])\n",
    "    # load train history\n",
    "    topology = 'Classification_(%s)_%s_%i_folds_%s'%(neurons_str, prefix_str, n_folds, params_str)\n",
    "    model_str = '%s/%s/%s'%(save_path,analysis_str,topology)\n",
    "\n",
    "    if not development_flag:        \n",
    "        load_file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        load_file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "        print load_file_name \n",
    "    # Check if the topology has already been trained    \n",
    "    if not os.path.exists(load_file_name):\n",
    "        def SAEClassificationTrainFold(ifold):\n",
    "            return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                                  trgt=all_trgt,\n",
    "                                                  ifold=ifold,\n",
    "                                                  n_folds=n_folds, \n",
    "                                                  hidden_neurons=hidden_neurons,\n",
    "                                                  trn_params=trn_params, \n",
    "                                                  save_path=results_path,\n",
    "                                                  dev=development_flag)\n",
    "           \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(load_file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "   \n",
    "    from sklearn.neighbors import KernelDensity\n",
    "\n",
    "    m_bins = np.linspace(-0.5, 1.5,100)\n",
    "\n",
    "    m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "    kernel = 'gaussian' # other kernels: 'gaussian', 'tophat', \n",
    "                        #'epanechnikov', 'exponential', 'linear', 'cosine'\n",
    "    for i_target in range(trgt_sparse[test_id].shape[1]):\n",
    "        for i_output in range(output.shape[1]):\n",
    "            subplot_id = output.shape[1]*i_target+i_output\n",
    "            m_pts = output[norm_trgt==i_target,i_output]\n",
    "\n",
    "            kde = KernelDensity(kernel=kernel,algorithm='auto',\n",
    "                                bandwidth=0.5).fit(m_pts[:, np.newaxis])\n",
    "            log_dens_x = kde.score_samples(m_bins[:, np.newaxis])\n",
    "            ax[i_target,i_output].plot(m_bins, np.exp(log_dens_x),\n",
    "                                       color=m_colors[i_target],linewidth=2.0)\n",
    "            if i_output == 0:\n",
    "                ax[i_target,i_output].set_ylabel('Target %s'%(class_labels[i_target]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            if i_target == 3:\n",
    "                ax[i_target,i_output].set_xlabel('Output %s'%(class_labels[i_output]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            ax[i_target,i_output].grid()\n",
    "    \n",
    "    #Save the figure\n",
    "    if analysis_full_data:\n",
    "        file_name = pict_results_path+'/Classification_(%s)_all_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    else: \n",
    "        file_name = pict_results_path+'/Classification_(%s)_test_data_'%neurons_str\\\n",
    "            +current_analysis+'_%i'%ifold+'_fold_'+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "print 'Topology (%s)'%topology\n",
    "if analysis_full_data:\n",
    "    print 'Analysis in All Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "\n",
    "current_analysis = 'report'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "prefix_str = 'RawData'\n",
    "save_path = results_path\n",
    "\n",
    "# Choose Topology\n",
    "hidden_neurons = [400,200]\n",
    "analysis_full_data = False\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='tanh',\n",
    "                                                         output_activation='tanh',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=50,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "neurons_str = str(all_data.shape[1])\n",
    "for ineuron in hidden_neurons:\n",
    "    neurons_str = neurons_str + 'x' + str(ineuron)\n",
    "neurons_str = neurons_str + 'x' + str(trgt_sparse.shape[1])\n",
    "\n",
    "print \"Results for (%s) neurons (%s)\"%(neurons_str, params_str)\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data[test_id,:])\n",
    "    \n",
    "    # load train history\n",
    "    topology = 'Classification_(%s)_%s_%i_folds_%s'%(neurons_str, prefix_str, n_folds, params_str)\n",
    "    model_str = '%s/%s/%s'%(save_path,analysis_str,topology)\n",
    "\n",
    "    if not development_flag:        \n",
    "        load_file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        load_file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "        print load_file_name\n",
    "    # Check if the topology has already been trained    \n",
    "    if not os.path.exists(load_file_name):\n",
    "        def SAEClassificationTrainFold(ifold):\n",
    "            return TrainFunctions.SAEClassificationTrainFunction(data=all_data,\n",
    "                                                  trgt=all_trgt,\n",
    "                                                  ifold=ifold,\n",
    "                                                  n_folds=n_folds, \n",
    "                                                  hidden_neurons=hidden_neurons,\n",
    "                                                  trn_params=trn_params, \n",
    "                                                  save_path=results_path,\n",
    "                                                  dev=development_flag)\n",
    "           \n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "        folds = range(len(CVO))\n",
    "        results = p.map(trainFold, folds)\n",
    "        p.close()\n",
    "        p.join()  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(load_file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "   \n",
    "    num_classes = len(class_labels.keys())\n",
    "    \n",
    "    efficiency = sklearn.metrics.recall_score(trgt_sparse[test_id], np.round(output), average=None)\n",
    "    sp_index = np.sum(efficiency)/num_classes * np.power(np.prod(efficiency), 1/num_classes)\n",
    "    sp_index = np.sqrt(sp_index)\n",
    "    \n",
    "    precision = sklearn.metrics.precision_score(trgt_sparse[test_id], np.round(output), average=None)\n",
    "    f1_score = sklearn.metrics.f1_score(trgt_sparse[test_id], np.round(output), average=None)\n",
    "    print '\\tPrecision\\tEfficiency\\tF1_Score'\n",
    "    for iclass in range(num_classes):\n",
    "        print '%s:\\t%f\\t%f\\t%f\\n'%(class_labels[iclass], precision[iclass], efficiency[iclass],f1_score[iclass])\n",
    "    print 'SP index: %f\\n'%sp_index\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Treinamento da 3ª Camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
