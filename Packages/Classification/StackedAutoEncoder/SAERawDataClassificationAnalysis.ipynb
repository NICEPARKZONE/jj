{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "## Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "### Autor: Vinícius dos Santos Mello <viniciusdsmello@poli.ufrj.br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 2.09808349609e-05 seconds\n",
      "Time to read data file: 1.02709698677 seconds\n",
      "Qtd event of 0 is 12939\n",
      "Qtd event of 1 is 29352\n",
      "Qtd event of 2 is 11510\n",
      "Qtd event of 3 is 23760\n",
      "\n",
      "Biggest class is 1 with 29352 events\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (12939, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (29352, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (11510, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (23760, 400)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Functions import TrainParameters as trnparams\n",
    "from Functions import TrainFunctions\n",
    "from Functions.StackedAutoEncoders import StackedAutoEncoders\n",
    "from Functions import FunctionsDataVisualization\n",
    "from Functions.StatisticalAnalysis import KLDiv, EstPDF\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "analysis_name = 'StackedAutoEncoder'\n",
    "\n",
    "# Enviroment variables\n",
    "data_path = os.getenv('OUTPUTDATAPATH')\n",
    "results_path = os.getenv('PACKAGE_NAME')\n",
    "\n",
    "# paths to export results\n",
    "base_results_path = '%s/%s'%(results_path,analysis_name)\n",
    "pict_results_path = '%s/pictures_files'%(base_results_path)\n",
    "files_results_path = '%s/output_files'%(base_results_path)\n",
    "\n",
    "# For multiprocessing purpose\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# Read data\n",
    "m_time = time.time()\n",
    "\n",
    "# Database caracteristics\n",
    "database = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "spectrum_bins_left = 400\n",
    "development_flag = False\n",
    "development_events = 400\n",
    "\n",
    "# Check if LofarData has created...\n",
    "if not os.path.exists('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                      (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left)):\n",
    "    print 'No Files in %s/%s\\n'%(data_path,database)\n",
    "else:\n",
    "    #Read lofar data\n",
    "    [data,trgt,class_labels] = joblib.load('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                                           (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left))\n",
    "\n",
    "\n",
    "    m_time = time.time()-m_time\n",
    "    print 'Time to read data file: '+str(m_time)+' seconds'\n",
    "\n",
    "    # correct format\n",
    "    all_data = data\n",
    "    all_trgt = trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))\n",
    "\n",
    "    # Process data\n",
    "    # unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "    # Same number of events in each class\n",
    "    qtd_events_biggest_class = 0\n",
    "    biggest_class_label = ''\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "            qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "            biggest_class_label = class_label\n",
    "        print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "    print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "    balanced_data = {}\n",
    "    balanced_trgt = {}\n",
    "\n",
    "    from Functions import DataHandler as dh\n",
    "    m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if development_flag:\n",
    "            class_events = all_data[all_trgt==iclass,:]\n",
    "            if len(balanced_data) == 0:\n",
    "                balanced_data = class_events[0:development_events,:]\n",
    "                balanced_trgt = (iclass)*np.ones(development_events)\n",
    "            else:\n",
    "                balanced_data = np.append(balanced_data,\n",
    "                                          class_events[0:development_events,:],\n",
    "                                          axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,(iclass)*np.ones(development_events))\n",
    "        else:\n",
    "            if len(balanced_data) == 0:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                balanced_data = m_datahandler.CreateEventsForClass(\n",
    "                    class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "                balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "            else:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                created_events = (m_datahandler.CreateEventsForClass(all_data[all_trgt==iclass,:],\n",
    "                                                                     qtd_events_biggest_class-\n",
    "                                                                     (len(class_events))))\n",
    "                balanced_data = np.append(balanced_data,created_events,axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,\n",
    "                                          (iclass)*np.ones(created_events.shape[0]),axis=0)\n",
    "\n",
    "    all_data = balanced_data\n",
    "    all_trgt = balanced_trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar parâmetros de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /home/vinicius.mello/Workspace/SonarAnalysis/Results/NoveltyDetection/2_folds_cross_validation.jbl exists\n",
      "beta_1: 0.9\n",
      "metrics: ['accuracy']\n",
      "beta_2: 0.999\n",
      "verbose: 2\n",
      "loss: mean_squared_error\n",
      "nesterov: True\n",
      "epsilon: 1e-08\n",
      "learning_rate: 0.001\n",
      "n_inits: 1\n",
      "batch_size: 128\n",
      "n_epochs: 500\n",
      "train_verbose: False\n",
      "patience: 30\n",
      "learning_decay: 1e-06\n",
      "momentum: 0.3\n",
      "optmizerAlgorithm: Adam\n",
      "output_activation: linear\n",
      "hidden_activation: tanh\n",
      "norm: mapstd\n"
     ]
    }
   ],
   "source": [
    "# Load train parameters\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if os.path.exists(trn_params_folder):\n",
    "    os.remove(trn_params_folder)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.SAENoveltyDetectionTrnParams(n_inits=1,\n",
    "                                                       hidden_activation='tanh', # others tanh, relu, sigmoid, linear \n",
    "                                                       output_activation='linear',\n",
    "                                                       n_epochs=500,\n",
    "                                                       patience=30,\n",
    "                                                       batch_size=128,\n",
    "                                                       verbose=2,\n",
    "                                                       optmizerAlgorithm='Adam',\n",
    "                                                       metrics=['accuracy'], #mean_squared_error\n",
    "                                                       loss='mean_squared_error') #kullback_leibler_divergence\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "    \n",
    "# Choose how many fold to be used in Cross Validation\n",
    "n_folds = 2\n",
    "CVO = trnparams.ClassificationFolds(folder=results_path,\n",
    "                                    n_folds=n_folds,\n",
    "                                    trgt=all_trgt,\n",
    "                                    dev=development_flag,\n",
    "                                    verbose=True)\n",
    "trn_params.printParams()\n",
    "#print '\\n'+trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do Stacked Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "SAE = StackedAutoEncoders(params           = trn_params,\n",
    "                          development_flag = development_flag,\n",
    "                          n_folds          = n_folds,\n",
    "                          save_path        = results_path,\n",
    "                          CVO              = CVO,\n",
    "                          noveltyDetection = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Train Process #####\n",
    "\n",
    "# Choose layer to be trained\n",
    "layer = 1\n",
    "\n",
    "n_folds = len(CVO)\n",
    "\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "print hidden_neurons\n",
    "\n",
    "regularizer = \"dropout\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# Functions defined to be used by multiprocessing.Pool()\n",
    "def trainNeuron(ineuron):\n",
    "    for ifold in range(n_folds):\n",
    "        SAE.trainLayer(data = all_data,\n",
    "                        trgt = all_trgt,\n",
    "                        ifold = ifold,\n",
    "                        hidden_neurons = hidden_neurons + [ineuron],\n",
    "                        layer = layer,\n",
    "                        regularizer = regularizer,\n",
    "                        regularizer_param = regularizer_param)\n",
    "\n",
    "def trainFold(ifold):\n",
    "    return  SAE.trainLayer(data = all_data,\n",
    "                            trgt = all_trgt,\n",
    "                            ifold = ifold,\n",
    "                            hidden_neurons = hidden_neurons,\n",
    "                            layer = layer,\n",
    "                            regularizer = regularizer,\n",
    "                            regularizer_param = regularizer_param)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if K.backend() == 'theano':\n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    ####################### SAE LAYERS ############################\n",
    "    # It is necessary to choose the layer to be trained\n",
    "\n",
    "    # To train on multiple cores sweeping the number of folds\n",
    "    folds = range(len(CVO))\n",
    "    results = p.map(trainFold, folds)\n",
    "\n",
    "    # To train multiple topologies sweeping the number of neurons\n",
    "    # neurons_mat = range(0,400,50) (start,final,step)\n",
    "    # results = p.map(trainNeuron, neurons_mat)\n",
    "\n",
    "    p.close()\n",
    "    p.join()\n",
    "else: \n",
    "    for ifold in range(len(CVO)):\n",
    "        result = trainFold(ifold)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(array([14676, 14677, 14678, ..., 88053, 88054, 88055]),\n",
       "   array([    0,     1,     2, ..., 73377, 73378, 73379])),\n",
       "  (array([    0,     1,     2, ..., 73377, 73378, 73379]),\n",
       "   array([14676, 14677, 14678, ..., 88053, 88054, 88055]))],\n",
       " 1: [(array([14676, 14677, 14678, ..., 88053, 88054, 88055]),\n",
       "   array([    0,     1,     2, ..., 73377, 73378, 73379])),\n",
       "  (array([    0,     1,     2, ..., 73377, 73378, 73379]),\n",
       "   array([14676, 14677, 14678, ..., 88053, 88054, 88055]))],\n",
       " 2: [(array([14676, 14677, 14678, ..., 88053, 88054, 88055]),\n",
       "   array([    0,     1,     2, ..., 73377, 73378, 73379])),\n",
       "  (array([    0,     1,     2, ..., 73377, 73378, 73379]),\n",
       "   array([14676, 14677, 14678, ..., 88053, 88054, 88055]))],\n",
       " 3: [(array([14676, 14677, 14678, ..., 88053, 88054, 88055]),\n",
       "   array([    0,     1,     2, ..., 73377, 73378, 73379])),\n",
       "  (array([    0,     1,     2, ..., 73377, 73378, 73379]),\n",
       "   array([14676, 14677, 14678, ..., 88053, 88054, 88055]))]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. StandardScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2bd80a464b7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# normalize classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrn_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mapstd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtrn_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mapstd_rob'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRobustScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinicius.mello/.virtualenvs/sonarenvAnnecy/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinicius.mello/.virtualenvs/sonarenvAnnecy/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \"\"\"\n\u001b[1;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[0;32m--> 612\u001b[0;31m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinicius.mello/.virtualenvs/sonarenvAnnecy/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 451\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. StandardScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "n_folds = len(CVO)\n",
    "for ifold in range(n_folds):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    # normalize classes\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "    norm_data[ifold] = scaler.transform(all_data)\n",
    "    norm_data[ifold] = norm_data[ifold][test_id, :]\n",
    "    \n",
    "    models[ifold] = SAE.getModel(data=all_data, trgt=all_trgt,\n",
    "                                 hidden_neurons=hidden_neurons,\n",
    "                                 layer=layer, ifold=ifold)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimação de PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Reconstruction Analysis\n",
    "%matplotlib inline \n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'pdf_estimation'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "# Choose layer \n",
    "layer = 1 \n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = [400] #range(400,0,-50) + [2]\n",
    "\n",
    "# Choose model\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "neurons_str = SAE.getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "models = {}\n",
    "outputs = {}\n",
    "norm_data = {}\n",
    "\n",
    "n_folds = len(CVO)\n",
    "for ifold in range(n_folds):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    # normalize classes\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "    norm_data[ifold] = scaler.transform(all_data)\n",
    "    norm_data[ifold] = norm_data[ifold][test_id, :]\n",
    "    \n",
    "    models[ifold] = SAE.getModel(data=all_data, trgt=all_trgt,\n",
    "                                 hidden_neurons=hidden_neurons,\n",
    "                                 layer=layer, ifold=ifold)\n",
    "    \n",
    "    outputs[ifold] = models[ifold].predict(norm_data[ifold])\n",
    "\n",
    "reconstructed_data = {}\n",
    "klDivergenceFreq = {}\n",
    "\n",
    "n_bins = 100\n",
    "\n",
    "def getKlDiv(ifold):\n",
    "    kl = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    for ifrequency in range(0,400):\n",
    "        data = norm_data[ifold][:,ifrequency]\n",
    "        reconstructed_data = outputs[ifold][:,ifrequency]\n",
    "        m_bins = np.linspace(data.min(), data.max(), n_bins)\n",
    "        kl[ifrequency] = KLDiv(data.reshape(-1,1), reconstructed_data.reshape(-1,1),\n",
    "                               bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                               kernel_bw=0.1, verbose=False)\n",
    "        kl[ifrequency] = kl[ifrequency][0]\n",
    "    return ifold, kl\n",
    "# Start Parallel processing\n",
    "p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "folds = range(len(CVO))\n",
    "klDivergenceFreq = p.map(getKlDiv, folds)\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "print 'Finished!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = range(0,400)\n",
    "for ifold in range(n_folds):\n",
    "    fig, m_ax = plt.subplots(figsize=(10,10),nrows=1, ncols=1)\n",
    "    m_ax.plot(freqs, klDivergenceFreq[ifold][1])\n",
    "    m_ax.set_title('KL Divergence x Frequency - Layer %i - Fold %i'%(layer, ifold), fontweight='bold', fontsize=16)\n",
    "    m_ax.set_xlabel('Frequency', fontsize=18)\n",
    "    m_ax.set_ylabel('Kullback-Leibler Divergence', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron variation x KL Divergence\n",
    "%matplotlib inline \n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'klDivergence'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Choose layer \n",
    "layer = 1 \n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_mat = [1, 100, 200, 300, 350, 400]\n",
    "\n",
    "# Choose model\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "neurons_str = SAE.getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "n_folds = len(CVO)\n",
    "\n",
    "klDivergence = np.zeros([n_folds,len(neurons_mat)], dtype=object)\n",
    "klDivergenceFreq = {}\n",
    "\n",
    "for ineuron in neurons_mat:\n",
    "    \n",
    "    neurons_str = SAE.getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer-1]+[ineuron])\n",
    "\n",
    "    models = {}\n",
    "    outputs = {}\n",
    "    norm_data = {}\n",
    "    if verbose: \n",
    "        print '[*] Layer: %i - Topology: %s'%(layer, neurons_str)\n",
    "        \n",
    "    reconstructed_data = {}\n",
    "    \n",
    "    n_bins = 100\n",
    "\n",
    "    def getKlDiv(ifold):\n",
    "        train_id, test_id = CVO[ifold]\n",
    "\n",
    "        # normalize classes\n",
    "        if trn_params.params['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        norm_data = scaler.transform(all_data)\n",
    "        norm_data = norm_data[test_id, :]\n",
    "\n",
    "        models[ifold] = SAE.getModel(data=all_data, trgt=all_trgt,\n",
    "                                     hidden_neurons=hidden_neurons[:layer-1]+[ineuron],\n",
    "                                     layer=layer, ifold=ifold)\n",
    "\n",
    "        output = models[ifold].predict(norm_data)\n",
    "        kl = np.zeros([all_data.shape[1]], dtype=object)\n",
    "        for ifrequency in range(0,400):\n",
    "            data = norm_data[:,ifrequency]\n",
    "            reconstructed_data = output[:,ifrequency]\n",
    "            m_bins = np.linspace(data.min(), data.max(), n_bins)\n",
    "            kl[ifrequency] = KLDiv(data.reshape(-1,1), reconstructed_data.reshape(-1,1),\n",
    "                                   bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                                   kernel_bw=0.1, verbose=False)\n",
    "            kl[ifrequency] = kl[ifrequency][0]\n",
    "        return ifold, kl\n",
    "    \n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    folds = range(len(CVO))\n",
    "    if verbose:\n",
    "        print '[*] Calculating KL Div for all frequencies'\n",
    "    # Calculate the KL Div at all frequencies\n",
    "    klDivergenceFreq[ineuron] = p.map(getKlDiv, folds)\n",
    "    \n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "# Plot results    \n",
    "fig, m_ax = plt.subplots(figsize=(20,10),nrows=1, ncols=2)\n",
    "for ifold in range(n_folds):\n",
    "    # Sum the KL Div for all frequencies\n",
    "    for index in range(len(neurons_mat)):\n",
    "        klDivergence[ifold, index] = np.sum(klDivergenceFreq[neurons_mat[index]][ifold][1])\n",
    "        \n",
    "    m_ax[ifold].plot(neurons_mat, klDivergence[ifold,:], '-o')\n",
    "    m_ax[ifold].set_title('KL Divergence x Neurons - %i Layer - %i Fold'%(layer,ifold+1), fontsize=16, fontweight='bold')\n",
    "    m_ax[ifold].set_ylabel('Kullback-Leibler Divergence', fontsize=22)\n",
    "    m_ax[ifold].set_xlabel('Neurons', fontsize=22)\n",
    "    m_ax[ifold].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ineuron = 1\n",
    "reconstructed_data = {}\n",
    "\n",
    "neurons_str = SAE.getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer-1]+[ineuron])\n",
    "\n",
    "models = {}\n",
    "outputs = {}\n",
    "norm_data = {}\n",
    "if verbose: \n",
    "    print '[*] Layer: %i - Topology: %s'%(layer, neurons_str)\n",
    "for ifold in range(n_folds):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "\n",
    "    # normalize classes\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "    norm_data[ifold] = scaler.transform(all_data)\n",
    "    norm_data[ifold] = norm_data[ifold][test_id, :]\n",
    "\n",
    "    models[ifold] = SAE.getModel(data=all_data, trgt=all_trgt,\n",
    "                                 hidden_neurons=hidden_neurons[:layer-1]+[ineuron],\n",
    "                                 layer=layer, ifold=ifold)\n",
    "\n",
    "    outputs[ifold] = models[ifold].predict(norm_data[ifold])\n",
    "\n",
    "reconstructed_data = {}\n",
    "\n",
    "n_bins = 100\n",
    "\n",
    "def getKlDiv(ifold):\n",
    "    kl = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    for ifrequency in range(0,400):\n",
    "        data = norm_data[ifold][:,ifrequency]\n",
    "        reconstructed_data = outputs[ifold][:,ifrequency]\n",
    "        m_bins = np.linspace(data.min(), data.max(), n_bins)\n",
    "        kl[ifrequency] = KLDiv(data.reshape(-1,1), reconstructed_data.reshape(-1,1),\n",
    "                               bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                               kernel_bw=0.1, verbose=False)\n",
    "        kl[ifrequency] = kl[ifrequency]\n",
    "    return ifold, kl\n",
    "kl = {}\n",
    "for ifold in range(n_folds):\n",
    "    kl[ifold] = getKlDiv(ifold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kl[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reconstruction Analysis\n",
    "%matplotlib inline \n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'reconstruction'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "# Choose layer \n",
    "layer = 4 \n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "# Choose model\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "neurons_str = SAE.getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "models = {}\n",
    "mean = {}\n",
    "indexes = {}\n",
    "outputs = {}\n",
    "\n",
    "n_folds = len(CVO)\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    # normalize classes\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "    norm_data = scaler.transform(all_data)\n",
    "    norm_data = norm_data[test_id, :]\n",
    "    \n",
    "    if ifold == 0:\n",
    "        diffSquared = np.zeros([len(CVO), norm_data.shape[0], norm_data.shape[1]])\n",
    "        \n",
    "    print 'Topology: %s - fold %i'%(neurons_str, ifold)\n",
    "    \n",
    "    models[ifold] = SAE.getModel(data=all_data, trgt=all_trgt,\n",
    "                                 hidden_neurons=hidden_neurons,\n",
    "                                 layer=layer, ifold=ifold)\n",
    "    \n",
    "    outputs[ifold] = models[ifold].predict(norm_data)\n",
    "    \n",
    "    diffSquared[ifold] = np.power((norm_data - outputs[ifold]), 2)\n",
    "      \n",
    "mean = np.mean(np.mean(diffSquared, axis=0), axis=0)\n",
    "indexes = np.argsort(mean)[::-1]\n",
    "\n",
    "print \"Topology (%s)\"%trn_params.get_params_str()\n",
    "\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    # normalize classes\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "    norm_data = scaler.transform(all_data)\n",
    "    norm_data = norm_data[test_id, :]\n",
    "    \n",
    "    points = norm_data.shape[0]\n",
    "    \n",
    "    # Number of dimensions to analyse (even number is better!)\n",
    "    num_dim = 4\n",
    "    fig, m_ax = plt.subplots(figsize=(20,20),nrows=2, ncols=2)\n",
    "    for choose_index in range(num_dim):  \n",
    "        ax = plt.subplot(2,2,choose_index+1)\n",
    "        \n",
    "        # Plot classes\n",
    "        ax.plot(norm_data[:,indexes[choose_index]][0],\n",
    "                outputs[ifold][:,indexes[choose_index]][0],\n",
    "                \"b.\", label='Test Data', markersize=20)\n",
    "        ax.plot(norm_data[:,indexes[choose_index]][:points],\n",
    "                outputs[ifold][:,indexes[choose_index]][:points],\n",
    "                \"b.\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "        ax.set_title('Input x Output - Dim %i'%(indexes[choose_index]),fontsize=18, fontweight='bold')\n",
    "        ax.set_xlim(-2, 7)\n",
    "        ax.set_ylim(-4, 8)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        # sort both labels and handles by labels\n",
    "        labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "        plt.grid() \n",
    "        \n",
    "        # Small plot\n",
    "        rect = [0.065, 0.55, 0.35, 0.4]\n",
    "        ax1 = FunctionsDataVisualization.add_subplot_axes(ax,rect)\n",
    "        \n",
    "        eq_reconstruction = np.power((norm_data[:,indexes[choose_index]] - outputs[ifold][:,indexes[choose_index]]), 2)\n",
    " \n",
    "        mq_bins = np.linspace(np.min(eq_reconstruction), np.max(eq_reconstruction), 50)\n",
    "\n",
    "        n, bins, patches = ax1.hist(eq_reconstruction, bins=mq_bins,\n",
    "                                    fc=\"b\",\n",
    "                                    alpha=0.8, normed=0)   \n",
    "        ax1.set_xlim(0, np.max(mq_bins))\n",
    "        ax1.set_title(\"Squared Absolute Error\",fontsize=14, fontweight='bold')\n",
    "        ax1.grid() \n",
    "        \n",
    "        # Small plot\n",
    "        rect = [0.5, 0.05, 0.45, 0.4]\n",
    "        ax2 = FunctionsDataVisualization.add_subplot_axes(ax,rect)\n",
    "        \n",
    "        e_reconstruction = (norm_data[:,indexes[choose_index]] - outputs[ifold][:,indexes[choose_index]])\n",
    "      \n",
    "        m_bins = np.linspace(np.min(e_reconstruction), np.max(e_reconstruction), 50) \n",
    "        \n",
    "        n, bins, patches = ax2.hist(e_reconstruction,bins=m_bins,\n",
    "                                    fc=\"b\",\n",
    "                                    alpha=0.8, normed=0)   \n",
    "        ax2.grid()\n",
    "        ax2.set_title(\"Absolute Error\",fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlim(-np.max(mq_bins), np.max(mq_bins))\n",
    "        \n",
    "        ax.legend(handles, labels, ncol=1, loc='upper right')\n",
    "        plt.legend()\n",
    "\n",
    "        mse = metrics.mean_squared_error(norm_data[:,indexes[choose_index]], outputs[ifold][:,indexes[choose_index]])\n",
    "        ax.text(2, 6, 'MSE: %f'%mse, style='normal',fontsize=26, color='blue',\n",
    "        bbox={'alpha':0.0, 'pad':10})\n",
    "\n",
    "        #Save the figure\n",
    "        file_name = pict_results_path+'/'+current_analysis+'_%i_layer_%s_neurons_%i_fold_'%(layer,\n",
    "                                                                                            neurons_str,\n",
    "                                                                                            ifold)+trn_params.get_params_str()+'.pdf'\n",
    "        plt.savefig(file_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers to their corresponding folds\n",
    "def trainClassifierFold(ifold): \n",
    "    return SAE.trainClassifier(data=all_data,\n",
    "                               trgt = all_trgt,\n",
    "                               ifold = ifold,\n",
    "                               hidden_neurons=hidden_neurons,\n",
    "                               layer = 1)\n",
    "\n",
    "# Train classifier sweeping the number of layer\n",
    "def trainClassifierLayer(ilayer):\n",
    "    for ifold in range(len(CVO)):\n",
    "        SAE.trainClassifier(data=all_data,\n",
    "                            trgt = all_trgt,\n",
    "                            ifold = ifold,\n",
    "                            hidden_neurons=hidden_neurons,\n",
    "                            layer = ilayer)\n",
    "start_time = time.time()\n",
    "\n",
    "if K.backend() == 'theano':\n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    ####################### SAE LAYERS ############################\n",
    "    # It is necessary to choose the layer to be trained\n",
    "\n",
    "    # To train on multiple cores sweeping the number of folds\n",
    "    folds = range(len(CVO))\n",
    "    results = p.map(trainClassifierFold, folds)\n",
    "\n",
    "    # To train multiple topologies sweeping the number of layers\n",
    "    # neurons_mat = range(0,400,50) (start,final,step)\n",
    "    # results = p.map(trainClassifierLayer, neurons_mat)\n",
    "\n",
    "    p.close()\n",
    "    p.join()\n",
    "else: \n",
    "    for ifold in range(len(CVO)):\n",
    "        result = trainClassifierFold(ifold)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = {}\n",
    "for ifold in range(len(CVO)):\n",
    "    classifier[ifold] = SAE.loadClassifier(data = all_data, trgt = all_trgt, hidden_neurons=hidden_neurons, layer=1, ifold = ifold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    output = classifier[ifold].predict(all_data[test_id, :])\n",
    "    print sklearn.metrics.classification_report(trgt_sparse[test_id], np.round(output), target_names=class_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def KL(a, b):\n",
    "    a = np.asarray(a, dtype=np.float)\n",
    "    b = np.asarray(b, dtype=np.float)\n",
    "\n",
    "    return np.sum(np.where(a != 0, a * np.log(a / b), 0))\n",
    "\n",
    "values1 = [1.346112,1.337432,1.246655]\n",
    "values2 = [1.033836,1.082015,1.117323]\n",
    "\n",
    "print KL(values1, values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
