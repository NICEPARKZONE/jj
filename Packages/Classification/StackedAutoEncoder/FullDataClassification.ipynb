{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de Classificaçãoo para Marinha do Brasil\n",
    "### Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "Laboratório de Tecnologia Sonar\n",
    "\n",
    "Instituto de Pesquisas da Marinha - IPqM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e leitura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 2.09808349609e-05 seconds\n",
      "Time to read data file: 1.47833704948 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "outputpath = os.environ['OUTPUTDATAPATH']\n",
    "main_analysis_path = os.environ['SONAR_WORKSPACE']\n",
    "log_analysis_path = os.environ['PACKAGE_OUTPUT']\n",
    "result_analysis_path = os.environ['PACKAGE_OUTPUT']+'/StackedAutoEncoder'\n",
    "pict_results_path = result_analysis_path+'/picts'\n",
    "files_results_path = result_analysis_path+'/output_files'\n",
    "\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "\n",
    "subfolder = '4classes_old'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "\n",
    "if(not os.path.exists(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))):\n",
    "    print outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "        subfolder,n_pts_fft,decimation_rate)+' doesnt exist...please create it'\n",
    "    exit()\n",
    "    \n",
    "#Read lofar data\n",
    "[data,class_labels] = joblib.load(outputpath+'/'+\n",
    "                                  'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))\n",
    "m_time = time.time()-m_time\n",
    "print 'Time to read data file: '+str(m_time)+' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento dos dados\n",
    "\n",
    "Os dados encontram-se no formato do matlab, para isso precisam ser processados para o formato de python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process data...\n",
    "# create a full data vector\n",
    "all_data = {};\n",
    "all_trgt = {};\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    for irun in range(len(data[iclass])):\n",
    "        if len(all_data) == 0:\n",
    "            all_data = data[iclass][irun]['Signal']\n",
    "            all_trgt = (iclass)*np.ones(data[iclass][irun]['Signal'].shape[1])\n",
    "        else:\n",
    "            all_data = np.append(all_data,data[iclass][irun]['Signal'],axis=1)\n",
    "            all_trgt = np.append(all_trgt,(iclass)*np.ones(data[iclass][irun]\n",
    "                                                           ['Signal'].shape[1]),axis=0)\n",
    "            \n",
    "all_data = all_data.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceamento de Classes\n",
    "Os dados encontram-se desbalanceados. Com isso, os classificadores podem se especializar em uma classe (gerando mais SVs para a mesma) e não se especializar em outras\n",
    "\n",
    "Acessados em 21/12/2016\n",
    "\n",
    "https://svds.com/learning-imbalanced-classes/\n",
    "\n",
    "http://www.cs.utah.edu/~piyush/teaching/ImbalancedLearning.pdf\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n",
    "\n",
    "Para solucionar isso, a primeira solução é \"criar\" dados das classes com menos eventos de maneira aleatória. Outras soluções podem ser propostas posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd event of ClassA is 4312\n",
      "Qtd event of ClassB is 9781\n",
      "Qtd event of ClassC is 3833\n",
      "Qtd event of ClassD is 7918\n",
      "\n",
      "Biggest class is ClassB with 9781 events\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (4312, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (9781, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (3833, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (7918, 400)\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "# unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "# Same number of events in each class\n",
    "qtd_events_biggest_class = 0\n",
    "biggest_class_label = ''\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "        qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "        biggest_class_label = class_label\n",
    "    print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "\n",
    "balanced_data = {}\n",
    "balanced_trgt = {}\n",
    "\n",
    "from Functions import DataHandler as dh\n",
    "m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if len(balanced_data) == 0:\n",
    "        class_events = all_data[all_trgt==iclass,:]\n",
    "        balanced_data = m_datahandler.CreateEventsForClass(\n",
    "            class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "        balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "    else:\n",
    "        balanced_data = np.append(balanced_data,\n",
    "                                  (m_datahandler.CreateEventsForClass(\n",
    "                    all_data[all_trgt==iclass,:],\n",
    "                    qtd_events_biggest_class-sum(all_trgt==iclass))),\n",
    "                                  axis=0)\n",
    "        balanced_trgt = np.append(balanced_trgt,\n",
    "                                  (iclass)*np.ones(qtd_events_biggest_class),axis=0)\n",
    "        \n",
    "all_data = balanced_data\n",
    "all_trgt = balanced_trgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definições do treinamento\n",
    "\n",
    "Nessa célula temos os parâmetros do treinamento a ser realizado. No log, deve ficar armazenada a data do treinamento para a reconstrução dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "Dividing data in trn and tst\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/StackedAutoEncoder/train_info_files/2017_03_08_12_05_27_train_info.jbl']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(\"Classification\",'StackedAutoEncoder')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_inits = 1\n",
    "norm = 'mapstd'\n",
    "\n",
    "# params for top sweep\n",
    "top_min = 0\n",
    "top_max = 750\n",
    "top_step = 150\n",
    "\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_inits'] = n_inits\n",
    "train_info['norm'] = norm\n",
    "\n",
    "# top sweep\n",
    "train_info['sweep_top_min'] = top_min\n",
    "train_info['sweep_top_max'] = top_max\n",
    "train_info['sweep_top_step'] = top_step\n",
    "\n",
    "# divide data in train and test for novelty detection\n",
    "print 'Dividing data in trn and tst'\n",
    "CVO = cross_validation.StratifiedKFold(all_trgt, n_folds)\n",
    "CVO = list(CVO)\n",
    "train_info['CVO'] = CVO\n",
    "\n",
    "train_info['topologic_anal_done'] = False\n",
    "train_info['train_done'] = False\n",
    "train_info['results_done'] = False\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+date+'_train_info.jbl'\n",
    "joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'date': '2017_03_07_23_34_47', 'package': 'StackedAutoEncoder'}, 1: {'date': '2017_03_07_23_35_21', 'package': 'StackedAutoEncoder'}, 2: {'date': '2017_03_08_00_07_14', 'package': 'StackedAutoEncoder'}, 3: {'date': '2017_03_08_00_20_36', 'package': 'StackedAutoEncoder'}, 4: {'date': '2017_03_08_11_52_41', 'package': 'StackedAutoEncoder'}, 5: {'date': '2017_03_08_11_58_10', 'package': 'StackedAutoEncoder'}, 6: {'date': '2017_03_08_12_03_24', 'package': 'ConvNeuralNetwork'}, 7: {'date': '2017_03_08_12_05_27', 'package': 'StackedAutoEncoder'}}\n"
     ]
    }
   ],
   "source": [
    "# Read log files\n",
    "from Functions import LogFunctions as log\n",
    "mlog = log.LogInformation()\n",
    "log_entries = mlog.RecoverLogEntries(package_name=\"Classification\")\n",
    "print log_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing train performed in 2017_03_08_12_05_27 and for StackedAutoEncoder analysis\n",
      "StackedAutoEncoder Train Info File\n",
      "Date: 2017_03_08_12_05_27\n",
      "Number of Folds: 2\n",
      "Number of Inits: 1\n",
      "Top Sweep: min 0, max 750, step 150\n",
      "Topological Analysis Done: False\n",
      "Train Done: False\n",
      "Extract Results: False\n"
     ]
    }
   ],
   "source": [
    "# Read Information of Train Info File\n",
    "choose_date = '2017_03_08_12_05_27'\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'StackedAutoEncoder':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'Analysing train performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    print 'StackedAutoEncoder Train Info File'\n",
    "    print 'Date: %s'%(choose_date)\n",
    "    print 'Number of Folds: %i'%(train_info['n_folds'])\n",
    "    print 'Number of Inits: %i'%(train_info['n_inits'])\n",
    "    print 'Top Sweep: min %i, max %i, step %i'%(train_info['sweep_top_min'],\n",
    "                                                train_info['sweep_top_max'],\n",
    "                                                train_info['sweep_top_step'])\n",
    "    if train_info['topologic_anal_done']:\n",
    "        print 'Topological Analysis Done: True'\n",
    "    else:\n",
    "        print 'Topological Analysis Done: False'\n",
    "    if train_info['train_done']:\n",
    "        print 'Train Done: True'\n",
    "    else:\n",
    "        print 'Train Done: False'\n",
    "    if train_info['results_done']:\n",
    "        print 'Extract Results: True'\n",
    "    else:\n",
    "        print 'Extract Results: False'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked AutoEncoder Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first step: sweep the number of neurons in each layer of SAE and vary number of layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "StackedAutoEncoder analysis performed in 2017_03_08_12_05_27 and for StackedAutoEncoder analysis\n",
      "Fold: 1 of 2, neuron: 1 of 750, init: 1 of 1\n",
      "Epoch 00121: early stopping\n",
      "Fold: 1 of 2, neuron: 150 of 750, init: 1 of 1\n",
      "Fold: 1 of 2, neuron: 300 of 750, init: 1 of 1\n",
      "Fold: 1 of 2, neuron: 450 of 750, init: 1 of 1\n"
     ]
    }
   ],
   "source": [
    "# SAE parameters extraction\n",
    "\n",
    "# first layer\n",
    "\n",
    "%time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "\n",
    "from Functions import PreProcessing as preproc\n",
    "\n",
    "models = {}\n",
    "trn_descs = {}\n",
    "losses = np.zeros((train_info['n_folds'],\n",
    "                   len(range(train_info['sweep_top_min'], \n",
    "                             train_info['sweep_top_max']+1,\n",
    "                             train_info['sweep_top_step']))))\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'StackedAutoEncoder':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'StackedAutoEncoder analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if train_info['topologic_anal_done']:\n",
    "        print 'Topological Analysis is done, just analyse it'\n",
    "        continue\n",
    "    \n",
    "    trn_params = preproc.TrnParams(learning_rate= 0.005,verbose=False,\n",
    "                                   train_verbose=True, n_epochs=500)\n",
    "    \n",
    "    for ifold in range(train_info['n_folds']):\n",
    "        train_id, test_id = train_info['CVO'][ifold]\n",
    "        \n",
    "        models[ifold] = {}\n",
    "        trn_descs[ifold] = {}\n",
    "        \n",
    "        # normalize data based in train set\n",
    "        if train_info['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "        elif train_info['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "        elif train_info['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        \n",
    "        norm_all_data = scaler.transform(all_data)\n",
    "        \n",
    "        neuron_count = 0\n",
    "        for ineuron in range(train_info['sweep_top_min'], \n",
    "                             train_info['sweep_top_max']+1, \n",
    "                             train_info['sweep_top_step']):\n",
    "            if ineuron == 0:\n",
    "                ineuron = 1\n",
    "            \n",
    "            best_init = 0\n",
    "            best_loss = 999\n",
    "            \n",
    "            for i_init in range(train_info['n_inits']):\n",
    "                print ('Fold: %i of %i, neuron: %i of %i, init: %i of %i'%\n",
    "                       (ifold+1, train_info['n_folds'],ineuron, top_max,\n",
    "                        i_init+1, train_info['n_inits']))\n",
    "\n",
    "                # create model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(ineuron, input_dim=norm_all_data.shape[1], init='uniform'))\n",
    "                model.add(Activation('tanh'))\n",
    "                model.add(Dense(norm_all_data.shape[1], init='uniform')) \n",
    "                model.add(Activation('tanh'))\n",
    "                \n",
    "                sgd = SGD(lr=trn_params.learning_rate, \n",
    "                          decay=trn_params.learning_decay,\n",
    "                          momentum=trn_params.momentum, \n",
    "                          nesterov=trn_params.nesterov)\n",
    "                \n",
    "                model.compile(loss='mean_squared_error', \n",
    "                              optimizer=sgd,\n",
    "                              metrics=['mean_absolute_error'])\n",
    "\n",
    "                # Early Stopping\n",
    "                earlyStopping = callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                        patience=25, \n",
    "                                                        verbose=trn_params.train_verbose, \n",
    "                                                        mode='auto')\n",
    "                # train model\n",
    "                trn_desc = model.fit(norm_all_data[train_id], \n",
    "                                     norm_all_data[train_id], \n",
    "                                     nb_epoch=trn_params.n_epochs, \n",
    "                                     batch_size=trn_params.batch_size,\n",
    "                                     callbacks=[earlyStopping], \n",
    "                                     verbose=trn_params.verbose,\n",
    "                                     validation_data=(norm_all_data[test_id],\n",
    "                                                      norm_all_data[test_id]),\n",
    "                                     shuffle=True)\n",
    "                \n",
    "                # check if train was the best one\n",
    "                if i_init == 0:\n",
    "                    trn_descs[ifold][ineuron] = trn_desc\n",
    "                    models[ifold][ineuron] = model\n",
    "                    losses[ifold,neuron_count] = np.min(trn_desc.history['val_loss'])\n",
    "                else:\n",
    "                    if np.min(trn_desc.history['val_loss']) < best_loss:\n",
    "                        best_init = i_init\n",
    "                        best_loss = np.min(trn_desc.history['val_loss'])\n",
    "                        models[ifold][ineuron] = model\n",
    "                        trn_desc[ifold] = trn_desc\n",
    "                        losses[ifold,neuron_count]= np.min(trn_desc.history['val_loss'])\n",
    "                        \n",
    "            neuron_count = neuron_count+1\n",
    "                \n",
    "        \n",
    "    \n",
    "    print 'Train done'\n",
    "    # saving file\n",
    "    \n",
    "    top_info_name = result_analysis_path+'/output_files'+'/'+choose_date+'_top_sweep_losses.jbl'\n",
    "    joblib.dump([losses],top_info_name,compress=9)\n",
    "    \n",
    "    for ifold in range(train_info['n_folds']):\n",
    "        for ineuron in range(top_min, top_max+1, top_step):\n",
    "            if ineuron == 0:\n",
    "                ineuron = 1\n",
    "            model_name = (result_analysis_path+'/output_files'+'/'+\n",
    "                          choose_date+'_model_fold'+str(ifold)+\n",
    "                          '_neuron'+str(ineuron)+'.h5')\n",
    "            models[ifold][ineuron].save(model_name)\n",
    "    \n",
    "            \n",
    "    train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "    train_info['topologic_anal_done'] = True\n",
    "    joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/StackedAutoEncoder/train_info_files/2017_03_08_11_58_10_train_info.jbl']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to reset analysis\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "train_info['topologic_anal_done'] = False\n",
    "joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "StackedAutoEncoder analysis performed in 2017_03_08_12_05_27 and for StackedAutoEncoder analysis\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAooAAAGyCAYAAABjteXRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8FPX9x/HXJwkIcgoUPEBQ6w2I2taqFQGPet/3AYgH\nIODV1mp/3lZtrbZWgkAFFaoo2iJWPFqr4oF4X1CtVTkEBRRBRCSEJJ/fHzMLw7KbZEOS2Unez8dj\nHrs7x3c+u5/Z3e/OfGbW3B0RERERkXQFcQcgIiIiIvlJHUURERERyUgdRRERERHJSB1FEREREclI\nHUURERERyUgdRRERERHJSB1FkVpgZl3NrCIcymNYf0Vk2LYe1ndgZH1z6np9dSGOnJnZfZF1XlMH\n7bc3szFmNtfM1obrmVvb66lmLAVm9msz+8DMvo9un2Z2XeTxPXHEV5X6fk8lhZk1N7NbzOxjMytJ\nvUZxxyV1Rx3FBsTMBqR9uFWY2R4Z5jsyw3y90+bZ2syKzeyj8EP+ezNbaGavm9k4Mzsybf5rM7SZ\nPlTrCyF8HteGQ89Ne1XqXZwXJnWgvj+wG8KFWOvzOXhkqAv3AhcA2xJ8vsexTaRcAtwC7AxslhZL\nXb8OwLofNKnPkmNr0ESdx5hAvwd+DWwPNKGa25iZbWZmV5jZm2b2rZmtMbMlZjbLzB4ysxGVLNvZ\nzMrTvk8OzzJvpu/B9OG5Gj73Rqko7gCkTkQ/2EYA56VNv4hKPvzMbDvgNaBD2nxbhcOPgKbAE1Ws\nu6YGAgeGbc0F3q+FNuuLEc8Xy88i9xfFsP4kq8+c/Ra4O7z/WW02bGZNgMNZ/1yGA+8BJbW5nhwc\nF7l/N3A/QYdiETAeeCactqQOY+gDXBvevw94LMfl43o/57PjWP+a3EiQx0pfIzMrAqYD+4SjUvN3\nCIfdgf2AkVmaGMjGuRgIPFXJaiuLSTnNgTqKDZsBp5vZr9x9OYCZ7QwcnDZP+pvmWtZ3Et8HbgMW\nAq2AXsDxGZaJWgycnGF8XX4hNGru/krcMUjV3P1T4NM6an4roHD9qnx0Ha0HM2vh7quqmG2byP0H\n3f3lyOOF4VDb68ymUXYMzGxzd/++lpvdhvXfG/e6+/xqLHMmQSfRgeXANcCHBH2QXYAjgR9WsvzZ\nbJhDA442s7bu/k2WZVIx/iy8H7WiGjFLirtraCADMIDgF3s58B2wJrx/eWSeUeE8y8Pb1Py9I/P8\nJzL+qCzrapH2+NpIe3M2Mf5swz2ReVsCVwNvAd8Cqwm+gP8C/DCt3QOjsQFdgEnAUmAV8AKwX4Z4\nmgAXA68A34Sv52fAA8BeafN2jb6eGdrqDfwN+DxsZxnwInAuYBnmPwF4O3xenwE3ExzCy7iOtNdp\n27Rp24d5/zB8vivD+2OBJuE87YAxwKvAF+F6vwc+Dl/T7bK8puXVzTcwDHgyzMEKoJTgx8PTwHEZ\n5p8XeU79gF8C/yPYQzYHuDRt/gLgzvB1XRA+15KwnfuBParKGdANKAvHrQQ2T1vm15FlJofjjGAv\n/WuR5/Ul8Gb4mu4UWf6+yPLXRMa3IfhB9mH4upeE28p04FagWRWv7fRIPqK36e+b7YDRYV5Xh8/x\nXeB6oE0l7+l7gEMJ3gvfAW9XEkt0ufRY5oTzXJclvuhnwHMERy+eCV/Xr6v7WqXlNtMwtxrbazT+\nbSPjq7UdAy3C6ak2uqW1f0pkHa+lTTsUmEqw93VNuD09BvysivfJIWEu5wBrgYuq8TyrtU2wftvN\ntI09V8U6RkWWuS3LPC2yjN8vsuxn4eucejwkw/zRbWijz2INuQ+xB6ChFpO5YUfxC+DB1IciwZdZ\na4JOVTnwx7QPwmhH8ZXI+JnAUUDbKtZdWx3F8rQPoPLIcE84XyeCDkOmeSoIvsj6RdqNdmq+JtiT\nkb5MCXBAZJnNw+eebR2lwNmR+bN2FAk6OOVZ2qkAHgcKIvP3zzBvOUHHozzLOqLzR7/Ujgxfj0zP\noxxoHc63c5bpqfUtJfJFR806ijMrab+CtC81gu02Nc9HafGl7p8SmX+zKp5DCfDjqnIW5iPVzjlp\nMb0emfbzcNz1law3PcZ7I/NGO4ovZGkjNa5jFa/t8xmWSQ3jIzn7NsP01GvwKbBVhvd0OfAJQQc6\ntUxVHcVssXyaoe1MHcVy1nf2U8t+Xd3XKsxtZZ8ln1Zje83WUaz2dkxwKDXVxvVp7T8cmTY4Mv53\nWWKuCHNwQQ7vk0o7irlsEwTbbra8PlvFem6OLLOQ4LBxl2p+btwdWfb3wBmRx69m+R5RR7EWh9gD\n0FCLydzwDfIFsH/kDXUsQXF56sNmh7QPo2hH8f8i06LDJwS1RQdmWHf0gz/br/hjqoi/A8Gvx7cj\nbd0YjtsP2CGcb0pk+hcEHauj2fALZBHQPJw/2qmpAGYDJwInEeyVSE37MBLLHyPjVxDsRTgc+Htk\n/PfANuH82TodPdM+UO8FDgN+RdBxSY3/RTh/C4K9vanxL4XP7YLI+GwdxQ2+1MLX85tIWx8T1Kse\nHLb3Cus7ih3DvJ9EsFfiAIJO5sRIuyMj66tJR3EwwRfEEWH7BwEXEuzFKCfYyxrtMKe+AFMd86vD\nHDwXWfeMyPyFBHuqTiPYI3NA+FrfHpn/8cj82XJ2WGT+lyPjt42MX0i4JxiYFY5bAwwNX5sTgMvD\nWI+PtLFRRxFoHxk3L8xBH+B0gi/Yd4EfVPHa7h6uM7od7Ev4viHoRH8e2RZmEnwmnEmwlyY1Pvr6\npL+n3w9jOpi0DnRaLJ3D9X4RWX5YOG6vDG1n6yhWhLGdE24rw6v7WhHUUO9H8HmVmn8a6z9L9qrs\n9cz2nsp1OyY4rJpqY36kjWYEe+4qCD5H2oTjD4/M/x1wGcHe9Esi7a8hctSEDd8n5QQdq8MJPuP2\nr+T55bRNhNtRau9eal0nhON2r+K1/FlkueiwhODz/EygMMNyzQg+w1Lr60HwI35lZNwuWb4HK/su\nqnJPq4bIaxp3ABpqMZlpHcVw3NvhG+Z5go5CBTAtnBb9VRjtKDYBHolMy/SGuyNt3demtZdpqLSj\nGGnr+Uhb/dOmtWXDPRvHRqa1J9gDkVr2xHB8qlOz0QcLsFfatD3C8V9F1nFx2muzMDJ/qoOXrdMR\n7aS8m/Zcbo1MmxWOi37ZrwY6ROa/MNM6IrlM7yhG518BbFnF634Ewd60Lwg6Zuk5fyMyb006ip2B\nYtYfAk9vv5zIFw7BF2Bq/J8j438SWeartHXsS7CnZj5BRzza9gbzZ8tZOO3jyHI7h+Mui4y7OTLv\ny+H47wg6Ua0reQ0ydRQ3IzhMWE7Q0dkT2KwG7//Kns/RadtVx8i0IyLTylLbHBseJVgZ3RarGU80\nf73TplXVUUzFsnvacjm9VtnWU834s3UUc92On42MT+2FPiEy7sHIvI9Ext9H8GM/NUyLTLsp7XVO\nfVY9nMPzy3mbqOx1qcb6LmfDH8fp3yszgaZpy0T3Hr4fGX9/ZPwtacukdxQzfRepo5jDoMvjNHzF\nBIedexP8InSyn1kGgLuvdfeTgR8T/Ep/geAD0VlfUDzCzH6SpYnFBL8gD0gbXtykZxLYkaAWLVWc\nPCMS99cEh15Sdsmw/HJ3/29kmVQd4Lr2zewHBJ3OTOtYS3D4sbJ1kGX6y2nTUo8N2Cm1/tSqCA6P\nLa1k+arsFmnrNXdfnG1GMxtE8EV0JMGh/UI2voTJFjmuP9p+J4J60gsJDnM3y9B+Zet4PnL/68j9\ndpF1HEKwjZ1E8GWeunRHddpPFz0R5Nzw9qTIuAmR+2PCdTQH/gV8Y2aLzexpMzvfzAqphLuvibTX\ng+B1WmVmc8zsQTM7tJoxVya1Haa2qy8j06LblRHkZ4MQCfbcLqX+OPCJu/9ng5H181plVcPteFTk\nfmpbip7sd1/k/m6RdvoTHFFIDUekwiDYg7xBaOHtlOo9E2DTtomcufutBN9BvyL4QfoVG752PyHY\ncxo1IHL/gSz3zzKz9JNVYMOTWdK/ix6u8RNphNRRbPgmERwKSfnE3f9ZnQXd/W13v9rd+xF8IZ9O\n8Gs+ZZ8Mixmwxt1nuvsraUO2s9NykekDobbV5jqq21ZqvuiXjWeasYbrrqqtX7P+A/sp4BiCD9TL\nIu1syufFIIJDgk5wuGkQwV7JA9iw45dtHdFtuCzLPL9kfQf3dYKO3QEEeyVSz6G6+biH4AeEAWeb\nWTfWn7X5mruv+0Hi7veHz2UswQktywme66HhuFursb7zgbOAhwgOZa8h2EN4KvC0mR1dzbizqex5\nV2c7i+OSS9nWWdevVWVqsh0/RnCIN3Wm7jas7/R9QfDjIhOvZGiZZZlc8rSp20TO3P1zd/+jux/n\n7lsSHLaeG4ll3XdK+DodFMZiwC2Ri3unLs1mwNYE77Vs68z0XZT1R7NsTB3FBs7dS4BxrP91VVzV\nMmb2czNrntbOWnd/mOBsupS62n4qKllH6iSW1AfZ/qkJZtae4Jdvatp/2dgW4SWCUsvsRbAnKOWT\n8Jd19EM/uo4igj2tKZnWEfVhpnZC0WsfpjoeH6dWBexgZu0i8/SuYl3pUntjDPipmW1Zybzbsv7D\n+nJ3f8KDS+60ynGdlbWf8ld3n+DB5VIWEuy9re113Ojuj4bPoUmuDYU/ah4MH3YkqHVLvT73Zpj/\nZXe/0N33dff2wE9Tkwh+YFW1Pnf3B939THffw91bEByqq3YbVUhth6ntqmNk2gHRUNhwr3x0fH3L\nuM4cX6vKPktqIuft2N3LCX4wQFA7OZHgfeXARHePPs8PWb+d3eLuhZkG1nc0N1pdDs9lU7eJajOz\nfTJ9/rj7a2zYUY7mqH/kcbYOc0p0z6PUMl1HsWFK/7C4k6BgGjY8zJHNVUB3M5tKcMhjPsG2chQb\nHkqdmWXdm5lZeqcI4Dt3f68a64920k42s/kENXP/dfevzewxgms5AtxlZm0J9jj9gvWdviUEl7BI\njw3gb2Z2HcEH5A2R6f9z93fD+xOBS8N5rjOzMoJO8nmsvz7cGoK9GpWZSHA4xYCe4b/TPExw2Cz1\nTwTO+s7HvwjqCdsQ1GNNMbPbCX4138j6X9eZpOf9YYJ/xmgVDi+a2a0EJwFsR1CQf7i7fxs+t13D\n5a42s/EElyb5TRXrrK7oD4yTzOxVgr1/12xiu+nrSP0IuCzM2Q+p+nXLZhTBHiOAvuFtCTA5OpOZ\nPUKwl3M6wZ6jVcDPU5MJDk9WyoK/QZxGcFjzC4LXJvrDoMo2qvCvsN2tCTorU83s9wTbxc3hPA48\nWc+HmHOW42sV/Sw5wMyOIHh/LfbgmpZVSX9P1XQ7vpvgZKwi1m9LsGEJAwQ/SE4g2G4uD3+Ypk7S\n2xbYg2Bv/1lseinPpmwTuf5wOIrg+fwL+DfBD+y1BDWmZ0fmi36n9I/cv5cNS34I476G4LU61sxa\nh59lG8jyXVQWdlKlOuIuktRQewMZTmapYv5oUXL0ZJaXyH7GWKoYeGJaW9dmmT86ZL2kRlpb52dZ\n/oxweieCD5pMBdHlBIX32S6P8xXBh3162yVEzuYmOLNuRiXrWAOcGZm/a3R62vP5BetPwMn0Wv6D\nyBl/BB+c6estJ/hizLaObIX3RxOcZJEpn+WsP+v5gsi46PRnI4/nZHhNNxhfSU47EXxpp8cxi6Cm\nNdN2ODfL+IyvNUHnrKrnUF6dnEXmiV4qqhy4P8M8T2V4baPr/2Nk3nsj46OXx1ldRRvHVuM1rvT5\nEHSmVlSyLXwMbJ3hPZ3ziSCV5a+ytkm7jmKWdqv9WhH8sM303vtLjp+PqRPEct6OI+1NStuWZmRZ\n7y2RebI9xyrfJ9XMUU7bRGWfNVWs58ZKnlNq/GygZTj/vpHpZWQ4EY+gg7go0sYFGbahbMOyXLfn\nxjzo0HPDk2m3fK7zDgWuINgj9xFBzVUZwbX0ngcGuXv/Ktqr6lBBZcYT/KJdQPAB4ARv7mAl7ksI\n9nZdS3Dm4yqCjttcgsPse7r7c1naXklQF/NXgg/81QS/zA9y9xci6/ieoDN0KUHd2bcEv4A/Jzgk\nua+7P7Bh05mfp7vfTrAXYQrBB9tagks+zCD4cDvGg8NTqfn/SlBf9174vD4H/kBwiZGU9H9byLbu\nxwn+TWcMwWH71eHr9RHBXo7V4Xx/AYYQdMBXh9MvJtjjmi1/1c5rmLMDCfYmrCDYliaGr8vqStqp\navy6aR7U3p5AsE18T7An/HqCTnBNn0PqRITU3sj7MsxzV/hcPiTYs11GkN+ZBGdXXpZhnemuIKhl\nm0uwjZYRXGT5KYK9vtX967msz8fdXyTYFsYSXB9vDcHr9D7BF/ne7v5Fllhz3YOUHk+2aZnarion\n1X6tPDhx7WyCMozSarRdaRybsB3Dhie1QFAHu/FK3a8kqLl7lPVXIFgWPocJBJ8Nr2aJNSebsE3k\nur67CC4rNJmgU/0lwefgtwRX5rgO+Km7fxfO3z+yjow1hR70Ch+LxDEwS4yb8l0krL8OmEiDZWYH\nEnRwneBaZtvHHFKNmNkI4M8Ez+Ndd9875pAaPDNrQfBlBvC5u28bZzySbGa2hOBkmNUEe8lWxhyS\nSJVUoyiSZ8LLvAwiuFZY6v9Q+7FhPWV6bZPUIjNrRlB+kKovdYK/MhTJiZk1JaidPpX1Z0w/qE6i\nJIU6itKY1MeldWpDAcGXyqlp41OHTKZSjbPXZZM8zYYnSHxBcFKYSK7+woYnZqwCfhtTLCI5U42i\nNBZJqk35H0Hd00esr41cTNB5Od3dT3T3ikqWl02X2lZWENTqHuIZzqgUqYbUtrSKoB76UHefF2tE\nIjlQjaKIiIiIZKRDzyEzU49ZREREEsPd67ykSoeeI9yda6+9NvZrFmnIfVDekj0of8kdlLvkDspd\ncodrr7223vpG6iimmTdvXtwhSA0ob8mm/CWXcpdcyl1y1Wfu1FEUERERkYzUUUwzcODAuEOQGlDe\nkk35Sy7lLrmUu+Sqz9zprOeQmbleCxEREUkCM8N1Mkv9mz59etwhSA0ob8mm/CWXchePbt26YWYa\nGsHQrVu3jfJfn+87XR5HREQkYebPn4+OgjUOZvH+qZgOPYd06FlERJIiPOwYdxhSD7Llur4OPWuP\nooiISCM2a9YsZs2axezZs1mxYgVt2rShe/fu9OjRgx49esQdnsRMNYppVG+TTMpbsil/yaXcJV+P\nHj0444wzcHc+++wz3J0zzjij0XQSu3XrRmFhIS+++GJsMRQUFFBYWMhnn31Wrfnr832njqKIiIjE\nolu3bhQUFFBQUMDMmTPXjX/55ZfXjd9+++3XjZ87dy4nnHACnTp1onnz5nTp0oXDDz+cuXPnAkHt\nZmq56NCuXbusMZx33nlccskldO7cue6eaILp0HOaPn36xB2C1IDylmzKX3Ipdw3DkiVLmDFjBgsW\nLGDFihUsWbKETp061fl6U2f2AowePZp999133f3U9KjjjjuO2bNnc9BBB7HjjjuycOFCXnzxRRYt\nWsR22223QbtDhw6ladOmAGy++eZZY7jqqqtq9TnVh3p938X9f4X5MgQvhYiISP6rze+sxYsX+3HH\nHefbbbedd+zY0bfbbjs/7rjjfPHixbW2jmy6devmBQUF3r59e2/evLl//fXX/tVXX3mzZs28ffv2\nbma+3Xbbubv7smXL3My8Xbt2G7RRWlrqq1evdnf3efPmuZl5QUGBr1ixoloxdO3a1c3MX3jhBXd3\nP/DAA93M/Morr/TevXv75ptv7vvvv79/9tlnGZcfMGCAm5n/6U9/Wjdu4MCBbmZ+++23+9q1a/3g\ngw/2Lbfc0ps2bept27b1Y445xhcsWLBu/lTM8+fP36j9bLkOx9d5/0h7FNNMnz5dv5ATSHlLNuUv\nuZS7+N19993MmTOnxsvPmDGDhQsXsnz5ckpLSyktLeW9997j5JNPZv/9969xu9tvvz3nn39+teYd\nMGAAd9xxB/fccw8VFRWUlpYybNgw/vjHP66bp1WrVrRs2ZJvvvmGPffck379+tG7d28OOeSQjHsM\nr7jiCjbbbDMAfvjDHzJs2LCM647u1Yw+/sMf/sBpp53G/PnzmTlzJldddRUTJkzYaPn+/fszceJE\nJk+ezCWXXEJZWRn/+Mc/KCoq4qyzzqKiooIlS5Zw2GGH0bJlS2bOnMnjjz/O2rVrefLJJ6v1+qTT\ndRRFRESkWubMmcPs2bNrvPyCBQv4/vvvKS0tpaysDIBVq1axYMGCTWo3FwceeCD//Oc/GTt2LO7O\nbrvtxgEHHLBBR7GoqIh77rmHCy64gPfff5/333+fP/3pT3Tq1Ilp06ax9957b9Dm2LFjN2g/W0cx\nm6FDh3LnnXdy3333MWjQIN55552M8/Xt25cuXbrw+uuvM3/+fGbPns3y5cs5/PDD6dixIwBTpkzh\n8ccfZ/HixXTv3p233347MSeCqaOYRr+Mk0l5SzblL7mUu/hFT/aoiRUrVrBw4UJKS0sBaNq0KS1a\ntKBz585079693uIaMmQIF110EWbGyJEjM85z0kknccwxx/DCCy/w0ksvcffdd/Pll19y4403MnXq\n1A3m/eabb2jVqlWN4+/VqxcAbdu2BeC7777LOJ+ZcdZZZ/G73/2OyZMn88EHH2BmDBgwAICXXnqJ\nvn37UlFRscGeyzVr1rBy5coaxVif7zt1FEVERBKsuod3s1myZAlDhgzhvffeY9WqVbRo0YI99tiD\nMWPG1MsJLSn9+/fniiuuoLCwkLPPPpvnnntug+llZWW89tpr7L///hxyyCEccsghtG/fnksvvZSV\nK1du1J5v4gXJi4qCLlJ1/hmlf//+3HLLLTzwwAN89tlntG7dmmOPPRYI9iZWVFRw1FFH8fDDDzNr\n1iz22WefWomxPqijGPGb3/yGDz74gN12200XG00Y1Uklm/KXXMpd8nXq1IkxY8Zw8skns2DBAjp3\n7lzvnUSA1q1b89JLLwFk3Mu2Zs0aDjjgAHbddVf23HNPmjdvztSpUzEzDj300I3mj9YoAvzud7/b\n4HFt2nnnnfnxj3/MG2+8AcC55567bl2p13HmzJkMHz6cF154YZPXpxrFmLg7S5YsYdddd+WMM86I\nOxwREZF60alTJ/bff39mz55N9+7d67WTGN1jt+eee240LTW9WbNmXHbZZTz//PM89dRTrF69ms6d\nOzN8+HB+9atfbdRetEbRzLj++uuzdhSr2muYfsJLJgMGDODNN9/EzOjfv/+68cOHD+eNN97gX//6\nFy+99BJXXXUVgwYN2qi9uP/TORv913PIzPyKK65Y9ya55ZZb4g5JREQko7r4r+crr7xS34F5KO7/\netY/s4iIiDRis2bNYtKkSZgZ2267LWbGpEmTmDVrVtyhSR7Qoec0S5cujTsEqQHVSSWb8pdcyl3y\nqR4/efRfzyIiIiISO3UU03To0CHuEKQGtEcj2ZS/5FLuROpffb7v1FEUERERkYxi7yia2cVm9rCZ\nzTGzisjQv+qlN2prWzP7i5nNM7MSM1tiZlPNbL/qtqEaxWRKyl8hSWbKX3Ipd/Ho2rXruku2aGjY\nQ9euXTfKf2O7juJ1QOvwfo3P9TezvYBngC0i7XQAjgGOMrNz3P2vmxCniIhIXpg3b94mt6ETkaQ6\nYt+jCLwPjAcuBL4Ccr4mkJkVApOAtgSdxCcIOoi3p2YBxphZt6raUo1iMunDLtmUv+RS7pJLuUuu\nRvVfz+5+YOq+mV1Rw2YOB3YK768ATnb3NcATZtYLOBhoBgwFfp2tkRkzZrBgwQJWrFjBkiVL6v3v\ni0RERETyST7sUawN/cJbB94OO4kpMzLMl9GCBQtYvnw5CxcuZMiQISxZsqS245Q6ojqpZFP+kku5\nSy7lLrl0HcXcbR+5vzhtWuqxATtU1sjSpUspLS2loKCANWvWMG7cuNqMUURERCRRGkpHsUXkfmna\ntOjjlpU1UlFRQXl5OaWlpRQWFrJ4cXqfU/KVam2STflLLuUuuZS75GpUNYq1ZFXk/mZp06KPv6us\nkTVr1uDuLFmyhI8//piOHTuum5bazZtKjh7rsR7rsR7rsR7rcX09Tt2vjTPec2HuNb4iTa0zs7lA\nV4Jaw3PcfWI1l/sjcEm43Avu3i8y7XrgatbXL/44SxvesWNHli1bRpMmTdh9992ZNm2aTmhJiOnT\np697U0nyKH/Jpdwll3KXXNOnT6dv3764e85XislVQV2voJ48F94asJeZNYtM6x25/2xljey00040\nadKEZs2a0aFDB9auXVvbcYqIiIgkRux7FM3sEGDz8OFfgB8Q7P0bCTwfjn/J3ZeZ2b3AgHDcde5+\nQ9hGIfABsGO47NPAaOAg4OJw/hJgd3efmyUOv+KKK3j11VdZuHAhu+yyC927d+fmm2/GrM477CIi\nIiLVZmaNZo/i3cCj4fCDcJwBF0XGd09bZoPerbuXA2cA34SjDgf+QdBJdKACGJqtkxjVsmVLfvjD\nHwIwe/Zs/v3vf+f+jEREREQagHzoKFYQdOayDRVp82fcBerubwF7EvzLywKCs52XEnQYe7v7hOoE\ns3TpUnbffXfat28PwPjx4/nmm2+qWEriFi32leRR/pJLuUsu5S656jN3sXcU3X17dy+sZChy9xfD\nec+JjL8hQ1vz3f0Cd+/q7s3cvaO7H+/ur+QSU5MmTRg6dCgAq1at4u67766V5yoiIiKSJLF3FPNN\n6r+e99lnH/bdd18AXnzxRd566604w5Iq6My9ZFP+kku5Sy7lLrnqM3fqKFZi8ODBNG/eHIC77rqL\nkpKSmCMSERERqT/qKKZZunTpuvvt27dn4MCBAHz55ZdMmjQppqikKqq1STblL7mUu+RS7pKrUdUo\n5rvDDz+cnXfeGYDHHnuMOXPmxByRiIiISP1QRzFNqkYxxcwYPnw4hYWFVFRUMHLkSCoq0k/Elrip\n1ibZlL/kUu6SS7lLLtUo5plu3bpx4oknAvDJJ5/w+OOPxxyRiIiISN1TRzFNtEYx6tRTT2WrrbYC\n4P777+err76qz7CkCqq1STblL7mUu+RS7pJLNYoxMTM6deqEmTFp0iRmzZq1blrTpk0ZNmwYACUl\nJYwePZo/YBDHAAAgAElEQVS4//5QREREpC7F/l/P+cLMvDqvxR133MGzzz4LwK9//Wt+9rOf1XVo\nIiIiIhtoTP/1nCiDBg2iVatWAPzlL39h1apVMUckIiIiUjfUUUxT1XH/1q1bc/755wOwfPly7rvv\nvroPSqqkWptkU/6SS7lLLuUuuVSjmOf69OnDHnvsAcDTTz/NBx98EHNEIiIiIrVPNYqh6tYopixa\ntIjhw4dTWlpKly5d+POf/0yTJk3qMEIRERGRgGoU89xWW23F6aefDsCCBQv4+9//HnNEIiIiIrVL\nHcU0uRz3P+644+jWrRsAkydP5vPPP6+boKRKqrVJNuUvuZS75FLukks1iglRVFTE8OHDMTPKysoo\nLi7WtRVFRESkwVCNYijXGsWosWPHMm3aNAAuuugiDjnkkNoMTURERGQDqlFMkLPPPpv27dsDcM89\n97BixYqYIxIRERHZdOoopqnJcf/NN9+cIUOGAPDdd99x991313JUUhXV2iSb8pdcyl1yKXfJpRrF\nBPrpT3/KvvvuC8ALL7zAW2+9FXNEIiIiIptGNYqhTalRTPn6668ZOnQoq1evpmPHjowaNYpmzZrV\nUoQiIiIiAdUoJlD79u0ZMGAAAF9++SUPPvhgzBGJiIiI1Jw6imk29bj/4Ycfzs477wzA1KlTmTNn\nTi1EJVVRrU2yKX/Jpdwll3KXXKpRTLCCggKGDx9OYWEhFRUVFBcXU1FREXdYIiIiIjlTjWKoNmoU\noyZOnMgjjzwCwPnnn88xxxxTa22LiIhI46YaxYQ77bTT2GqrrQD461//yldffRVzRCIiIiK5UUcx\nTW0d92/atCkXXnghACUlJYwePVp/71eHVGuTbMpfcil3yaXcJZdqFBuIXr160a9fPwDeeOMNXnnl\nlZgjEhEREak+1SiGartGMeXbb79lyJAhrFy5ki222ILRo0fTokWLWl+PiIiINB6qUWwgWrduzXnn\nnQfA8uXLmTBhQswRiYiIiFSPOopp6uK4f9++fdljjz0AeOqpp/jggw9qfR2NnWptkk35Sy7lLrmU\nu+RSjWIDY2YMGzaMpk2bAlBcXExZWVnMUYmIiIhUTjWKobqqUYx65JFHmDhxIgBnnXUWp556ap2u\nT0RERBom1Sg2QMcffzxdu3YFYPLkyXz++ecxRyQiIiKSnTqKaeryuH9RURHDhw/HzFi7di2jRo3S\ntRVriWptkk35Sy7lLrmUu+RSjWIDtssuu3DEEUcAMGvWLJ599tmYIxIRERHJTDWKofqoUUz5/vvv\nGTp0KMuWLaNly5aMGTOGNm3a1Mu6RUREJPlUo9iAbb755gwZMgSA7777jnHjxsUckYiIiMjG1FFM\nU1/H/ffdd19++tOfrlvn22+/XS/rbahUa5Nsyl9yKXfJpdwll2oUG4nBgwfTvHlzAO666y7WrFkT\nc0QiIiIi66lGMVSfNYpR06ZNY+zYsQCceOKJDBw4sN5jEBERkWRRjWIjccQRR7DzzjsD8OijjzJn\nzpyYIxIREREJqKOYpr5rNgoKChg+fDiFhYVUVFRQXFxMRUVFvcbQEKjWJtmUv+RS7pJLuUsu1Sg2\nMt26deP4448H4OOPP+aJJ56IOSIRERER1SiuE1eNYkppaSnDhg1j8eLFNGvWjNGjR9OhQ4fY4hER\nEZH8pRrFRqZp06YMGzYMgJKSEkaPHq2/9xMREZFYqaOYJs6ajV69etG3b18AXn/9dWbOnBlbLEmj\nWptkU/6SS7lLLuUuuVSj2Iide+65tGrVCoCxY8eyatWqmCMSERGRxko1iqG4axSjnn32We644w4g\nuHzO0KFDY45IRERE8olqFBuxfv360bNnTwCefPJJPvzww5gjEhERkcZIHcU0+VCzYWYMGzaMJk2a\nADBy5EjKyspijiq/5UPepOaUv+RS7pJLuUsu1SgKW2+9NaeddhoACxYsYMqUKTFHJCIiIo2NahRD\n+VSjmFJWVsbFF1/MZ599RpMmTRg5ciTbbLNN3GGJiIhIzBpdjaKZHWNm/zKzr81stZn9z8xuM7N2\nObRRZGYjzOylsJ21ZrbSzGaZ2R/MrGNdPofaVlRUxIgRIzAz1q5dy1133aVrK4qIiEi9yYuOopld\nD0wFDgbaAk2BHYDLgDfNrLq70SYDfwb2D9spADYHdgd+AbxuZltU1kC+1WzssssuHH744QC8//77\nPPfcczFHlJ/yLW+SG+UvuZS75FLukqtR1Sia2QHAVYAD5cCVwPHAq+EsXYFx1Whnh3A5D4fRwCEE\nHcTycFwX4JTafQZ1r3///rRrF+xYHT9+PCtWrIg5IhEREWkMYq9RNLO/AScQdOTGufvgcHxnYD5g\n4bTu7p71OjFmtjfwRvjQgVbu/n04bRbBXkUHLnH3kRmWz7saxahXXnmFW265BYC+ffty2WWXxRyR\niIiIxKUx1Sj2idx/OXXH3RcCn0Wm9auindnAIoLOIMBtZtbPzC4DdgnHryQ4xJ04++67L/vssw8A\nzz//PO+8807MEYmIiEhDF2tH0czaAu1Y37lbnDZL9PEOlbXl7muAw4F3CPZCDgH+DdxG8Dz/Dezn\n7gsqaydfazbMjCFDhtCsWTMARo0axZo1a2KOKn/ka96kepS/5FLukku5S67GVKPYIrxN7TotTZse\nfdyyGu19C/wPqGB9raKH7e9HAusTozp06ED//v0BWLJkCQ899FDMEYmIiEhDVhTz+leFt6k9ipul\nTY8+/q6yhsysDcEJMB3D9gYBDwPdgCnAzsDVZrbM3e/M1MbAgQPp1q0b06dPp23btvTq1Ys+ffoA\n63vvcT8+8sgjmT59Oq+88gp/+ctf6N27N9ttt13exBfX49S4fIlHj3N7nBqXL/HocfUf9+nTJ6/i\n0WM9bqiPU/fnzZtHfcqHk1m+BrYg6Nyd4+4TI9PmE5yp7MBF7j6qknYGEZwd7cB77r5XZNpFwB3h\ntFfdff8My+f1ySxR8+bN4+KLL6aiooIdd9yR2267jYKCgrjDEhERkXrSmE5meT5y/4DUHTPbjqCT\nmGm+TH4Qud8qbVqbLPc3Eu2556tu3bpxwgknAPDxxx/zxBNPxBxR/JKQN8lO+Usu5S65lLvkqs/c\n5UNHMXUY2ICBZnalmR0LpArwHHjG3T8AMLN7zawiHK6JtPNepJ3tzWysmR1iZhcAl0bme73unkr9\nOe200+jUqRMAEydOZOnSpTFHJCIiIg1N7IeeAczsRuA3qYeRSU5wLcUDU2crm9m9wIBw2vXufkOk\nnWkEZz6nt5Nq60uCM5/nZoghMYeeU9555x2uuSboK++zzz783//9H2Z1vhdaREREYtaYDj3j7lcT\n/KvKc8ByYA3wCfBH4McZLmmTrUd3LDAMeBH4GigDvgf+A/wJ2DNTJzGp9txzT/r27QvAa6+9xsyZ\nM2OOSERERBqSvOgoArj7P9z9EHdv7+7N3X0nd/+Vu3+dNt857l4YDjekTSt39zHu3tfdf+DuTd29\npbv3cPdfunv6dRo3krSajXPPPZdWrYKSzLFjx7Jq1aoqlmiYkpY32ZDyl1zKXXIpd8nV2GoUZRO0\nadOGc889F4Bly5YxceLEKpYQERERqZ68qFHMB0msUUxxd6666iref/99zIzf//737LrrrnGHJSIi\nInWkUdUoyqYxM4YNG0aTJk1wd4qLiykrK4s7LBEREUk4dRTTJLVmY+utt+a0004D4LPPPmPKlCkx\nR1S/kpo3CSh/yaXcJZdyl1yqUZQaOeGEE9h2220BeOihh/jiiy9ijkhERESSrNo1ima2E9ALWOvu\nj5pZIXArcDrQDLjP3S+rs0jrWJJrFKM+/PBDLr/8cgB69uzJb3/7W11bUUREpIHJxxrFy4AHgRHh\n40EE/3jSCWgLXGxmw2o3PMnVrrvuyhFHHAHA+++/z3PPPRdzRCIiIpJUuXQUfxTePh3enhjeWuT2\njNoIKk4NoWajf//+tGvXDoDx48ezYsWKmCOqew0hb42Z8pdcyl1yKXfJla81ituGt5+Et70I/iFl\nH4JD0AC6JkseaNGiBYMHDwZg5cqVjB8/PuaIREREJIlyqVEsBQqBfsA7wDfAcndvb2Z9gWeBMndv\nWlfB1qWGUqOY4u7cdNNNvPbaawDceOON9OrVK+aoREREpDbkY43i6vB2N+An4f3/hbctwttvayMo\n2XRmxpAhQ2jWrBkAo0aNYs2aNTFHJSIiIkmSS0fxv+HtHcBUgsPO74Tjtglvl9RSXLFpSDUbHTp0\noH///gAsXryYhx56KOaI6k5DyltjpPwll3KXXMpdcuVrjeIDBCesFAGbh+Mmhbd9wts3aicsqS1H\nHnkkO+64IwBTpkxh3rx58QYkIiIiiZFLjaIB1wEnACuAMe5+fzjtEaAlcJe7P143odathlajGDV3\n7lwuueQSKioq2GmnnfjDH/5AQYGutS4iIpJU9VWjWO2OYkPXkDuKAPfddx9///vfARg8eDBHHXVU\nzBGJiIhITeXjySzrmNmOZnacmZ1d2wHFraHWbJx++ul06tQJgAkTJrB06dKYI6pdDTVvjYXyl1zK\nXXIpd8mVrzWKmFlXM5tOcGLL34F7zay5mX1oZp+a2Z51EaRsus0224wLL7wQgJKSEsaMGRNzRCIi\nIpLvcqlR7EBwlvPWrP83Fnf3QjObChwN3OTu19RJpHWsoR96Trn99tvX/RL5zW9+w7777htvQCIi\nIpKzfDz0fCXBZXAMWJs27YVw/EG1FJfUkfPOO49WrVoBMGbMGFatWhVzRCIiIpKvcukoHk1w7cS/\nAYemTZsf3m5LwjX0mo02bdowaNAgAJYtW8bEiRNjjqh2NPS8NXTKX3Ipd8ml3CVXvtYopjqB44Cy\ntGnfhLc/2OSIpM4ddNBB9OjRA4CnnnqK//73v1UsISIiIo1RLjWKKwiulXg68DnwEutrFC8AxgDf\nuHu7ugq2LjWWGsWUzz//nBEjRrB27Vq6du3KHXfcQVFRUdxhiYiISDXkY41iarfTr4HOqZFm9kPg\nVwSHpT+svdCkLm2zzTaceuqpAMyfP59HH3005ohEREQk3+TSUfw7wQkrvVj/130GfATsED7+W+2F\nFo/GVLNx4okn0qVLFwAefPBBvvjii5gjqrnGlLeGSPlLLuUuuZS75MrXGsWRwH+IXBonHFKPZwN3\n1V5oUteKiooYMWIEAGvXrmXUqFE0psPvIiIiUrmc/sLPzNoTdAZPAArD0eXAFGCYuyf27z4aW41i\n1F133cVTTz0FwKWXXkq/fv1ijkhEREQqk9f/9WxmbYCdwof/c/cVtRpVDBpzR3HVqlUMHTqU5cuX\n06pVK0aPHk2bNm3iDktERESyyMeTWdZx9xXu/kY4JL6TGNUYazZatGjB4MGDAVi5ciXjx4+POaLc\nNca8NSTKX3Ipd8ml3CVXfeau2tdDMbNq/TWfu99Q83AkLvvttx8/+clPeP3113n++efp168fvXr1\nijssERERiVEu11GsIDh5pVLuXljVPPmoMR96Tvnqq6+48MILKSkpYauttqK4uJimTZvGHZaIiIik\nyddDz5ZhIO2+JNQPfvADzj77bAAWLVrEQw89FHNEIiIiEqdcOooTMgxPAisJ9jR+BCT+j4Mbe83G\nUUcdxY477gjAlClTmDdvXrwBVVNjz1vSKX/Jpdwll3KXXHl5HUV3PyfDcBTQDXg/vNV1FBOuoKCA\n4cOHU1BQQHl5OcXFxVRUVMQdloiIiMSgRpfH2agRs2EEF+R+xt1/vskNxkA1ihu69957mTJlCgBD\nhgzhyCOPjDkiERERScnXGsVsfhTe7ldL7UnMTj/9dDp16gTAhAkTWLo0sddSFxERkRqqdkfRzJ7L\nMLxoZp8C/cPZyuomzPqjmo1As2bNuPDCCwFYvXo1Y8eOjTmiyilvyab8JZdyl1zKXXLlZY0i0Ac4\nMG3Yn6A2EYITWp6uxdgkZnvttRcHHnggAK+++iozZ86MOSIRERGpT7leR7EyzwBnu/uXmxxVDFSj\nmNmKFSsYMmQI3333He3atWP06NFsvvnmcYclIiLSqOXdfz2b2YAMox1YTvB/zx/VZmD1TR3F7J55\n5hnuvPNOAI488kiGDBkSc0QiIiKNW96dzOLuEzIME9398aR3EqNUs7Gxgw8+mB49egDw5JNP8tFH\n+Zdu5S3ZlL/kUu6SS7lLrnytUZRGyswYNmwYRUVFuDsjR46krCzx5y2JiIhIFbIeejaz52rQnrv7\nQZsWUjx06LlqDz30EA888AAAAwYM4KSTToo5IhERkcYp9hrF8OSVXHpORtBRLKyNwOqbOopVKysr\n46KLLmLBggU0bdqU4uJittpqq7jDEhERaXTypUbRchgaBNVsZFdUVMTw4cMBKC0tZdSoUeRL51p5\nSzblL7mUu+RS7pKrPnNXVMm0c+otCkmM3XbbjcMOO4ynn36a9957j+nTp9O3b9+4wxIREZE6UCv/\n9dwQ6NBz9a1atYqhQ4eyfPlyWrVqxZgxY2jdunXcYYmIiDQa+XLoWWQjLVq04IILLgBg5cqVjB8/\nPuaIREREpC7k1FE0s9Zm9gsze8TM/p3hv5+fratA64tqNqpn//3358c//jEAzz33HO+9916s8Shv\nyab8JZdyl1zKXXLlS43iBszsB8CrrP9v541mIbezpCXBzIyhQ4cya9YsSkpKGDVqFMXFxTRt2jTu\n0ERERKSW5PIXfn8ELsky2dHlcRqlxx57jHHjxgFw8skn079//5gjEhERafjysUbx5wQdwqfDxw7c\nCowP7z8PDKrV6CTvHX300ey4444ATJkyhXnz5sUbkIiIiNSaXDqKXcPbsZFx/3D384E/AH2AklqK\nKzaq2chNQUEBw4cPp6CggPLycoqLi2O5tqLylmzKX3Ipd8ml3CVXvv7Xc5PwdhlQHt5vGd4+S3Do\n+de1FJckyPbbb8+xxx4LwEcffcRTTz0Vc0QiIiJSG3KpUVwM/AA4FHgQaA/cDwwFbgFGAKvdvUWN\nAjE7BhgO7A1sDiwA/gHc7O7LcmyrDzAM2C+M81tgPjAD+IW7l2dYRjWKm6CkpIRhw4bx5Zdf0rx5\nc0aPHk379u3jDktERKRByscaxc/D29bA+wR7EM8CVhJ08AC+qEkQZnY9MBU4GGgLNAV2AC4D3jSz\nbXJo61bgOeBEYEuCPaHtgb0IOrOb1SRGqVyzZs248MILAVi9ejVjx46tYgkRERHJd7l0FN8j6Bz+\nELgvMj71X88O3JtrAGZ2AHBVuHw5cCVwPMGleCCojRxXzbbOA34ZtvUtcDNwLHAYMBh4gPWHzTNS\nzUbN7b333vTu3RuAmTNn8uqrr1axRO1R3pJN+Usu5S65lLvkysvrKAK/BSYD8939v2a2E3Ap0AJY\nBYwEfleDGC5mfUfzHne/FcDM3iY4XGzAoWa2q7t/mK0RMysCrouMOsbdX0ybrVodTqm5888/n7fe\neotVq1YxZswYevbsyeabbx53WCIiIlIDm/Rfz2HnrL27L9mENpYC7Qg6igPd/a+RaXMJ9ig6cJG7\nj6qknZ8BL4bzLiTY63lquPwSgkPb17n7N1mWV41iLXnmmWe48847ATjqqKMYPHhwzBGJiIg0LHlX\no2hmfzWzA6Pj3L1sEzuJbVnfSQRYnDZL9PEOVTTXM3K/C3A1sCNBvWMX4CJghpm1rmm8Uj0HH3ww\n3bt3B+CJJ57go48+ijkiERERqYlcahTPBJ4zs0/M7Eoz27oW1p86QzrVIy5Nmx593JLKtY3cd+A/\nwHHA+QQn3DiwC3BFZY2oZmPTmRnDhw+nqKgId6e4uJiysrI6XafylmzKX3Ipd8ml3CVXvtYopmxH\nUK94g5k9TVD3Ny3TJWeqYVV4m9qjmH5GcvTxd1W0lbrYd6re8TJ3fwbAzHYkuMajA0cAv8nUwMCB\nA4EgAW3btqVXr1706dNn3ThAj6v5+OOPP6ZHjx688847zJs3j5tuuokDDzywztb37rvv5tXz1+Pc\nHit/eqzH9f84JV/i0ePKH6fuz5s3j8WL0w/A1p1crqN4DXAawV65lNTCXwETCE5Gyek4o5l9DWwR\ntnWOu0+MTJtPcNi4OjWKJwKPROLaxd0/DqddAIwJx3/m7ttlWF41irVs7dq1XHzxxSxYsICmTZtS\nXFzMVlttFXdYIiIiiZd3NYrufoO770ZwQew/EVxXMXVpnI4El6X5Tw1ieD5y/4DUHTPbjqCTmGm+\nTF5mw0vfdMtyf35u4UlNNWnShOHDg0tslpaWctddd8Xy934iIiJSM9XuKKa4+zvu/gtgW6Af8HQ4\nKdVpzNWdkeUHhvWPxwIPpVYJPOPuHwCY2b1mVhEO10TiWgL8LdLWbWZ2jJkNIvj3mJQHKgsmfZe8\nbJrddtuNww47DAgOL9bV66u8JZvyl1zKXXIpd8lVn7nLuaMIEJ45fA7BmcWHsv4QdM7Cax3eFLZR\nEN5/FPhxOG4+wQkpGy2aYdxFwH/DaT0ILokzDmgTjks9lno0cOBAtthiCwDGjRvHt99+G3NEIiIi\nUh251Cg2BY4kOPv5CNafaJLai7gWeNzdT6pRIMF/PY8g+Ku91H89Pwb8zt2/jsx3L9A/fHi9u9+Q\n1k4r4HKCv/DrBpQBs4F73f3uStavGsU69PLLL/P73/8egIMOOohLLrkk5ohERESSq75qFHPpKC4j\n2DMHGx5i/gC4B5jo7ktrN7z6o45i3XJ3brzxRt544w0AbrrpJnr27FnFUiIiIpJJ3p3MQnCdwlQd\n4kpgPLCfu3d39z8muZMYpZqNumFmDBkyhM02C3ZEFxcXU1qaftnMmlPekk35Sy7lLrmUu+TK5xrF\nGcAgYCt3P9/dX62DmKSB6tixI2effTYAixYtYvLkyTFHJCIiIpXJ5dDzTu7+vzqOJzY69Fw/ysvL\n+eUvf8knn3xCYWEhf/7zn+natWvcYYmIiCRK3h16bsidRKk/hYWFDB8+HDOjvLyckSNH6tqKIiIi\neapGl8dpyFSzUfd22GEHjjvuOAA++ugjnnrqqU1uU3lLNuUvuZS75FLukiufaxRFasUZZ5xBx44d\nAZgwYQJff/11FUuIiIhIfat2jWJDpxrF+vfWW29x3XXXAbDffvtx5ZVXxhuQiIhIQuRdjaJIbdt7\n773p3bs3AK+88gqvvfZazBGJiIhIlDqKaVSzUb/OP/98WrRoAcDo0aP5/vvva9SO8pZsyl9yKXfJ\npdwll2oUpdFo27YtgwYNAuDrr7/m/vvvjzkiERERSam0RtHM7gnv3uTun4bjjgnHveju34Tj9gDu\nBdzd967DeOuMahTj4+785je/Yfbs2ZgZt912GzvttFPcYYmIiOStfKlRHAgMADpFxk0FHgV2i4xr\nCfQKB5GcmBnDhg2jqKgId2fkyJGUlZXFHZaIiEijp0PPaVSzEY/OnTtzyimnADBv3jymTp2a0/LK\nW7Ipf8ml3CWXcpdcqlGURumkk06ic+fOADz44IMsWrQo5ohEREQat6pqFCsABw5w91cqGbc/8BJB\njWJhnUddB1SjmB/+85//cMUVVwDQq1cvbrjhBszqvARDREQkUfKlRrEy6lVJrdt99935+c9/DsC7\n776rQyMiIiIxqm5H8REzm2NmcyLj/hYZ93AdxBYLdUziN3DgQNq2bQvAuHHjWLlyZZXLKG/Jpvwl\nl3KXXMpdcuVjjeKWQLdw8AzjtqzdsKQxa9myJRdccAEA3377LePHj485IhERkcapOjWKuVCNotQK\nd+eGG27gzTffBOCmm26iZ8+eMUclIiKSH+qrRrGqjuK1uTbo7tdvUkQxUUcx/3z55ZdceOGFrFmz\nhq222ori4mKaNm0ad1giIiKxy4uOYmOS6ihOnz6dPn36xB2OhKZOnbru0PMpp5zC2WefnXE+5S3Z\nlL/kUu6SS7lLrunTp9O3b9+8P+tZpM4dffTR7LDDDgD8/e9/Z/78+TFHJCIi0njUeI+imf0E2Ieg\nsznT3V+vzcDqmw49569PP/2USy+9FHdnl1124dZbb9W1FUVEpFHLi0PPZnYkcCrBmc4XuvuqcPwd\nwIi02R9w9/51FWhdU0cxv40fP37d3/oNHTqUI444glmzZjFr1ixmz57NihUraNOmDd27d6dHjx70\n6NEj5ohFRETqTr5ccPt44Cyge6STeCBwEWBpw5lmNqAOY60Xuq5UfjrzzDPp2LEjABMmTGDZsmX0\n6NGDM844A3fn7bffxt0544wz1ElMIL3vkku5Sy7lLrny6TqKPQn2Jj4aGZfaa+hAKfDfyLTMZxqI\nbKJmzZoxdOhQAL7//nvGjh0bc0QiIiINX1Udxa3D23cj4w6M3O/v7rsBowj2Kib+Qnc6Ayx//ehH\nP+KAAw4A4JVXXuG1115bN61Dhw5xhSW1QO+75FLukku5S676zF1VHcV24e1KADNrC2wfjvue9Xsa\np4W3bWs1OpE0559/Pi1atABgzJgxrF69OuaIREREGq7qXh6nc3j7s/DWgTfcfW34uCy8rfpPefOc\najby2xZbbME555wDwNKlS/nrX/+67r4kl953yaXcJZdyl1z5VKO4ILz9pZkdBfxfZNqMyP2u4e2X\ntRWYSDaHHnoou+++OwDTpk1j2bJlMUckIiLSMFV1eZxi4EKCPYjrRoePf+zub4fz3QsMAJ5096Pq\nLty6o8vjJMuCBQu46KKLKCsrY9GiRXTq1ImePXtyyy23xB2aiIhIncuXy+PcDCxlw8vgAEyJdBKb\nA8cRdB5fqKM4RTbQpUsXTj75ZEpKSpgzZw5vvPEGM2bMYMmSJXGHJiIi0mBU2lF09y8I/n1lIvAh\n8CpwNXBGZLbewGzgFeCpugmz/qhmIzl69+7NJ598QklJCd988w1z585lyJAh6iwmkN53yaXcJZdy\nl1z5VKOIu89194Huvru77+fuN0VOYsHd/+nuB4TD7LoNV2S9CRMm0KVLl9Tud5YtW8aHH37IHXfc\nEfTioSoAACAASURBVHdoIiIiDUKN/+u5oVGNYvKMGDGCefPm8eabb7J8+XIKCwtp2bIlLVu25Lrr\nruOkk06iefPmcYcpIiJS6+qrRrGoiiB659qgu79Y83BEqm/LLbfk008/pUWLFpSVleHuuDtNmjTh\n4Ycf5plnnuHss8/moIMOoqCguleCEhERkZSqvj2nA8/nMDxXV4HWF9VsJMd5553HZpttRkVFBWVl\nZbRu3ZqePXuuu2L98uXLufPOO7n00kuZPVtVEflM77vkUu6SS7lLrryqUQwZG5/5nG0ekXrRqVMn\nxowZQ+fOndlss83o3LkzDzzwAKNHj+ayyy6jffv2AMyZM4crr7ySW265hUWLFsUctYiISHJUdR3F\nCtZfQ7E6nUB398LaCKy+qUYxua688kpmz55N9+7dN7iOYklJCY8++ih/+9vfKC0tBaCoqIhjjjmG\nU045Zd1fAYqIiCRNvlxHMWoFcAewo7sXZBkS2UmUhqlZs2acfvrpjB07lr59+wJQVlbGlClTGDx4\nME8//TTl5eUxRykiIpK/quooHg38O7zfBrgY+MjMppnZz+s0spioZiOZKvuv5w4dOnDZZZdx++23\ns+uuuwKwYsUKRo0axSWXXMK7775bX2FKFnrfJZdyl1zKXXLlTY2iuz/h7j8HdgNGA6vCZY4AnjSz\nj8xshJm1qvtQRTbNTjvtxO9//3suv/xyOnbsCMC8efO4+uqrueGGG/j8889jjlBERCS/5HQdRTNr\nDQwChgE7hKMduN7db6j98OqPahSTK1uNYmVKS0uZOnUqjzzyCCUlJQAUFhZy1FFHcdppp9GyZcu6\nDFlERGST5GONIsBKYC6wkKCD6OhMZ0mgpk2bcsoppzB27FgOOeQQzIzy8nIee+wxLrjgAqZNm0ZZ\nWVncYYqIiMSqWh1FM9vCzC4H5gBTCP7f2YDlwK3AuDqLsJ6pZiM5Zs2axaRJkzAzCgsLMTMmTZrE\nrFmzqt1Gu3btuOiii7jjjjvo3r07ACtXrmTs2LGMGDGCt956q67Clwi975JLuUsu5S656jN3Vf0z\nyx7ACOB0oBnr9x6+CxQDk9y9pE4jFMmiR48e9OjRAwjeNKkLbdfE9ttvz80338zMmTO59957Wbx4\nMQsXLuS6665jr7324txzz2XbbbetpchFRESSobrXUTRgLcHexGJ3n1E/4dUf1ShKytq1a3n88ceZ\nPHky33//PQAFBQUcdthhnHnmmbRu3TrmCEVEpLGrrxrFXC64vRr4tor23N23qaXY6pU6ipJuxYoV\n3H///fzzn/8ktW20aNGC008/nSOPPJKiokp3yIuIiNSZfDyZpTnQCdgywxAdn2iq2UimushbmzZt\nGDZsGHfeeSe9evUCYNWqVYwbN45hw4bx2muvoR8XtUPvu+RS7pJLuUuuvLmOYqi6//Ms0iB169aN\nG264gWuuuYZttgl2mH/xxRf89re/5aqrrmLevHnxBigiIlJHqjr0PCDXBt19wiZFFBMdepbqKCsr\n48knn2TSpEmsWrUKCHb/H3rooZx11lm0bds25ghFRKQxyIsaxcZEHUXJxcqVK3nwwQd54oknqKio\nAKB58+aceuqpHHPMMTRp0iTmCEVEpCHLxxrFRkE1G8lU33lr1aoVF1xwAcXFxey9994ArF69mvvu\nu4+hQ4cyY8YM1S/mQO+75FLukku5S658q1GsF2Z2jJn9y8y+NrPVZvY/M7vNzNrVsL0WZvaJmVVE\nht61Hbc0bl26dOG6667juuuuo0uXLgAsWbKE3/3ud1x55ZV88sknMUcoIiJSc3lx6NnMrgeuDh9G\nAzJgHnCAu3+eY5t3A+emtdfX3V/MMr8OPcsmKS8v55///Cf3338/K1euBIJDA/369aN///60a1ej\n3zwiIiIbaTQ1imZ2ADA9fFgB/B/wX+DXwL4EHb1/ufvhObR5FPAPgms/pv5RxlFHUerBd999x+TJ\nk3n88ccpLy8HoFmzZpx00kkcf/zxNG3aNOYIRUQk6RpTjeLFrL+8zj3ufqu7/wM4lfX/CnOome1a\nncbMrAPBf087QWczpxdRNRvJlE95a9myJeeeey533XUX++yzDwAlJSXcf//9DB48mBdffFH1i2ny\nKX+SG+UuuZS75GpsNYp9IvdfTt1x94XAZ5Fp/arZ3t1AR+AZ///27jxOqurM//jnobuhgRB2GlF2\nlaAQt3GLiorjGjWaSNxFjSJmMo5mxhiymfgLJr+YOI5Rg0hcozE/SRiXaFyi6IhLdGIUBFHZZNcW\nQaDp/fn9cW513y6qF7CpW5f+vl+velF3rVN9uFVPnfOcc91vyZzusxRQZHsMGjSIH/zgB0ydOpVh\nw4YBUF5ezg033MB3vvMdFi5cmGwBRUREWpFo17OZ9QLW0RjIneDuT8e2vwwcHG3/L3f/divnu5jQ\nmrgOGOPua7JuQ6iuZ0lEfX09Tz/9NPfddx8bNmxoWH/UUUcxceJE+vXrl2DpREQkbTpK13P36N/M\nG63O2h5f/lxLJzKzYcB/EoLCye6+ph3KJ9IuOnXqxPHHH8/06dM544wzGu4TPXv2bC677DLuv/9+\nKisrEy6liIhIU8UJv/7m6N9MU16XrO3x5U2tnOsWQjD5gLvP3J7CXHjhhUC4ZVuvXr3Yd999Oeqo\no4DGfAAtF+byTTfdlJr6mjhxIj169OAvf/kLq1evprq6mltuuYV7772Xa665hqOPPprnn3++YMqb\nj+U01Z+Wmy5nnhdKebTc9uXMukIpj5ZbXs48X7p0KWvW5K8trBBGPX8M9CYEixe5+72xbcuAwdG2\nK9z91hbO8wawTwsvlRkYA9DL3T/NOt7dndmzZzdUjqRHWutt3rx5zJgxg0WLFjWs22OPPbj00ksZ\nPbpN47d2CmmtP1HdpZnqLr1mz57N0Ucf3WGmx5kJfJUQyN3p7pdG64cDmW9PB8a6+/wWzvMG8MVc\nm2LnyOjdXKAokm/uzrPPPsu9997LunXrGtYffvjhXHTRRQwYMCDB0omISCHqSPMojgNmR4t1wI+A\n+cD3gAMJAd7T7n5CtP9dwMRo/x+7+3XR+rOB/jle4iYag8RbgfeB37h7TVY5FChKoiorK5k5cyaz\nZs2iujqk55aUlHDaaacxYcIEunbtmnAJRUSkUHSUwSxEo5CnEoK5TtHzWTQGicuAS3MdmnWe37v7\nzdmPaHPmDzkzWl9DM+K5AJIeO0O9lZaWct555zFt2jTGjQt3m6ypqeGhhx5i0qRJPP3009TX1ydc\nyh1jZ6i/jkp1l16qu/TKZ90lHigCuPsPgdOBZ4FPgCpCy9+NwIHuvjz7kG05fewhUvD69+/P1Vdf\nzQ033MCoUaMAWL9+PTfffDNXXXUVb731VsIlFBGRjiLxrudCoa5nKUTuzvPPP88999xDeXl5w/pD\nDjmEiy++mF122SXB0omISFI6TI5ioVCgKIWsqqqKWbNmMXPmTKqqqgAoLi7mlFNO4cwzz6R79+6t\nnEFERHYmHSZHsdAoZyOddvZ669KlC2eddRa3334748ePB6C2tpZZs2YxadIknnjiCerq6hIu5fbb\n2etvZ6a6Sy/VXXp1uBxFEWmbvn37ctVVV3HjjTc2zLP46aefctttt3HFFVfwxhtvJFxCERHZmajr\nOaKuZ0kbd2fOnDncddddfPjhhw3rDzzwQC6++GJ22223BEsnIiI7knIU80yBoqRVdXU1jzzyCH/4\nwx8a7hddVFTESSedxNlnn02PHj0SLqGIiLQ35SgmRDkb6dSR661z586cccYZTJ8+nWOPPRYzo66u\njkcffZRJkybx6KOPUltbm3QxW9SR6y/tVHfppbpLL+Uoisg26927N1dccQU33XQTY8eOBWDTpk1M\nnz6db33rW7z22muo1VxERLaFup4j6nqWnYm78+qrr3LnnXeyevXqhvX77bcfl1xyCUOGDEmwdCIi\n8lkpRzHPFCjKzqimpobHHnuMBx98kIqKCiB8uJx44omcc8459OzZM+ESiojI9lCOYkKUs5FOqrfc\nSkpKOP3005k+fTonnXRS5oOFxx9/nMsuu4xZs2YVRP6i6i+9VHfppbpLL+Uoiki76tmzJ5dffjm/\n/vWv2XfffQHYvHkzd955J9/85jd55ZVXlL8oIiJbUddzRF3P0lG4O6+//jq//e1vWblyZcP6L37x\ni1xyySUMHz48wdKJiEhbKEcxzxQoSkdTW1vLE088wQMPPMCmTZuA8MFz7LHHct5559G7d++ESygi\nIs1RjmJClLORTqq3bVdcXMwpp5zC9OnTOeWUU+jUqRPuzlNPPcWkSZOYOXMm1dXVeSmL6i+9VHfp\npbpLL+Uoikje9OjRg0mTJnHrrbdy4IEHAlBZWck999zD5Zdfzosvvqj8RRGRDkpdzxF1PYsEb7zx\nBjNmzOCDDz5oWLfXXntx6aWXsvvuuydYMhERyVCOYp4pUBRpVFdXx5NPPsnvfvc7Nm7c2LD+mGOO\n4YILLqBPnz4Jlk5ERJSjmBDlbKST6q19FRUVcdJJJ3HHHXdw+umnU1xcDMBf//pXJk2axIMPPkhV\nVVW7vZ7qL71Ud+mluksv5SiKSEHo3r07F198MbfddhuHHHIIAFVVVdx///1MnjyZ559/XvmLIiI7\nMXU9R9T1LNK6t956ixkzZrBkyZKGdaNGjeLSSy9l1KhRCZZMRKRjUY5inilQFGmb+vp6nnnmGe69\n9142bNjQsH7cuHFceOGF9O/fP8HSiYh0DMpRTIhyNtJJ9ZY/nTp14rjjjmP69OmcccYZlJSUAPDC\nCy8wefJk7r//fiorK7fpnKq/9FLdpZfqLr3yWXfFeXslEdmpdOvWjYkTJ3LCCSdw99138+KLL1Jd\nXc2DDz7IU089xQUXXMD48eMxy/2Dd+7cucydO5d58+Yxf/58nnrqKcaMGcPYsWMZO3Zsnt+NiIjk\noq7niLqeRT6bt99+mxkzZvD+++83rNt999255JJL2HvvvZs9bsqUKcybN48xY8bws5/9LB9FFRFJ\nPXU9i0iq7L333tx4441ceeWVDfMsvv/++3z3u9/l5z//OWvXrk24hCIisq0UKGZRzkY6qd4Kg5lx\nzDHHcPvtt3PWWWfRuXNnAObMmcPkyZO55557qKio2Oq48vLyfBdV2omuvfRS3aWX5lEUkVQrLS3l\n3HPPZdq0aRx55JEA1NbWMnPmTCZNmsSTTz5JfX19wqUUEZHWKEcxohxFkR1n4cKF3HHHHSxcuLBh\n3bBhw/jqV7/Ktddey/Llyxk8eDAPPfQQZWVlCZZURCQdNI9inilQFNmx3J0XXniBu+++m/Lycior\nK5k3b17DrQB79OjB/vvvz7Rp0xQsioi0QoNZEqKcjXRSvRU+M+PII49k2rRpnHfeeaxatYq6ujqq\nqqrYuHEj5eXlvPrqq3z9619nxowZPPfccyxdupTa2tqkiy4t0LWXXqq79NI8iiKy0+rSpQtnnnkm\nzzzzDC+99BKbN29u2FZXV8cHH3zAww8/3LCupKSEYcOGMXz4cEaOHMmIESMYNmwYpaWlSRRfRKRD\nUddzRF3PIvk1depU5syZw8KFC1m/fj1dunShV69e9OrVi7KyshZbEs2MQYMGMXLkyCYBZM+ePfP4\nDkREkqMcxTxToCiSX2vXrmXy5Mm8+eabbN68me7du7PPPvswbdo0+vbty/Lly1m8eHGTR66pdeL6\n9OnTEDRmHmVlZc3eHUZEJK0UKOZZJlCcPXs2Rx11VNLFkW2kekuntWvXMmHCBN5991323HPPFkc9\nuztr165l8eLFLFq0iCVLlrBo0SLWrVvX4mt069atSeA4YsQIBg8eTHGxMm/ag6699FLdpdfs2bM5\n+uij8xIo6pNSRBJTVlbGYYcdRk1NDYcddliLo53NjIEDBzJw4EC+9KUvNazfsGFDQ/CYaXlctWoV\nmR/BFRUVzJs3j3nz5jUcU1xczNChQxsCx5EjRzJs2DC6du26496siEgKqUUxoq5nkWTsiHs9V1ZW\nsmTJkoZWx8WLF7Ns2TJqamqaPcbM2GWXXZq0PI4cOZJevXq1S5lERNpTvrqe1aIoIjud0tJSRo8e\nzejRoxvW1dbWsmLFiq3yHjOjrt2dVatWsWrVKl588cWG43r37t0QNGYGzgwcOFB5jyLSIahFMaIc\nxXRTvaXP3LlzmTt3LvPmzWP+/PnstddejBkzhrFjxzJ27Ni8lMHd+eijj5p0Wy9evLjVe0937dq1\nyWjrESNGMGTIkA6Z96hrL71Ud+mlHEUR2enFA8KkvrDMjAEDBjBgwAAOPfTQhvWffvppQ9CYGTiz\nYsWKhrzHLVu2MH/+fObPn99wTHFxMUOGDGkSQA4fPpxu3brl/X2JiLQXtShGlKMoIi2prKxk2bJl\nTQLIZcuWUV1d3eJx2XmPI0aMoHfv3uq6FpHPRNPj5JkCRRHZVnV1daxcubLJdD2LFy9m06ZNLR7X\ns2fPreZ7HDRokIJHEWkzBYp5phzFdFO9pdvOVH/uTnl5+VZT9nz00UctHldaWsrw4cObBI9Dhw6l\npKQkTyXfPjtT3XU0qrv0Uo6iiEhKmRn9+/enf//+HHzwwQ3rN27c2KTVcfHixSxfvrwh77GyspIF\nCxawYMGChmOKiooYPHjwVl3X3bt3z/v7EpGOSS2KEXU9i0i+VVdXs3Tp0iYjrpcsWdJq3mNZWVmT\nuR6HDx9O37591XUt0oGo6znPFCiKSCGor69n5cqVTYLHRYsWsXHjxhaP+/znP98keMzkPXbq1Knd\nyxif2mjDhg307Nkz71MbiXR0ChTzTDmK6aZ6SzfVX8vcnY8//nir4PHDDz9s8bguXbowbNiwJpOF\nDx06lM6dO7dLuaZMmdJQd+11Vx3JH1136aUcRRERaWBm9OvXj379+nHQQQc1rN+0aRNLlixpMt/j\nBx98QH19PQBVVVUsXLiQhQsXNhzTqVMndttttyajrocPH06PHj3y/r5EpPCpRTGirmcR2RlUV1ez\nbNmyJgNnlixZQlVVVYvHDRgwYKu7zfTr16/FvMcdcZ9uEWkbdT3nmQJFEdlZ1dfXs3r16q3me9yw\nYUOLx/Xo0aPJaOuRI0cyaNAgioqKWLt2LRMmTGD58uUMHjyYhx56iLKysjy9I9leyi/deShQzDPl\nKKab6i3dVH/55+588sknW93nes2aNS0e17lzZ/r378/LL7/Mhx9+yKZNm+jVqxejRo3i+uuvZ9Cg\nQXTu3LnJo6ioSCOyC4zyS9NNOYoiIrJDmRl9+vShT58+HHjggQ3rN2/e3JD3mMl9XL58OXV1dUDo\n2p49ezbr1q1j8+bNVFdXs27dOt566y0uuOAC9thjj5yv1aVLl60CyC5dulBSUpJzW/Z+zR3b3PaS\nkhIFp7JTibcGx+8zv6MpUMyiVo10Ur2lm+qvcHTv3p0xY8YwZsyYhnU1NTV88MEHDcHj7bffzqef\nfgrQMP2OmTWbB+nuVFZWUllZuePfQExLQWdzAWprwWtr23fEdEQ7Sr9+/ZIugmyDTHrAlClTGn64\n5YMCRRERaVFJSQkjR45k5MiRAJSXlzNnzhwWLFjAxo0b6datG0OGDGHvvffm3HPPpaqqipqaGqqq\nqqiurm7yaGlbru2fReac+VRcXNzmVtFtCV5banEtLt62r/K1a9cyZ84cli9fzoYNG1i7dq3yS6VZ\nBRMomtmpwLeAA4BuwHLgEeB6d1/XhuM/D1wAHAWMBQZE51kLvADc4O5vtnYe5Uqlk+ot3VR/6XLJ\nJZfw+uuvY2bU1dVRXFxM//79ue6669o14HB3ampqmgSOLQWguQLP+La2HpuZXmh71NbWUltbS0VF\nRbv9HVrTqVOnNrd6VlZWMnPmTFauXElFRQUVFRWcfPLJTJ48mb59+1JUVERRURHFxcUNz7MfxcXF\ndOrUabv3SVOra6EqLy/P22sVRKBoZj8BfhgtZkbXjAS+DXzVzI5w95WtnGY0cHPs+IxdgXOACWZ2\nqrs/2U7FFhHpkMrKypg2bRoTJkygoqKC3XbbjWnTprV7q5SZNQQ4+by/dW1tLTU1Nc0GoK21irYU\nvGYfm3leW1u73eWtr69vc9f+e++9x7p166ioqKC6uppNmzaxbNkybrjhhpz5pTuCmTUbYG5PoJor\nIG1rMNvc8W3ZJ/Mazb3ejsiRzbQGL1mypN3P3ZzEA0UzOwL4ASHAqwe+D7wDXAMcCgwFZgAntuF0\ndcCfgVmEFskDCAFod8J7vQVo8UpQq0Y6qd7STfWXPmVlZRx22GEN06vsTF2XxcXFFBcX07Vr17y9\nZn19fZtaPT9rl/57773XMNCnLfmlO4K7N7S87swyAXFLQWg80GwtMN6yZQt//OMfWb16dV5TKhIP\nFIF/A4wQKN7p7r8AMLO/A8uibceZ2Wh3X9DCeVYA+7n7vNi6Z82sFvhVtDzCzPq5e/7abEVERFrR\nqVMnSktLKS0t3aF3yZk6dSpz5szh3XffZePGjfTo0YM99tiDgw8+mKuvvpq6urpmH7W1tdTV1VFf\nX9/wPL4tvr6lfeLnau51ch3/WfZJQnsHxJnW4C1btuT1PRVCoHhU7PmLmSfuvsLMPiC0KAKMB5oN\nFKOu6Vzd0wuzlje3VBjlSqWT6i3dVH/plc9cKfnsMvmlmRbM+vp6SktLufzyy/PavZ9P7o675wx6\n2xKEtiUwbuk87RUYL168mNLSUiorK6mpqcnb3y/RQNHMegF9aMwrzJ7pdQ2NgeLI7XyZs6J/HXjG\n3bds53lERITG+dzMjLKyMsyMBx54QHf3SIF85ZcWEjNr6GovKSlJujjbLd4a/PHHH7N+/fq8vG7S\nLYqZny+ZrufsTvf48ue29eRm9h/AedHiBuCK1o5Rq0Y6qd7STfWXLgoI021nzi/dmcVbgzt37py3\n1006UMx0A2daFLtkbY8vb9qWE5vZr4CrosVPgBPcPbsbuokLL7yQYcOGAdCrVy/23Xffhi+w2bNn\nA2hZy1rWspa1nPrlZcuWkVEI5dFy68sAI0eO5KWXXmLz5haz6NpV4vd6NrOPgd6EYPEid783tm0Z\nMDjadoW739qG83UG7gW+Hh23HDjJ3Vu8343u9Zxuqrd0U/2ll+ounXSv5/TK1N0rr7ySl3s9d9rR\nL9AGz8WeH5F5YmbDCUFirv1yMrOewNM0BolvAoe2FiSKiIh0BHPnzuWBBx7YKr907ty5SRdNClQh\ntCiOA2ZHi3XAj4D5wPeAAwkB39PufkK0/13AxGj/H7v7ddH6/tF5RtPYkjgZ2Jj1knPd/dMc5fCk\n/xYiIiIiLZkyZQrz5s3jsccey0uLYtI5irj7C2Y2lRAYdgKmxjcT5lK8NNehWct7EYJECINjhgCP\n5zjuKMIt/URERERSIT7bwJAhQ/L2uoXQ9Yy7/xA4HXiWMPCkCngfuBE40N2XZx/S3KlaebR6A894\n0qikh+ot3VR/6aW6Sy/VXbqMHTuWc845h+uvv54JEybk7XUTb1HMcPdHgEfasN9FwEU51j8PFO2A\noomIiIh0SInnKBYK5SiKiIhIWphZhxn1LCIiIiIFSIFiFuVspJPqLd1Uf+mluksv1V165bPuFCiK\niIiISE7KUYwoR1FERETSQjmKIiIiIpIoBYpZlLORTqq3dFP9pZfqLr1Ud+mlHEURERERSZxyFCPK\nURQREZG0UI6iiIiIiCRKgWIW5Wykk+ot3VR/6aW6Sy/VXXopR1FEREREEqccxYhyFEVERCQtlKMo\nIiIiIolSoJhFORvppHpLN9Vfeqnu0kt1l17KURQRERGRxClHMaIcRREREUkL5SiKiIiISKIUKGZR\nzkY6qd7STfWXXqq79FLdpZdyFEVEREQkccpRjChHUURERNJCOYoiIiIikigFilmUs5FOqrd0U/2l\nl+ouvVR36aUcxQT94x//SLoIsh1Ub+mm+ksv1V16qe7SK591p0Axy/r165MugmwH1Vu6qf7SS3WX\nXqq79Mpn3SlQFBEREZGcFChmWbp0adJFkO2geks31V96qe7SS3WXXvmsO02PEzEz/SFEREQkNfIx\nPY4CRRERERHJSV3PIiIiIpKTAkURERERyUmBooiIiIjkpEBRRERERHJSoCgiIu3GzCz+PL4sIumj\nQFFECkp2oBH9q8+qlHB3N7PiaNFcU2ukhq699NqRdafpcaTDMDN9aaWAmfUDSoD+wBZ3fy+2TXVY\nwMxsHDABOBLYCKwDfgG87e7rkiybtE7XXnrtyLrr0IGimY0CPnX31UmXRdqXme0F7AYcDCwD3nT3\nN5MtlbTGzI4HrgbGAAMIgcZjwAzgdXevNLNO7l6fYDElBzM7Dbgf6ELT3qr1wH3APe7+9yTKJq3T\ntZdeO7ruOmygaGZnA78Ffgrc6e5rEi6StBMzOwX4JTAQ6AHUAbXRusfc/dUEiyfNMLOvALOa2fwP\n4EHgN+6+Ua0bhcXMjgOeAAx4B6gA9o/tUgE8Bfzc3f+W/xJKS3TtpVc+6q649V12PmY2gfDLF+AK\noMbM7lOwmH5RkPhwtFgLVAOdgSLgGuAAM7vN3R9LqIiSg5kdDtwbLf4FWEkI9L8crdsX6A2Umtmv\n3H1z/kspuZhZT+CbhCDxaeA7wALgcGAaMAzoBhwPbDSzj919UTKllWy69tIrX3XX4ZJUoxyazB+2\nktBMew1wvpkNTKxg8pmZ2d7AzdHiQ8DlwFnA4mhdMXAccJWZjc9/CSVbLAH7eKAUmA98F5jk7qcA\nU4HyaJ+hwNnAaRpJW1C6AwdFz9e7+5vuXu3uzwLnAa8BNUBX4FRC/qIGSSRM11565bvuOtSFambD\ngH8n5NC8BnwabeqDgsWdwWigjJBE/4C7/9bd/xs4FngS2EL4Pz8OONfM+idWUgEaRsh2Ar5ESMTu\nTQg2PNr+Q8JgiExr/yjgfMI1LIWhjJDiUQuUmdnumQ1RN/OVhC8ygJ7A98ysj3LdkqVrL73yM96n\n2AAAF2xJREFUXXcdKlAkfJAdQ4i0f0j4tbsq2pYzWDSz4thUD1LYxhJ+XX0O6Aeh1cLdlwCTgL8S\nWjaKgIsIF5kkrwvhww7CZ9IxZlaa2ejuvwRuJeSaOqFV+Lx8F1KaMrOi6OkiwoCxYsLgsTPMrFdm\nP3d/Dfg3oCpaNQAYkceiSvN07aVX3uquQwWK7r6C0Hf/LDDH3Z8BLiB3sNgvaqY9LrOcRJllm2yI\n/jXCRbO7u9dHCbzLgX8F5sb2/2b0Q6BDXQeFJKqbLYRrEkLr1CRgv3i9uPtU4G5C3Toh2JAEZFoM\n3b0uChargJejzV2AHwBnmtnno/3N3V8gDCaD8ENuUH5LLdl07aVTNId9p3zWXYf7gnT354Fz3X2z\nmRVHeTTns3WweClwDvAzwujo8xVQFCYzy/yqeomQzAshN/EiM+sfNdObuy8j/DCoiPbpC9SpCyw5\nsRF4bxFSAyC0Sv2MMPAo3pp/J7CJ8KGnL6sEmNlXgblm9iMIwaK7VwH/RZgGB8LAlV8Srr/BsQm4\ne0Tbl9H0B5skQNdeupjZ0Wa2X1Rvmbp7gzzUXYcMfKJfwubutdHyc2wdLH6XMHXOWEJFPKGAonCY\n2VfM7BYAd6+JVr9NmJojYwrwr2a2a+xDsRxYET1fH1sveWBmg83sIDM708wON7NdAdz9T4QRshnj\ngBuBE8ysc7RuOOEz61NgTj7LLQ1B4kxCq2FZrMWwyN3nEX6cVUe7dweuB641s4uibYdG2xbSGFRK\nnujaSy8z+xohdepPmR9fAO7+MPDr2K47pu7cvcM/aJxP8ghCi1Q94QOvnhBYjEq6jHo0qa+vRnVT\nD5wdresU/TsIWBLbXk/I0zic0OV1dGz7zwm/sizp99QRHsBJwP8SBhtl6uZl4NrYPndk1d1C4P8R\nWqxei9b9HRiU9PvpSI+sa+53wJBm9juV0GKf2bcq+ndD9O8qYI+k309He+jaS+8j69q7J/P3B7rk\nq+4S/yMUwiMTZETPv0topq0HPgZGJ10+PZrUVfyiuQ8YmmOfAwjdW/EL513C5KPvRMuLgGFJv5+O\n8gC+klUftdG/NdG/dwMl0b63ZW2LP1YAeyb9fjrSA/ha7O9/P7BLtH5MFIBMBL4AfC5afzDwKrA2\ndtzH0ReW6i7/9adrL6UPtv6Btltsm2Xte+uOqrsOe2eWbGbWFRgPfI/QRfIJcLi7L0i0YNIg1vUF\n8ADwHXdfFW3r7+4fRXmntRZuz/gQYVqAkqxTLQVOdPeFeSp6h2Zm/wQ8Ski4/iOhJWMoYT69L8Z2\nfcDdz4uOuYowrdE/E0bTLgXmAd929/fzVvgOzszOIlxrEILEq919jZn9H+AMwvUFoQXjGWBqtH1X\nQh2PI+RTvQX83d3X5vUNdHC69tLLzM4gtApCuLvKt6Nr62RC3R0APE+4rl6MjrmSMAC3fesu6Yi5\nUB5AL0L/fT3wIfCFpMukR5P6+TpNWzUGRusvA34fXRD/Qwj0vxBt60u4/+V/E77IZgM3AMOTfj8d\n6UGY+LyC8OPruGhdEWGqqmdo+sv3v2LH9SK0Wh1NuLtHj6TfS0d6APvR2FW5GZgYrf8NTVumMmk6\n6wkD/3omXXY9GupQ114KH4QfYZl6mQn0jdbfGNVlvN7+DlweO7Z3e9dd4n+QQnoQfmWtUpBYWA/C\nlEbxvJoB0fpb2LqJfT3hnrNjon0yreYDow/I4qTfT0d7AH+K6mYtsVQBQoL1foR7AMe7KE+M150e\nidXbIELu04qobjYR0j0y3VsLCTMNbI7V3xLgS9HxRbFzqS7zW3eZzz1deyl7RN9Vr8Tq5RlCfv2N\n0XJd1ndeHWEg5w6ruw456rk5HqbOGeHu77S6s+RFNCXRoYQPMQj5T6ea2VTC/WWd8KWVyaH4PGEi\n7YlROkHmlkVrPUzlUZu3wkvGUkL99CfMsQeAh1kE/gFcS+M8fJlfw3j0qSfJ8JDW8SNCi/xqwrQ3\n5xJaEqcAX3H3LxGuw9WEL62hRKOb3b0udi7VZR7F/t5L0bWXGtEMAmsIMwb8lXCtjSfk/F5JCAof\nJdx15QXC96IR7kp2aK5ztgcFilncvTLpMkij6APtesLorcXR6umEL6o64FeE+S4n0vgrrAdwIiFB\nuz46jz74krOMxoD93MwcfNBQL38nJGqvi1YfZmYluqds/pnZBDP7RWbZ3VcT7hs7i8b6+TVwd+YH\ntbvfQ/hSy3yfFCGJMLNhZrZfbNVSdO2lgpmdBzxsZt3d/RHgZuBFQkPIaELQ+H1gsrt/lzDIbHbs\nFEfG78zSnnRrOilo0S+sCjO7kfAFdAFhXign/Bqe5u7ron3XE7paILRsDAfezH+pO67ofuq7ALsB\nS9z9dcIcbV8mJFiXApebWZ2Huwbg7lVm9jyNn0cbvHFuTMmTKHn+D9HzSnf/EYRgMWrBh9Bq8Rd3\nL48d15VwD2cIXZwvR+tNP9Dyx8yOAX4MdDWzW9z9buAuwo/m49G1V7DMbAJwb7T4f4FvufujZlZH\nCPQPJ+Qq3u9hQEsnd//YzO4hjGovBlbuqIYuBYpScKIRy5+6+2qPbhMWBYs3EALEiwm5ULPcfV3s\nC+k1wryXAwn5Uxqhl0dmdgLwE2BvQjflO2Z2s7tPM7PfAYMJo2TLgH83s77u/m0z+xywK2FC2C7A\n69H5FGjkiZmdSuMIy2rgB9F1931oCBZ/Duzv7k/HjisB9gdGRqveBuZHx6ju8sTMTiOMjO1M+HHc\nNaqbymj9CGAPdO0VnPgPNMK1d4yZjXb3Be7+eNS4Wwv8t7uvhNDTFrUeHkgIJDcRuqJ3SN0pUJSC\nYmZnE0ZO/tTM7nT3NVnB4i8JH2jvejR1kbt79KF4JOFXsxOS7PXLOE/M7HTCdESZ7kcH9gTOMbOH\nom1DgAsJX1q9gCvN7BDCqMwBhFbIhYScOAUa+TU8+reaEGwAXGNmxILFFTTe1Qgz609oqfpXYC9g\nDWH05cdI3pjZcYSpb4zQo/JL4G+ZtBszu48QIF5MuCZ17RWIaMq3zA+0jwkzdYwiXE+Z77fHzWyJ\nx6bqi1rxDyBcf0WEeYEfjfZv97rTPIpSMKLm98wvqw8J+Yf3Rcm9mW7oOjMriXePmFkP4CDgOkLX\n2GLgn919aT7L31GZ2UnAY9HiAkKw3pNwK8xFwHh3Xx7dUupi4BuED7lsKwj19u6OL7XExeZsW02o\ns8OjTfXADe4+JWp9OtDdn4vmV/wZoa7LCBPZn6a6y69ovsr7gKMII5gvc/dlZnYhcAihpfcfwF+i\n5fGEaVOy6drLs6x5gX9PaDU8P1p+jjBFzsbsAZhRj9uphNz8fQg/0I7akXWnFkUpCGY2jsYcjUrC\nr9xrom33xVoWLRMkRq2IRwOnET4A9wSWA19WkJgfZnYojfeJ/T0hAXsZITfqeEJdfs3MNhPSCaZF\nOVH/Qrirxy6EIOMt4CfuvhhJwvuE1qU6wt05IASLnYCrzaw7oT57mNn5hLnchgIfEX7cTdE1l4g+\nhNYngGeiIPHXhOsr4xjC7Wn/RMhhfJPwmalrLyHRvZsfihbvA/6dMOr85OjfYUA3d/8k05VsZkWE\na/K52KneAM70HT0RenPz5uihR74e0UXxMKH14lXCL6TMHFHlhEmzB+Y4bhDhCy4zx9vzwMik309H\neRDm9poR/f0fJvy6zdwKLDPHZWVWff4udnwZoTu6M1Ca9PvpyI+oLucTfmjtTggKX4rVW2Zy7XeB\nMwkB5CGEIKRX0uXvqA9gUlQvFYQgYmKsvj6k6Xx7f6dxjktde8nV2fGxOrmLkPZRBHQn9Mxktv0y\nx7FdCAHmu4R5FQfno8xqUZRCUEv4wikHfkjj/UcHEX4xN2lZjJ4XufuqaEqBKwhJ2A9ktkteVBDu\nhnMc4QPubQ+3TzyZMLcehC+iPtHzOkLO4kp3vwb40KNPP03HkZxorlInTJHyBULL/DOEibRvBP6J\nkP9WD6wEHvOQ//ZKIgWWuI3Rv12AyYT6qyfMf/kCoUv6Xwg9NPsS7tTyErr2krSGcG39EbjO3ZdE\n6zeb2cOEnhaAL0Sjm+szB3oYpX42YWDgGnffko8CK1CUxLn7CjP7MuFDbI67bzazCwhd0dnB4l2E\npN/jzWwg4WKbGE6jybTzycPIuwcJI9Bfi4LEA4FHol2eIXxZrSG0Qh1DCEiGZo/Miz+X/Iq+iDab\n2YuEHN8h7l5tZisIX0gQAkUjtCL+BPiPRAor2eYSelM+R0jD2QV4j/CjeSnwYpT28SvCtbebmXWN\nBxi69vLL3d80s9HR8xUQfqxF1+HjhNSAfQgB46lEA4xix9cSPnPzRhNuS0HwcFecc6MgsdjdnyUk\n9q6KdskEi5cSknh/Tuj2/AZQqyAxGe5e5e4vxL54diN0i8wCrnH3n7r7DMJtFSGa/FdfToUj1qJU\nTqifIWY2hDCJdhlhbsSl0T5dCHc9GpDvckpO7xFyD2sJQSKEOusa22cG4Zo0YEu+WqGkee6+IhMk\nRsuZG0OsJBrtTKjTw6Gh1T8xChSlYHjjYJXaaPk5tg4Wvwv8lHCrqS3A4wo6Coe7zyLU2b8RRlsS\njXYui3YpJ5rGQV1eBedvhG7LIwktwcMJLRc3Ee4IMY8w5dQ4d/8wqUJKI3evIvxo/ihaVUu4jelv\nzGxEtO4gQuBYT7jThxSgWDB4M+Fzshg4y8wGxrufk6DpcaRgxUZ7HUGYNHYXwgdhMSGf6jB3X5hk\nGaVRvDs5VnedCd2ZNwL7EXIaz1QuaeExs5HA/xICDQij128Ffuth9OVxhPlLlyZURGmGmf0T8Cyh\nCzpjDeEmBCMIk+C/Dxzr7svyX0Jpq2jKoycJo9krgavc/fYky6QWRSlkmW7K/yHcX7aCECR+Ahyh\nILGw5AgS+xFybH5BCBLXAN9QkFiY3H0RIbWjnjBI4tfAve7+SbTL0woSC5OHW2UeRei2rIpWDwRO\nIQSJK4BTFCQWvqj7OTNFVSlwSNK9L2pRlIIWzUA/HvgeoWXqE+Bwj81SL4XHzA4CphAm1t6N0Dp1\ngoL7whZ1f32Z0O38B3dfm3CRZBuY2XDgAsK0OSWEH2evAz91zZOYGtGgwFcJ33dHuvu8RMujQFEK\nmZn1Av5MCBLLCflR7yRbKmmNme1PyHnbRPiiuixqsZIUMLMuUf6bpJCZ9Sbcb309UOPu1QkXSbZR\nNMPHf7r7W4mXRYGiFDozO5Jw14/xChLTw8y+AIwDHlbLlIhI62KpO02mEEuSAkVJBTMrdffKpMsh\n2yZ7wlgREUkXBYoiIiIikpNGPYuIiIhITgoURURERCQnBYoiIiIikpMCRRERERHJSYGiiIiIiOSk\nQFFEREREclKgKCKSMDObbWb10eOCHfQad8Ve40c74jVEZOejQFFEcjKzibHAot7M/pJjn6Wx7ZOS\nKOdOwmOPfLyWiEibFCddABEpeJnA4lgzG+fuL2RtU+Dx2X0L6Bk9fzfJgoiIxClQFJG2MEJAOBU4\nopltBcPMurv75qTL0Vbu/nbSZUiDtNWryM5AXc8i0hZOCAi/ZGYntuUAM+tqZt8xs1fNbIOZVZrZ\nu2b2KzPrl7VvvJv72axtOXPrzOza2Po7zew4M3vJzDYB/xPbb4CZ3WBmb5vZZjOrMLMFZnajme3S\nUjnMbC8ze9jM1pvZJjP7s5mNzDpmkJndbmaLovdYYWYfmNlTZvbjNv6tcuYoZr93MzvFzF6JXuND\nM5tmZl3b8hotvPYBZvY7M3srOme1mX1qZm+Y2Y/NrHts3zuby3M0s5Lo75TZPjq2bVczuyn6u1eY\n2UYze93MrjSz4qzztKleRSQ/1KIoIm3xMfAJsAfwU+CJlnY2s77AbGBvmrY2jgSuAr5uZoe7+7Ks\nQ1tqmWxumwPjgAsIwWy8HKOjcvTPOn5PYBRwnpmNd/d5Oc45CngV6BZbfyLw38DY6PzFwAvAiKzz\n7wrsBhwO/LiF95T93lp6j+dG58rs0wWYBNQD32zDazTnn4Bzsl67O/BFYB/gJDM7xN3rgVuBC6N9\nLwKuix1zLPD5aNvf3H0BgJkdAjwO9Mp6jf2jx8lmdqK712SVq9l6FZH8UYuiiLRFLXBt9HxfMzuj\nlf1vozFI/AdwNiHI+mO0fRBwTzuWbwQwHzgPOB74dbT+d0C/qBzvAWcBE6J9HegL3N/MOXcBFgBf\nA64k/A0A9jKzY6Pn+9AYJL4FnE4ImCYCN0Wv2R4M2D0q68nAb2gMnr5hZt2aO7AN3gS+TWPZjwa+\nDrwebT8g2oa7/y/wt+i1h5jZP8fOE/8/cTeAmXUG/kDIv3RgJvDlaN83o32PBr7fTNmaq1cRyRO1\nKIpIawzA3R80symE1rTrzOxPOXc26wl8lcbWo18AK6PntwJfAUqAI8xsD3f/rMGUAZuB8e5eHivH\nWGC/aNGBM939zWjbO0CmFXGMmR0QBUHxc1YDp7j72uiYE4ETou17Ak8DG2LHfEQIDN9z91pCkNpe\nHHjb3c+PyvIXQsteN8Ln+HBge/McXyMEg1cDexGCuuxGhENoDPJvBQ6Knn8DeMbMioBTo3VVwIPR\n82OBwVH5PwJujtZvBGbQGPhdwtYtrznrVUTyS4GiiGyLHxK6XkcRugRz2RMoojGv8fctnG9vPnur\nmwNzcgQTX4g935IJEgHcfb6ZrSd0h2b2jQeKDryTCRIjH8ee94nO876ZPQMcA/wzIVirM7NFwMvA\nNHd/dfvfWhMNuZvu7mb2CY3d4n0+w3nvIrTYwdZd4JlWy96x/f8A/IrQUnuamfUmBI59ouMedvdM\nAL1X7LgB5M4xNGAXM+vl7utj65urVxHJI3U9i0ibufsjhK5HCF3RndtyWAuPHrF9MrJ/wPZvw2us\nzrEunte2PaOy12Ut18aex899MjAZ+BPwDlBHyOWcCDxvZvtvx2t/lvK0mZkNIgSJmfr4T+A4wsj2\n+2K7NnxXuHs18NtosXN0/Ndi+97VzMu19P8g/n8hLle9ikgeKVAUkW31faIcNUIeX7Z3CcFSZtqc\nUe5elP0Aerh7Jhj5JHb8rpknZtaLMCCktUAv1/YFsefdzGyf2Hn3orE1EUKAt13cvdrd73D3M9x9\nL8JAkEyXaglNc/cKzeDoXwM+dvf/cPe/uvtLxOohh98QBtEAXAqcFj1fReiSz4jXwQdASQv/F5bn\neJ2CmnZJpCNS17OItKbJl7W7/9XMniMMQsh0L8e3b4jyFydE254wsxuA9wnB2VDgSEL3dWYKlfgk\n08PM7C7CYIpLCC1N2zxXo7vPNbO/E0bWGvCgmV1LCGKvje06Nys/sc3MrAyYQ8jfm0toAetOGEmc\nUbo9586TxbHnfc3se4S/+wRCd3rOv7m7f2BmfwZOAcZkVgP3uHv8mKeB5YSAdCjwlJndAXxI+JGx\nOyGP8T3g4vZ6UyLSfhQoikhrcnVrfh94qZltAP9CyPsbQxi5Oi1ruwNLGxbcF5rZ04Q8PyN0204k\nDIx4lxBUbo/zCbl9A6JzPBjb5kA5YdqZbNvSlTuCMBAkl1rggW04V165+0dm9iBwJuE9/zTaVEuY\n9mdcC4ffSggU4z8Wmoxkd/cqMzsT+DPhR8L46NFkN9pvdLiItDN1PYtIS3Leg9jdXwEezbUt2l5O\nGODwH8ArwHrCKOKVhEEeU9m6S/Z84P8RRhJvJrRGjYuOb2l+QZrbHs3l90XC4IsFwJbosZAwfc0+\nOe6K0tJ9l7PXrQd+ADwJLAMqgJrofc4EjnD312mbtr5mW45paf9s3yD8LZYTyv8ycBLwHC38Ldz9\nKRoDPAdezjWCPfq/Mha4kTDSfHP0OouBpwjzav4o+7AWyisieWRNewlERETaxsx+Qfgx4MAkd/9t\nK4eISMooUBQRkTYzs05AV0KX+6OEQU2fArvqPswiOx/lKIqIyLY4gtAtneHATxUkiuycFCiKiMi2\ncsLo8Q8Ik4r/MuHyiMgOoq5nEREREclJo55FREREJCcFiiIiIiKSkwJFEREREclJgaKIiIiI5KRA\nUURERERyUqAoIiIiIjn9fzaHgVsuVqkTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12208b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Topological analysis\n",
    "# SAE parameters extraction\n",
    "%time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from Functions import PreProcessing as preproc\n",
    "\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'StackedAutoEncoder':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'StackedAutoEncoder analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if not train_info['topologic_anal_done']:\n",
    "        continue\n",
    "        \n",
    "    top_info_name = result_analysis_path+'/output_files'+'/'+choose_date+'_top_sweep_losses.jbl'\n",
    "    [losses]= joblib.load(top_info_name)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline  \n",
    "\n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    plt.rcParams['xtick.labelsize'] = 18\n",
    "    plt.rcParams['ytick.labelsize'] = 18\n",
    "    plt.rcParams['legend.numpoints'] = 1\n",
    "    plt.rcParams['legend.handlelength'] = 3\n",
    "    plt.rcParams['legend.borderpad'] = 0.3\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(10,6))\n",
    "\n",
    "    xtick = range(top_min, top_max+1, top_step)\n",
    "    if top_min == 0:\n",
    "        xtick[0] = 1\n",
    "    plt.errorbar(xtick,\n",
    "                  np.mean(losses,axis=0),\n",
    "                  np.std(losses,axis=0),fmt='o-',\n",
    "                  color='k',alpha=0.7,linewidth=2.5, label='MSE in val')\n",
    "    plt.xticks(xtick,rotation=45)\n",
    "    plt.xlim([top_min-1, top_max+1])\n",
    "    plt.title('MSE topological analysis for first layer of SAE',fontsize=18, fontweight='bold')\n",
    "    plt.xlabel('Neurons in layer',fontsize=18, fontweight='bold')\n",
    "    plt.ylabel('MSE values',fontsize=18, fontweight='bold')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topological nalysis performed in 2017_03_08_12_05_27 and for StackedAutoEncoder analysis\n",
      "Fold: 1 of 2, neuron: 1 of 750, init: 1 of 1\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error when checking model input: expected dense_input_146 to have shape (None, 750) but got array with shape (19564, 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-ef623a5fe24b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m                                      validation_data=(norm_all_data[test_id],\n\u001b[1;32m     99\u001b[0m                                                       norm_all_data[test_id]),\n\u001b[0;32m--> 100\u001b[0;31m                                      shuffle=True)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;31m# check if train was the best one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/natmourajr/.virtualenvs/sonarcessy/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/natmourajr/.virtualenvs/sonarcessy/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                                                                            \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                                                                            \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                                                                            batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mval_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/natmourajr/.virtualenvs/sonarcessy/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_dim, batch_size)\u001b[0m\n\u001b[1;32m    957\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m                                    \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m                                    exception_prefix='model input')\n\u001b[0m\u001b[1;32m    960\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[1;32m    961\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/natmourajr/.virtualenvs/sonarcessy/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[1;32m    106\u001b[0m                                         \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                                         \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                                         str(array.shape))\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error when checking model input: expected dense_input_146 to have shape (None, 750) but got array with shape (19564, 400)"
     ]
    }
   ],
   "source": [
    "#load first layer model\n",
    "\n",
    "ineuron = 750\n",
    "            \n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'StackedAutoEncoder':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'Topological nalysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if not train_info['topologic_anal_done']:\n",
    "        print 'Topological Analysis is not done!!!'\n",
    "        continue\n",
    "    \n",
    "    trn_params = preproc.TrnParams(learning_rate= 0.005,verbose=False,\n",
    "                                   train_verbose=True, n_epochs=500)\n",
    "    \n",
    "    for ifold in range(train_info['n_folds']):\n",
    "        train_id, test_id = train_info['CVO'][ifold]\n",
    "        \n",
    "        first_layer_model_name = (result_analysis_path+'/output_files'+'/'+\n",
    "                                  choose_date+'_model_fold'+str(ifold)+\n",
    "                                  '_neuron'+str(ineuron)+'.h5')\n",
    "        \n",
    "        first_layer_model = load_model(first_layer_model_name)\n",
    "        \n",
    "        # normalize data based in train set\n",
    "        if train_info['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "        elif train_info['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "        elif train_info['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        \n",
    "        norm_all_data = scaler.transform(all_data)\n",
    "        \n",
    "        # get the output of an intermediate layer\n",
    "        from keras import backend as K\n",
    "\n",
    "        # with a Sequential model\n",
    "        get_layer_output = K.function([first_layer_model.layers[0].input],\n",
    "                                      [first_layer_model.layers[1].output])\n",
    "        proj_all_data = get_layer_output([norm_all_data])[0]\n",
    "        \n",
    "        neuron_count = 0\n",
    "        for ineuron in range(train_info['sweep_top_min'], \n",
    "                             train_info['sweep_top_max']+1, \n",
    "                             train_info['sweep_top_step']):\n",
    "            if ineuron == 0:\n",
    "                ineuron = 1\n",
    "            \n",
    "            best_init = 0\n",
    "            best_loss = 999\n",
    "            \n",
    "            for i_init in range(train_info['n_inits']):\n",
    "                print ('Fold: %i of %i, neuron: %i of %i, init: %i of %i'%\n",
    "                       (ifold+1, train_info['n_folds'],ineuron, top_max,\n",
    "                        i_init+1, train_info['n_inits']))\n",
    "\n",
    "                # create model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(ineuron, input_dim=proj_all_data.shape[1], init='uniform'))\n",
    "                model.add(Activation('tanh'))\n",
    "                model.add(Dense(proj_all_data.shape[1], init='uniform')) \n",
    "                model.add(Activation('tanh'))\n",
    "                \n",
    "                sgd = SGD(lr=trn_params.learning_rate, \n",
    "                          decay=trn_params.learning_decay,\n",
    "                          momentum=trn_params.momentum, \n",
    "                          nesterov=trn_params.nesterov)\n",
    "                \n",
    "                model.compile(loss='mean_squared_error', \n",
    "                              optimizer=sgd,\n",
    "                              metrics=['mean_absolute_error'])\n",
    "\n",
    "                # Early Stopping\n",
    "                earlyStopping = callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                        patience=25, \n",
    "                                                        verbose=trn_params.train_verbose, \n",
    "                                                        mode='auto')\n",
    "                # train model\n",
    "                trn_desc = model.fit(proj_all_data[train_id], \n",
    "                                     proj_all_data[train_id], \n",
    "                                     nb_epoch=trn_params.n_epochs, \n",
    "                                     batch_size=trn_params.batch_size,\n",
    "                                     callbacks=[earlyStopping], \n",
    "                                     verbose=trn_params.verbose,\n",
    "                                     validation_data=(proj_all_data[test_id],\n",
    "                                                      proj_all_data[test_id]),\n",
    "                                     shuffle=True)\n",
    "                \n",
    "                # check if train was the best one\n",
    "                if i_init == 0:\n",
    "                    trn_descs[ifold][ineuron] = trn_desc\n",
    "                    models[ifold][ineuron] = model\n",
    "                    losses[ifold,neuron_count] = np.min(trn_desc.history['val_loss'])\n",
    "                else:\n",
    "                    if np.min(trn_desc.history['val_loss']) < best_loss:\n",
    "                        best_init = i_init\n",
    "                        best_loss = np.min(trn_desc.history['val_loss'])\n",
    "                        models[ifold][ineuron] = model\n",
    "                        trn_desc[ifold] = trn_desc\n",
    "                        losses[ifold,neuron_count]= np.min(trn_desc.history['val_loss'])\n",
    "                        \n",
    "            neuron_count = neuron_count+1\n",
    "                \n",
    "        \n",
    "    \n",
    "    print 'Train done'\n",
    "    # saving file\n",
    "    \n",
    "    top_info_name = result_analysis_path+'/output_files'+'/'+choose_date+'_top_sweep_losses_second_layer.jbl'\n",
    "    joblib.dump([losses],top_info_name,compress=9)\n",
    "    \n",
    "    for ifold in range(train_info['n_folds']):\n",
    "        for ineuron in range(top_min, top_max+1, top_step):\n",
    "            if ineuron == 0:\n",
    "                ineuron = 1\n",
    "            model_name = (result_analysis_path+'/output_files'+'/'+\n",
    "                          choose_date+'_model_fold'+str(ifold)+\n",
    "                          '_neuron'+str(ineuron)+'second_layer.h5')\n",
    "            models[ifold][ineuron].save(model_name)\n",
    "    \n",
    "            \n",
    "    train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "    train_info['2nd_topologic_anal_done'] = True\n",
    "    joblib.dump([train_info],train_info_name,compress=9)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39124, 750)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10]\n"
     ]
    }
   ],
   "source": [
    "# SAE parameters extraction\n",
    "\n",
    "# second layer\n",
    "\n",
    "%time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "\n",
    "from Functions import PreProcessing as preproc\n",
    "\n",
    "models = {}\n",
    "trn_descs = {}\n",
    "losses = np.zeros((train_info['n_folds'],\n",
    "                   len(range(train_info['sweep_top_min'], \n",
    "                             train_info['sweep_top_max']+1,\n",
    "                             train_info['sweep_top_step']))))\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'StackedAutoEncoder':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'StackedAutoEncoder analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if train_info['topologic_anal_done']:\n",
    "        print 'Topological Analysis is done, just analyse it'\n",
    "        continue\n",
    "    \n",
    "    trn_params = preproc.TrnParams(learning_rate= 0.005,verbose=False,\n",
    "                                   train_verbose=True, n_epochs=500)\n",
    "    \n",
    "    for ifold in range(train_info['n_folds']):\n",
    "        train_id, test_id = train_info['CVO'][ifold]\n",
    "        \n",
    "        ineuron = 750\n",
    "\n",
    "        model_name = (result_analysis_path+'/output_files'+'/'+\n",
    "                      choose_date+'_model_fold'+str(ifold)+\n",
    "                      '_neuron'+str(ineuron)+'.h5')\n",
    "            \n",
    "\n",
    "        first_layer_model = load_model(model_name)\n",
    "\n",
    "        \n",
    "        models[ifold] = {}\n",
    "        trn_descs[ifold] = {}\n",
    "        \n",
    "        # normalize data based in train set\n",
    "        if train_info['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "        elif train_info['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "        elif train_info['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        \n",
    "        norm_all_data = scaler.transform(all_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        neuron_count = 0\n",
    "        for ineuron in range(train_info['sweep_top_min'], \n",
    "                             train_info['sweep_top_max']+1, \n",
    "                             train_info['sweep_top_step']):\n",
    "            if ineuron == 0:\n",
    "                ineuron = 1\n",
    "            \n",
    "            best_init = 0\n",
    "            best_loss = 999\n",
    "            \n",
    "            for i_init in range(train_info['n_inits']):\n",
    "                print ('Fold: %i of %i, neuron: %i of %i, init: %i of %i'%\n",
    "                       (ifold+1, train_info['n_folds'],ineuron, top_max,\n",
    "                        i_init+1, train_info['n_inits']))\n",
    "\n",
    "                # create model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(ineuron, input_dim=norm_all_data.shape[1], init='uniform'))\n",
    "                model.add(Activation('tanh'))\n",
    "                model.add(Dense(norm_all_data.shape[1], init='uniform')) \n",
    "                model.add(Activation('tanh'))\n",
    "                \n",
    "                sgd = SGD(lr=trn_params.learning_rate, \n",
    "                          decay=trn_params.learning_decay,\n",
    "                          momentum=trn_params.momentum, \n",
    "                          nesterov=trn_params.nesterov)\n",
    "                \n",
    "                model.compile(loss='mean_squared_error', \n",
    "                              optimizer=sgd,\n",
    "                              metrics=['mean_absolute_error'])\n",
    "\n",
    "                # Early Stopping\n",
    "                earlyStopping = callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                        patience=25, \n",
    "                                                        verbose=trn_params.train_verbose, \n",
    "                                                        mode='auto')\n",
    "                # train model\n",
    "                trn_desc = model.fit(norm_all_data[train_id], \n",
    "                                     norm_all_data[train_id], \n",
    "                                     nb_epoch=trn_params.n_epochs, \n",
    "                                     batch_size=trn_params.batch_size,\n",
    "                                     callbacks=[earlyStopping], \n",
    "                                     verbose=trn_params.verbose,\n",
    "                                     validation_data=(norm_all_data[test_id],\n",
    "                                                      norm_all_data[test_id]),\n",
    "                                     shuffle=True)\n",
    "                \n",
    "                # check if train was the best one\n",
    "                if i_init == 0:\n",
    "                    trn_descs[ifold][ineuron] = trn_desc\n",
    "                    models[ifold][ineuron] = model\n",
    "                    losses[ifold,neuron_count] = np.min(trn_desc.history['val_loss'])\n",
    "                else:\n",
    "                    if np.min(trn_desc.history['val_loss']) < best_loss:\n",
    "                        best_init = i_init\n",
    "                        best_loss = np.min(trn_desc.history['val_loss'])\n",
    "                        models[ifold][ineuron] = model\n",
    "                        trn_desc[ifold] = trn_desc\n",
    "                        losses[ifold,neuron_count]= np.min(trn_desc.history['val_loss'])\n",
    "                        \n",
    "            neuron_count = neuron_count+1\n",
    "                \n",
    "        \n",
    "    \n",
    "    print 'Train done'\n",
    "    # saving file\n",
    "    \n",
    "    top_info_name = result_analysis_path+'/output_files'+'/'+choose_date+'_top_sweep_losses.jbl'\n",
    "    joblib.dump([losses],top_info_name,compress=9)\n",
    "    \n",
    "    for ifold in range(train_info['n_folds']):\n",
    "        for ineuron in range(top_min, top_max+1, top_step):\n",
    "            if ineuron == 0:\n",
    "                ineuron = 1\n",
    "            model_name = (result_analysis_path+'/output_files'+'/'+\n",
    "                          choose_date+'_model_fold'+str(ifold)+\n",
    "                          '_neuron'+str(ineuron)+'.h5')\n",
    "            models[ifold][ineuron].save(model_name)\n",
    "    \n",
    "            \n",
    "    train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "    train_info['topologic_anal_done'] = True\n",
    "    joblib.dump([train_info],train_info_name,compress=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
