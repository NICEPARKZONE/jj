{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "# Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "# Laboratorio de Processamento de Sinais - UFRJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 9.3936920166e-05 seconds\n",
      "Time to read data file: 4.09174919128 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd event of 0 is 12939\n",
      "Qtd event of 1 is 29352\n",
      "Qtd event of 2 is 11510\n",
      "Qtd event of 3 is 23760\n",
      "\n",
      "Biggest class is 1 with 29352 events\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "analysis_name = 'NeuralNetwork'\n",
    "data_path = os.getenv('OUTPUTDATAPATH')\n",
    "results_path = os.getenv('PACKAGE_NAME')\n",
    "\n",
    "pict_results_path = results_path+'/'+analysis_name+'/picts'\n",
    "files_results_path = results_path+'/'+analysis_name+'/output_files'\n",
    "\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "database = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "spectrum_bins_left = 400\n",
    "development_flag = True\n",
    "development_events = 100\n",
    "\n",
    "if not os.path.exists('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                      (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left)):\n",
    "    print 'No Files in %s/%s\\n'%(data_path,database)\n",
    "else:\n",
    "    #Read lofar data\n",
    "    [data,trgt,class_labels] = joblib.load('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                                           (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left))\n",
    "\n",
    "\n",
    "    m_time = time.time()-m_time\n",
    "    print 'Time to read data file: '+str(m_time)+' seconds'\n",
    "\n",
    "    # correct format\n",
    "    all_data = data\n",
    "    all_trgt = trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))\n",
    "    \n",
    "    # Process data\n",
    "    # unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "    # Same number of events in each class\n",
    "    qtd_events_biggest_class = 0\n",
    "    biggest_class_label = ''\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "            qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "            biggest_class_label = class_label\n",
    "        print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "    print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "\n",
    "    balanced_data = {}\n",
    "    balanced_trgt = {}\n",
    "\n",
    "    from Functions import DataHandler as dh\n",
    "    m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if development_flag:\n",
    "            class_events = all_data[all_trgt==iclass,:]\n",
    "            if len(balanced_data) == 0:\n",
    "                balanced_data = class_events[0:development_events,:]\n",
    "                balanced_trgt = (iclass)*np.ones(development_events)\n",
    "            else:\n",
    "                balanced_data = np.append(balanced_data,\n",
    "                                          class_events[0:development_events,:], \n",
    "                                          axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,(iclass)*np.ones(development_events))\n",
    "        else:\n",
    "            if len(balanced_data) == 0:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                balanced_data = m_datahandler.CreateEventsForClass(\n",
    "                    class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "                balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "            else:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                created_events = (m_datahandler.CreateEventsForClass(all_data[all_trgt==iclass,:],\n",
    "                                                                     qtd_events_biggest_class-\n",
    "                                                                     (len(class_events))))\n",
    "                balanced_data = np.append(balanced_data,created_events,axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,\n",
    "                                          (iclass)*np.ones(created_events.shape[0]),axis=0)\n",
    "        \n",
    "    all_data = balanced_data\n",
    "    all_trgt = balanced_trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train process\n",
    "## The train will modify one file and create three different files\n",
    "\n",
    "### Log File:\n",
    "This file will store basic information of all Package's trains and it will guide the analyses file to recognize which train information file should load. In each train this file should be appended with a new line contend the basic information to find the train information file (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n",
    "\n",
    "### Train Information File\n",
    "This file will store full information of the train performed (all parameters) in its name (each train information file will have a different name). And it will guide which train classifier file or which train result file should be open for analysis (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n",
    "\n",
    "### Train Result File\n",
    "This file will store the classifier result for all data and classification target (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\n",
      "Wall time: 16.9 µs\n",
      "Dividing data in trn and tst\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/output_files/2017_07_04_18_37_57_train_info.jbl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(\"Classification\",'NeuralNetwork')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_inits = 2\n",
    "norm = 'mapstd'\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_inits'] = n_inits\n",
    "train_info['norm'] = norm\n",
    "\n",
    "# divide data in train and test for novelty detection\n",
    "print 'Dividing data in trn and tst'\n",
    "CVO = cross_validation.StratifiedKFold(all_trgt, n_folds)\n",
    "CVO = list(CVO)\n",
    "train_info['CVO'] = CVO\n",
    "\n",
    "train_info['preprocessing_extraction_done'] = False\n",
    "train_info['preprocessing_analysis_done'] = False\n",
    "train_info['train_done'] = False\n",
    "train_info['results_done'] = False\n",
    "train_info['dev'] = development_flag\n",
    "\n",
    "train_info_name = files_results_path+'/'+date+'_train_info.jbl'\n",
    "joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'date': '2017_07_04_16_29_21', 'package': 'NeuralNetwork'}, 1: {'date': '2017_07_04_16_31_28', 'package': 'NeuralNetwork'}, 2: {'date': '2017_07_04_17_50_05', 'package': 'EspClassPCDNeuralNetwork'}, 3: {'date': '2017_07_04_17_51_00', 'package': 'EspClassPCDNeuralNetwork'}, 4: {'date': '2017_07_04_18_13_23', 'package': 'NeuralNetwork'}, 5: {'date': '2017_07_04_18_14_37', 'package': 'NeuralNetwork'}, 6: {'date': '2017_07_04_18_16_04', 'package': 'EspClassPCDNeuralNetwork'}, 7: {'date': '2017_07_04_18_18_45', 'package': 'EspClassPCDNeuralNetwork'}, 8: {'date': '2017_07_04_18_23_07', 'package': 'NeuralNetwork'}, 9: {'date': '2017_07_04_18_29_04', 'package': 'NeuralNetwork'}, 10: {'date': '2017_07_04_18_30_48', 'package': 'NeuralNetwork'}, 11: {'date': '2017_07_04_18_37_28', 'package': 'NeuralNetwork'}, 12: {'date': '2017_07_04_18_37_57', 'package': 'NeuralNetwork'}}\n"
     ]
    }
   ],
   "source": [
    "# Read log files\n",
    "from Functions import LogFunctions as log\n",
    "mlog = log.LogInformation()\n",
    "log_entries = mlog.RecoverLogEntries(package_name=\"Classification\")\n",
    "print log_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing train performed in 2017_07_04_18_37_57 and for NeuralNetwork analysis\n",
      "NeuralNetwork Train Info File\n",
      "Date: 2017_07_04_18_37_57\n",
      "Number of Folds: 2\n",
      "Number of Inits: 2\n",
      "Normalization Method: mapstd\n",
      "Preprocessing Extraction Done: False\n",
      "Preprocessing Analysis Done: False\n",
      "Train Done: True\n",
      "Extract Results: False\n"
     ]
    }
   ],
   "source": [
    "# Read Information of Train Info File\n",
    "choose_date = '2017_07_04_18_37_57'\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'NeuralNetwork':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'Analysing train performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = files_results_path+'/'+date+'_train_info.jbl'\n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    print 'NeuralNetwork Train Info File'\n",
    "    print 'Date: %s'%(choose_date)\n",
    "    print 'Number of Folds: %i'%(train_info['n_folds'])\n",
    "    print 'Number of Inits: %i'%(train_info['n_inits'])\n",
    "    print 'Normalization Method: %s'%(train_info['norm'])\n",
    "    if train_info['preprocessing_extraction_done']:\n",
    "        print 'Preprocessing Extraction Done: True'\n",
    "    else:\n",
    "        print 'Preprocessing Extraction Done: False'\n",
    "    if train_info['preprocessing_analysis_done']:\n",
    "        print 'Preprocessing Analysis Done: True'\n",
    "    else:\n",
    "        print 'Preprocessing Analysis Done: False'\n",
    "    if train_info['train_done']:\n",
    "        print 'Train Done: True'\n",
    "    else:\n",
    "        print 'Train Done: False'\n",
    "    if train_info['results_done']:\n",
    "        print 'Extract Results: True'\n",
    "    else:\n",
    "        print 'Extract Results: False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Process for 1 Fold of 2 Folds\n",
      "Init: 0 of 2\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/3\n",
      "200/200 [==============================] - 0s - loss: 0.2533 - acc: 0.2350 - val_loss: 0.2475 - val_acc: 0.2850\n",
      "Epoch 2/3\n",
      "200/200 [==============================] - 0s - loss: 0.2519 - acc: 0.2350 - val_loss: 0.2458 - val_acc: 0.2850\n",
      "Epoch 3/3\n",
      "200/200 [==============================] - 0s - loss: 0.2499 - acc: 0.2350 - val_loss: 0.2437 - val_acc: 0.3000\n",
      "Estimated Finish Time: 2017/07/04-18:39:16\n",
      "Init: 1 of 2\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/3\n",
      "200/200 [==============================] - 0s - loss: 0.2557 - acc: 0.2000 - val_loss: 0.2486 - val_acc: 0.2450\n",
      "Epoch 2/3\n",
      "200/200 [==============================] - 0s - loss: 0.2542 - acc: 0.2050 - val_loss: 0.2469 - val_acc: 0.2450\n",
      "Epoch 3/3\n",
      "200/200 [==============================] - 0s - loss: 0.2521 - acc: 0.2050 - val_loss: 0.2448 - val_acc: 0.2550\n",
      "Estimated Finish Time: 2017/07/04-18:39:17\n",
      "Train Process for 2 Fold of 2 Folds\n",
      "Init: 0 of 2\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/3\n",
      "200/200 [==============================] - 0s - loss: 0.2532 - acc: 0.2250 - val_loss: 0.2490 - val_acc: 0.2150\n",
      "Epoch 2/3\n",
      "200/200 [==============================] - 0s - loss: 0.2517 - acc: 0.2300 - val_loss: 0.2473 - val_acc: 0.2200\n",
      "Epoch 3/3\n",
      "200/200 [==============================] - 0s - loss: 0.2497 - acc: 0.2300 - val_loss: 0.2452 - val_acc: 0.2200\n",
      "Estimated Finish Time: 2017/07/04-18:39:20\n",
      "Init: 1 of 2\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/3\n",
      "200/200 [==============================] - 0s - loss: 0.2517 - acc: 0.2500 - val_loss: 0.2490 - val_acc: 0.2600\n",
      "Epoch 2/3\n",
      "200/200 [==============================] - 0s - loss: 0.2503 - acc: 0.2500 - val_loss: 0.2473 - val_acc: 0.2650\n",
      "Epoch 3/3\n",
      "200/200 [==============================] - 0s - loss: 0.2483 - acc: 0.2500 - val_loss: 0.2452 - val_acc: 0.2700\n",
      "Estimated Finish Time: 2017/07/04-18:39:21\n"
     ]
    }
   ],
   "source": [
    "# Train Process\n",
    "from Functions import LogFunctions as log\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta \n",
    "\n",
    "train_info_name = files_results_path+'/'+choose_date+'_train_info.jbl'\n",
    "[train_info] = joblib.load(train_info_name)\n",
    "\n",
    "# try to estimate time to be done...\n",
    "total_trains = train_info['n_folds']*train_info['n_inits']\n",
    "nn_trained = 0 \n",
    "\n",
    "n_neurons = 10\n",
    "\n",
    "for ifold in range(train_info['n_folds']):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    classifiers = []\n",
    "    trn_desc = {}\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if train_info['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif train_info['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif train_info['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        \n",
    "    norm_all_data = scaler.transform(all_data)\n",
    "       \n",
    "    print 'Train Process for %i Fold of %i Folds'%(ifold+1,train_info['n_folds'] )\n",
    "    \n",
    "    best_init = 0\n",
    "    best_loss = 999\n",
    "    \n",
    "    for i_init in range(train_info['n_inits']):\n",
    "        print 'Init: %i of %i'%(i_init+1,train_info['n_inits'])\n",
    "        model = Sequential()\n",
    "        model.add(Dense(n_neurons, input_dim=all_data.shape[1], init=\"uniform\"))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Dense(trgt_sparse.shape[1], init=\"uniform\")) \n",
    "        model.add(Activation('tanh'))\n",
    "\n",
    "        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=sgd\n",
    "                      ,metrics=['accuracy'])\n",
    "\n",
    "        # Train model\n",
    "        earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, \n",
    "                                            verbose=0, mode='auto')\n",
    "        start_time = time.time()\n",
    "        init_trn_desc = model.fit(norm_all_data[train_id], trgt_sparse[train_id], \n",
    "                                nb_epoch=3, \n",
    "                                batch_size=256,\n",
    "                                callbacks=[earlyStopping], \n",
    "                                verbose=1,\n",
    "                                validation_data=(all_data[test_id],trgt_sparse[test_id]),\n",
    "                                shuffle=True)\n",
    "        end_time = time.time()\n",
    "        train_time = end_time -start_time\n",
    "        nn_trained += 1\n",
    "        \n",
    "        now = datetime.now()\n",
    "        finish_time  = now + timedelta(seconds = (total_trains-nn_trained)*train_time)\n",
    "        \n",
    "        print 'Estimated Finish Time: %s'%(\n",
    "        finish_time.strftime(\"%Y/%m/%d-%H:%M:%S\"))\n",
    "        if np.min(init_trn_desc.history['val_loss']) < best_loss:\n",
    "            best_init = i_init\n",
    "            best_loss = np.min(init_trn_desc.history['val_loss'])\n",
    "            classifiers = model\n",
    "            trn_desc['epochs'] = init_trn_desc.epoch\n",
    "            trn_desc['acc'] = init_trn_desc.history['acc']\n",
    "            trn_desc['loss'] = init_trn_desc.history['loss']\n",
    "            trn_desc['val_loss'] = init_trn_desc.history['val_loss']\n",
    "            trn_desc['val_acc'] = init_trn_desc.history['val_acc']\n",
    "            \n",
    "    # save model\n",
    "    base_name = files_results_path+'/'+date+'_'\n",
    "    save_file_name = base_name+\"_fold_%i_classifier.h5\"%(ifold)\n",
    "    classifiers.save(save_file_name)\n",
    "    save_file_name = base_name+\"_fold_%i_trn_desc.jbl\"%(ifold)\n",
    "    joblib.dump([train_info,n_neurons,trn_desc],save_file_name,compress=9)\n",
    "    \n",
    "    train_info['train_done'] = True\n",
    "    train_info_name = files_results_path+'/'+date+'_train_info.jbl'\n",
    "    joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train updates\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "fig1 = plt.figure(figsize=(10,6))\n",
    "\n",
    "choose_date = '2017_07_04_18_23_07'\n",
    "ifold = 0\n",
    "\n",
    "base_name = files_results_path+'/'+choose_date+'_'\n",
    "\n",
    "load_file_name = base_name+\"fold_%i_trn_desc.jbl\"%(ifold)\n",
    "[train_info,n_neurons,trn_desc] = joblib.load(load_file_name)\n",
    "\n",
    "# load weights into new model\n",
    "from keras.models import load_model\n",
    "load_file_name = base_name+\"fold_%i_classifier.h5\"%(ifold)\n",
    "model = load_model(load_file_name)\n",
    "\n",
    "l1 = plt.plot(trn_desc['epochs'],\n",
    "              trn_desc['loss'],color=[0,0,1],\n",
    "              linewidth=2.5,linestyle='solid',label='Train Perf.')\n",
    "l2 = plt.plot(trn_desc['epochs'],\n",
    "              trn_desc['val_loss'],color=[1,0,0],\n",
    "              linewidth=2.5,linestyle='dashed',label='Test Perf.')\n",
    "cost = ''\n",
    "if model.loss == 'mean_squared_error':\n",
    "    cost = 'MSE'\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"# Epochs\",fontsize=18,fontweight='bold')\n",
    "plt.ylabel(cost,fontsize=18,fontweight='bold')\n",
    "plt.title(cost+\" vs #Epochs\",fontsize=18,fontweight='bold')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_aspect(1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = class_labels.values()\n",
    "\n",
    "# read trained model\n",
    "choose_date = '2017_07_04_18_23_07'\n",
    "ifold = 0\n",
    "\n",
    "base_name = files_results_path+'/'+choose_date+'_'\n",
    "\n",
    "train_info_name = base_name+\"fold_%i_trn_desc.jbl\"%(ifold)\n",
    "[train_info,n_neurons,trn_desc] = joblib.load(train_info_name)\n",
    "\n",
    "ifold = 0\n",
    "train_id, test_id = train_info['CVO'][ifold]\n",
    "    \n",
    "# normalize data based in train set\n",
    "if train_info['norm'] == 'mapstd':\n",
    "    scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "elif train_info['norm'] == 'mapstd_rob':\n",
    "    scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "elif train_info['norm'] == 'mapminmax':\n",
    "    scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        \n",
    "norm_all_data = scaler.transform(all_data)\n",
    "\n",
    "from keras.models import load_model\n",
    "load_file_name = base_name+\"fold_%i_classifier.h5\"%(ifold)\n",
    "model = load_model(load_file_name)\n",
    "\n",
    "\n",
    "output = model.predict(norm_all_data)\n",
    "all_output = np.argmax(output,axis=1)\n",
    "#confusion_matrix(trgt_sparse, output, labels=[\"ClassA\", \"ClassB\", \"ClassC\",\"ClassD\"])\n",
    "cm = confusion_matrix(all_trgt[test_id], all_output[test_id])\n",
    "cm_normalized = 100.*cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "im =ax.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Greys,clim=(0.0, 100.0))\n",
    "\n",
    "width, height = cm_normalized.shape\n",
    "\n",
    "for x in xrange(width):\n",
    "    for y in xrange(height):\n",
    "        if cm_normalized[x][y] < 50.:\n",
    "            ax.annotate('%1.3f%%'%(cm_normalized[x][y]), xy=(y, x),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "        else:\n",
    "            ax.annotate('%1.3f%%'%(cm_normalized[x][y]), xy=(y, x),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center',color='white')\n",
    "\n",
    "ax.set_title('Confusion Matrix',fontweight='bold',fontsize=15)\n",
    "fig.colorbar(im)\n",
    "tick_marks = np.arange(len(labels))\n",
    "ax.xaxis.set_ticks(tick_marks)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "\n",
    "ax.yaxis.set_ticks(tick_marks)\n",
    "ax.yaxis.set_ticklabels(labels)\n",
    "\n",
    "ax.set_ylabel('True Label',fontweight='bold',fontsize=15)\n",
    "ax.set_xlabel('Predicted Label',fontweight='bold',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20),nrows=4, ncols=4)\n",
    "\n",
    "labels = class_labels.values()\n",
    "\n",
    "# read trained model\n",
    "choose_date = '2017_07_04_18_23_07'\n",
    "ifold = 0\n",
    "\n",
    "base_name = files_results_path+'/'+choose_date+'_'\n",
    "\n",
    "train_info_name = base_name+\"fold_%i_trn_desc.jbl\"%(ifold)\n",
    "[train_info,n_neurons,trn_desc] = joblib.load(train_info_name)\n",
    "\n",
    "# normalize data based in train set\n",
    "if train_info['norm'] == 'mapstd':\n",
    "    scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "elif train_info['norm'] == 'mapstd_rob':\n",
    "    scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "elif train_info['norm'] == 'mapminmax':\n",
    "    scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        \n",
    "norm_all_data = scaler.transform(all_data)\n",
    "\n",
    "output = model.predict(norm_all_data)\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "m_bins = np.linspace(-0.5, 1.5, 50)\n",
    "\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "kernel = 'gaussian' # other kernels: 'gaussian', 'tophat', \n",
    "                    #'epanechnikov', 'exponential', 'linear', 'cosine'\n",
    "for i_target in range(trgt_sparse.shape[1]):\n",
    "    for i_output in range(output.shape[1]):\n",
    "        subplot_id = output.shape[1]*i_target+i_output\n",
    "        m_pts = output[all_trgt==i_target,i_output]\n",
    "        \n",
    "        n, bins, patches = ax[i_target,i_output].hist(m_pts,bins=m_bins,\n",
    "                                                      fc=m_colors[i_target],\n",
    "                                                      alpha=0.8, normed=1)\n",
    "        \n",
    "        if i_output == 0:\n",
    "            ax[i_target,i_output].set_ylabel('Target %s'%(class_labels[i_target]),\n",
    "                                             fontweight='bold',fontsize=15)\n",
    "        if i_target == 3:\n",
    "            ax[i_target,i_output].set_xlabel('Output %s'%(class_labels[i_output]),\n",
    "                                             fontweight='bold',fontsize=15)\n",
    "        ax[i_target,i_output].grid()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20),nrows=4, ncols=4)\n",
    "\n",
    "labels = class_labels.values()\n",
    "\n",
    "# read trained model\n",
    "choose_date = '2017_07_04_16_31_28'\n",
    "ifold = 0\n",
    "\n",
    "base_name = files_results_path+'/'+choose_date+'_'\n",
    "\n",
    "train_info_name = base_name+\"fold_%i_trn_desc.jbl\"%(ifold)\n",
    "[train_info,n_neurons,trn_desc] = joblib.load(train_info_name)\n",
    "\n",
    "# normalize data based in train set\n",
    "if train_info['norm'] == 'mapstd':\n",
    "    scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "elif train_info['norm'] == 'mapstd_rob':\n",
    "    scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "elif train_info['norm'] == 'mapminmax':\n",
    "    scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        \n",
    "norm_all_data = scaler.transform(all_data)\n",
    "\n",
    "output = model.predict(norm_all_data)\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "m_bins = np.linspace(-0.5, 1.5,100)\n",
    "\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "kernel = 'gaussian' # other kernels: 'gaussian', 'tophat', \n",
    "                    #'epanechnikov', 'exponential', 'linear', 'cosine'\n",
    "for i_target in range(trgt_sparse.shape[1]):\n",
    "    for i_output in range(output.shape[1]):\n",
    "        subplot_id = output.shape[1]*i_target+i_output\n",
    "        m_pts = output[all_trgt==i_target,i_output]\n",
    "        \n",
    "        kde = KernelDensity(kernel=kernel,algorithm='auto',\n",
    "                            bandwidth=0.5).fit(m_pts[:, np.newaxis])\n",
    "        log_dens_x = kde.score_samples(m_bins[:, np.newaxis])\n",
    "        ax[i_target,i_output].plot(m_bins, np.exp(log_dens_x),\n",
    "                                   color=m_colors[i_target],linewidth=2.0)\n",
    "        if i_output == 0:\n",
    "            ax[i_target,i_output].set_ylabel('Target %s'%(class_labels[i_target]),\n",
    "                                             fontweight='bold',fontsize=15)\n",
    "        if i_target == 3:\n",
    "            ax[i_target,i_output].set_xlabel('Output %s'%(class_labels[i_output]),\n",
    "                                             fontweight='bold',fontsize=15)\n",
    "        ax[i_target,i_output].grid()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
