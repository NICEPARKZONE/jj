{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de Classificação para Marinha do Brasil\n",
    "\n",
    "## Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "\n",
    "Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "Laboratório de Tecnologia Sonar\n",
    "\n",
    "Instituto de Pesquisas da Marinha - IPqM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas e leitura dos dados\n",
    "As bibliotecas necessárias para a inclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 0.00004 seconds\n",
      "Time to read data file: 1.39961 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: %1.5f seconds'%(m_time-init_time)\n",
    "\n",
    "outputpath = os.environ['OUTPUTDATAPATH']\n",
    "main_analysis_path = os.environ['SONAR_WORKSPACE']\n",
    "log_analysis_path = os.environ['PACKAGE_OUTPUT']\n",
    "result_analysis_path = os.environ['PACKAGE_OUTPUT']+'/NeuralNetwork'\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "\n",
    "subfolder = '4classes_old'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "\n",
    "if(not os.path.exists(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))):\n",
    "    print outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "        subfolder,n_pts_fft,decimation_rate)+' doesnt exist...please create it'\n",
    "    exit()\n",
    "    \n",
    "#Read lofar data\n",
    "[data,class_labels] = joblib.load(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))\n",
    "\n",
    "m_time = time.time()-m_time\n",
    "print 'Time to read data file: %1.5f seconds'%m_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento dos dados\n",
    "Os dados encontram-se no formato do matlab, para isso precisam ser processados para o formato de python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process data...\n",
    "# create a full data vector\n",
    "all_data = {};\n",
    "all_trgt = {};\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    for irun in range(len(data[iclass])):\n",
    "        if len(all_data) == 0:\n",
    "            all_data = data[iclass][irun]['Signal']\n",
    "            all_trgt = (iclass)*np.ones(data[iclass][irun]['Signal'].shape[1])\n",
    "        else:\n",
    "            all_data = np.append(all_data,data[iclass][irun]['Signal'],axis=1)\n",
    "            all_trgt = np.append(all_trgt,(iclass)*np.ones(data[iclass][irun]\n",
    "                                                           ['Signal'].shape[1]),axis=0)\n",
    "            \n",
    "all_data = all_data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanceamento de Classes\n",
    "Os dados encontram-se desbalanceados. Com isso, os classificadores podem se especializar em uma classe (gerando mais SVs para a mesma) e não se especializar em outras\n",
    "\n",
    "Acessados em 21/12/2016\n",
    "\n",
    "https://svds.com/learning-imbalanced-classes/\n",
    "\n",
    "http://www.cs.utah.edu/~piyush/teaching/ImbalancedLearning.pdf\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n",
    "\n",
    "Para solucionar isso, a primeira solução é \"criar\" dados das classes com menos eventos de maneira aleatória. Outras soluções podem ser propostas posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd event of ClassA is 4312\n",
      "Qtd event of ClassB is 9781\n",
      "Qtd event of ClassC is 3833\n",
      "Qtd event of ClassD is 7918\n",
      "\n",
      "Biggest class is ClassB with 9781 events\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (4312, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (9781, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (3833, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (7918, 400)\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "# unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "# Same number of events in each class\n",
    "qtd_events_biggest_class = 0\n",
    "biggest_class_label = ''\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "        qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "        biggest_class_label = class_label\n",
    "    print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "\n",
    "balanced_data = {}\n",
    "balanced_trgt = {}\n",
    "\n",
    "from Functions import DataHandler as dh\n",
    "m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if len(balanced_data) == 0:\n",
    "        class_events = all_data[all_trgt==iclass,:]\n",
    "        balanced_data = m_datahandler.CreateEventsForClass(\n",
    "            class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "        balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "    else:\n",
    "        balanced_data = np.append(balanced_data,\n",
    "                                  (m_datahandler.CreateEventsForClass(\n",
    "                    all_data[all_trgt==iclass,:],\n",
    "                    qtd_events_biggest_class-sum(all_trgt==iclass))),\n",
    "                                  axis=0)\n",
    "        balanced_trgt = np.append(balanced_trgt,\n",
    "                                  (iclass)*np.ones(qtd_events_biggest_class),axis=0)\n",
    "        \n",
    "all_data = balanced_data\n",
    "all_trgt = balanced_trgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "Dividing data in trn and tst\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/train_info_files/2017_05_09_17_27_30_train_info.jbl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(\"Classification\",'EspClassPCDNeuralNetwork')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_pcds = 2\n",
    "norm = 'mapstd'\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_pcds'] = n_pcds\n",
    "train_info['norm'] = norm\n",
    "\n",
    "# divide data in train and test for novelty detection\n",
    "print 'Dividing data in trn and tst'\n",
    "CVO = cross_validation.StratifiedKFold(all_trgt, n_folds)\n",
    "CVO = list(CVO)\n",
    "train_info['CVO'] = CVO\n",
    "\n",
    "train_info['preprocessing_extraction_done'] = False\n",
    "train_info['preprocessing_analysis_done'] = False\n",
    "train_info['train_done'] = False\n",
    "train_info['results_done'] = False\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+date+'_train_info.jbl'\n",
    "joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'date': '2017_03_07_23_34_47', 'package': 'StackedAutoEncoder'}, 1: {'date': '2017_03_07_23_35_21', 'package': 'StackedAutoEncoder'}, 2: {'date': '2017_03_08_00_07_14', 'package': 'StackedAutoEncoder'}, 3: {'date': '2017_03_08_00_20_36', 'package': 'StackedAutoEncoder'}, 4: {'date': '2017_03_08_11_52_41', 'package': 'StackedAutoEncoder'}, 5: {'date': '2017_03_08_11_58_10', 'package': 'StackedAutoEncoder'}, 6: {'date': '2017_03_08_12_03_24', 'package': 'ConvNeuralNetwork'}, 7: {'date': '2017_03_08_12_05_27', 'package': 'StackedAutoEncoder'}, 8: {'date': '2017_03_08_12_27_09', 'package': 'ConvNeuralNetwork'}, 9: {'date': '2017_03_08_12_27_21', 'package': 'ConvNeuralNetwork'}, 10: {'date': '2017_05_09_17_08_12', 'package': 'EspClassPCDNeuralNetwork'}, 11: {'date': '2017_05_09_17_08_18', 'package': 'EspClassPCDNeuralNetwork'}, 12: {'date': '2017_05_09_17_09_16', 'package': 'EspClassPCDNeuralNetwork'}, 13: {'date': '2017_05_09_17_10_17', 'package': 'EspClassPCDNeuralNetwork'}, 14: {'date': '2017_05_09_17_21_10', 'package': 'EspClassPCDNeuralNetwork'}, 15: {'date': '2017_05_09_17_24_27', 'package': 'EspClassPCDNeuralNetwork'}, 16: {'date': '2017_05_09_17_27_30', 'package': 'EspClassPCDNeuralNetwork'}}\n"
     ]
    }
   ],
   "source": [
    "# Read log files\n",
    "from Functions import LogFunctions as log\n",
    "mlog = log.LogInformation()\n",
    "log_entries = mlog.RecoverLogEntries(package_name=\"Classification\")\n",
    "print log_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing train performed in 2017_05_09_17_27_30 and for EspClassPCDNeuralNetwork analysis\n",
      "NeuralNetwork Train Info File\n",
      "Date: 2017_05_09_17_27_30\n",
      "Number of Folds: 2\n",
      "Number of Used PCDs: 2\n",
      "Normalization Method: mapstd\n",
      "Preprocessing Extraction Done: False\n",
      "Preprocessing Analysis Done: False\n",
      "Train Done: False\n",
      "Extract Results: False\n"
     ]
    }
   ],
   "source": [
    "# Read Information of Train Info File\n",
    "choose_date = '2017_05_09_17_27_30'\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'EspClassPCDNeuralNetwork':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'Analysing train performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    print 'NeuralNetwork Train Info File'\n",
    "    print 'Date: %s'%(choose_date)\n",
    "    print 'Number of Folds: %i'%(train_info['n_folds'])\n",
    "    print 'Number of Used PCDs: %i'%(train_info['n_pcds'])\n",
    "    print 'Normalization Method: %s'%(train_info['norm'])\n",
    "    if train_info['preprocessing_extraction_done']:\n",
    "        print 'Preprocessing Extraction Done: True'\n",
    "    else:\n",
    "        print 'Preprocessing Extraction Done: False'\n",
    "    if train_info['preprocessing_analysis_done']:\n",
    "        print 'Preprocessing Analysis Done: True'\n",
    "    else:\n",
    "        print 'Preprocessing Analysis Done: False'\n",
    "    if train_info['train_done']:\n",
    "        print 'Train Done: True'\n",
    "    else:\n",
    "        print 'Train Done: False'\n",
    "    if train_info['results_done']:\n",
    "        print 'Extract Results: True'\n",
    "    else:\n",
    "        print 'Extract Results: False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "PCD extraction performed in 2017_05_09_17_27_30 and for EspClassPCDNeuralNetwork analysis\n",
      "Preprocessing Extraction done, just analyse it\n"
     ]
    }
   ],
   "source": [
    "# PCD extraction\n",
    "%time\n",
    "\n",
    "from Functions import PreProcessing as preproc\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'EspClassPCDNeuralNetwork':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCD extraction performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if train_info['preprocessing_extraction_done']:\n",
    "        print 'Preprocessing Extraction done, just analyse it'\n",
    "        continue\n",
    "    \n",
    "    trn_params = preproc.TrnParams(learning_rate=0.005, \n",
    "                                   verbose=True,\n",
    "                                   train_verbose=False)\n",
    "    \n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt)\n",
    "    \n",
    "    pcds = {}\n",
    "    pcd_objs = {}\n",
    "    \n",
    "    for ifold in range(len(train_info['CVO'])):\n",
    "        print 'PCD extraction process: fold %i of %i'%(ifold+1,len(train_info['CVO']))\n",
    "        # split data in trn set, tst set\n",
    "        train_id, test_id = train_info['CVO'][ifold]\n",
    "\n",
    "        # normalize data based in train set\n",
    "        if train_info['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "        elif train_info['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "        elif train_info['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "                \n",
    "        norm_data = scaler.transform(all_data)\n",
    "            \n",
    "        pcd = preproc.PCDIndependent(n_components=train_info['n_pcds'])\n",
    "            \n",
    "        pcd.fit(norm_data, trgt_sparse, \n",
    "                train_id, test_id, trn_params=trn_params)\n",
    "\n",
    "        pcd_objs[ifold] = pcd\n",
    "        pcds[ifold] = pcd.pcds\n",
    "\n",
    "    print 'Extraction done'\n",
    "    # saving file\n",
    "    pcd_file_path = result_analysis_path+'/result_files'+'/'+choose_date+'_pcd_file.jbl'\n",
    "\n",
    "    if pcds != {}:\n",
    "        joblib.dump([pcds],pcd_file_path,compress=9)\n",
    "\n",
    "    train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "    train_info['preprocessing_extraction_done'] = True\n",
    "    joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
