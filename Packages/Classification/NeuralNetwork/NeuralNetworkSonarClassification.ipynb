{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "# Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "# Laboratorio de Processamento de Sinais - UFRJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 5.60283660889e-05 seconds\n",
      "Time to read data file: 5.17068195343 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd event of 0 is 12939\n",
      "Qtd event of 1 is 29352\n",
      "Qtd event of 2 is 11510\n",
      "Qtd event of 3 is 23760\n",
      "\n",
      "Biggest class is 1 with 29352 events\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "analysis_name = 'NeuralNetwork'\n",
    "data_path = os.getenv('OUTPUTDATAPATH')\n",
    "results_path = os.getenv('PACKAGE_NAME')\n",
    "\n",
    "base_results_path = '%s/%s'%(results_path,analysis_name)\n",
    "pict_results_path = '%s/picts'%(base_results_path)\n",
    "files_results_path = '%s/output_files'%(base_results_path)\n",
    "\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "database = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "spectrum_bins_left = 400\n",
    "development_flag = True\n",
    "development_events = 100\n",
    "\n",
    "if not os.path.exists('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                      (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left)):\n",
    "    print 'No Files in %s/%s\\n'%(data_path,database)\n",
    "else:\n",
    "    #Read lofar data\n",
    "    [data,trgt,class_labels] = joblib.load('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                                           (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left))\n",
    "\n",
    "\n",
    "    m_time = time.time()-m_time\n",
    "    print 'Time to read data file: '+str(m_time)+' seconds'\n",
    "\n",
    "    # correct format\n",
    "    all_data = data\n",
    "    all_trgt = trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))\n",
    "    \n",
    "    # Process data\n",
    "    # unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "    # Same number of events in each class\n",
    "    qtd_events_biggest_class = 0\n",
    "    biggest_class_label = ''\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "            qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "            biggest_class_label = class_label\n",
    "        print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "    print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "\n",
    "    balanced_data = {}\n",
    "    balanced_trgt = {}\n",
    "\n",
    "    from Functions import DataHandler as dh\n",
    "    m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if development_flag:\n",
    "            class_events = all_data[all_trgt==iclass,:]\n",
    "            if len(balanced_data) == 0:\n",
    "                balanced_data = class_events[0:development_events,:]\n",
    "                balanced_trgt = (iclass)*np.ones(development_events)\n",
    "            else:\n",
    "                balanced_data = np.append(balanced_data,\n",
    "                                          class_events[0:development_events,:], \n",
    "                                          axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,(iclass)*np.ones(development_events))\n",
    "        else:\n",
    "            if len(balanced_data) == 0:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                balanced_data = m_datahandler.CreateEventsForClass(\n",
    "                    class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "                balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "            else:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                created_events = (m_datahandler.CreateEventsForClass(all_data[all_trgt==iclass,:],\n",
    "                                                                     qtd_events_biggest_class-\n",
    "                                                                     (len(class_events))))\n",
    "                balanced_data = np.append(balanced_data,created_events,axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,\n",
    "                                          (iclass)*np.ones(created_events.shape[0]),axis=0)\n",
    "        \n",
    "    all_data = balanced_data\n",
    "    all_trgt = balanced_trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Function\n",
    "from Functions import TrainParameters as trnparams\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def NeuralTrainFunction(data=None, trgt=None, \n",
    "                        ifold=0, n_folds=2, n_neurons=10, \n",
    "                        trn_params=None, save_path='', dev=False):\n",
    "    \n",
    "    \n",
    "    # turn targets in sparse mode\n",
    "    trgt_sparse = np_utils.to_categorical(trgt.astype(int))\n",
    "    \n",
    "    # load or create cross validation ids\n",
    "    #n_folds = 2\n",
    "    CVO = trnparams.ClassificationFolds(folder=save_path,n_folds=n_folds,trgt=trgt,dev=dev)\n",
    "\n",
    "\n",
    "    n_folds = len(CVO)\n",
    "    n_inits = trn_params.params['n_inits']\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "    \n",
    "    analysis_str = 'NeuralNetwork'\n",
    "    prefix_str = 'RawData'\n",
    "\n",
    "    model_str = '%s/%s/%s_%i_folds_%s_%i_neurons'%(save_path,analysis_str,\n",
    "                                                   prefix_str,n_folds,\n",
    "                                                   params_str,n_neurons)\n",
    "\n",
    "    \n",
    "    if not development_flag:\n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "        if os.path.exists(file_name):\n",
    "            print 'File %s exists'%(file_name)\n",
    "            return 0\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "        if os.path.exists(file_name):\n",
    "            print 'File %s exists'%(file_name)\n",
    "            return 0\n",
    "\n",
    "    #print \"Fold: %i\"%(ifold+1)\n",
    "    train_id, test_id = CVO[ifold]\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(data)\n",
    "\n",
    "    best_init = 0\n",
    "    best_loss = 999\n",
    "\n",
    "    classifier = []\n",
    "    trn_desc = {}\n",
    "\n",
    "    for i_init in range(n_inits):\n",
    "        print 'Neuron: %i - Fold %i of %i Folds -  Init %i of %i Inits'%(n_neurons, \n",
    "                                                                         ifold+1, \n",
    "                                                                         n_folds, \n",
    "                                                                         i_init+1,\n",
    "                                                                         n_inits)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(n_neurons, input_dim=data.shape[1], init=\"uniform\"))\n",
    "        model.add(Activation(trn_params.params['activation']))\n",
    "        model.add(Dense(trgt_sparse.shape[1], init=\"uniform\")) \n",
    "        model.add(Activation('tanh'))\n",
    "\n",
    "        sgd = SGD(lr=trn_params.params['learning_rate'], \n",
    "                  decay=trn_params.params['learning_decay'], \n",
    "                  momentum=trn_params.params['momentum'], \n",
    "                  nesterov=trn_params.params['nesterov'])\n",
    "        model.compile(loss='mean_squared_error', \n",
    "                      optimizer=sgd,\n",
    "                      metrics=['accuracy'])\n",
    "        # Train model\n",
    "        earlyStopping = callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                patience=trn_params.params['patience'],\n",
    "                                                verbose=trn_params.params['train_verbose'], \n",
    "                                                mode='auto')\n",
    "\n",
    "        init_trn_desc = model.fit(norm_data[train_id], trgt_sparse[train_id], \n",
    "                                  nb_epoch=trn_params.params['n_epochs'], \n",
    "                                  batch_size=trn_params.params['batch_size'],\n",
    "                                  callbacks=[earlyStopping], \n",
    "                                  verbose=trn_params.params['verbose'],\n",
    "                                  validation_data=(norm_data[test_id],\n",
    "                                                   trgt_sparse[test_id]),\n",
    "                                  shuffle=True)\n",
    "        if np.min(init_trn_desc.history['val_loss']) < best_loss:\n",
    "            best_init = i_init\n",
    "            best_loss = np.min(init_trn_desc.history['val_loss'])\n",
    "            classifier = model\n",
    "            trn_desc['epochs'] = init_trn_desc.epoch\n",
    "            trn_desc['acc'] = init_trn_desc.history['acc']\n",
    "            trn_desc['loss'] = init_trn_desc.history['loss']\n",
    "            trn_desc['val_loss'] = init_trn_desc.history['val_loss']\n",
    "            trn_desc['val_acc'] = init_trn_desc.history['val_acc']\n",
    "\n",
    "    # save model\n",
    "    if not development_flag:        \n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "        classifier.save(file_name)\n",
    "        file_name = '%s_fold_%i_trn_desc.jbl'%(model_str,ifold)\n",
    "        joblib.dump([trn_desc],file_name,compress=9)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "        classifier.save(file_name)\n",
    "        file_name = '%s_fold_%i_trn_desc_dev.jbl'%(model_str,ifold)\n",
    "        joblib.dump([trn_desc],file_name,compress=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_1_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_2_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_3_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_4_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_5_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_6_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_7_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_8_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_9_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_10_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_11_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_12_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_13_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_14_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_15_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_16_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_17_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_18_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_19_neurons_fold_0_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_1_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_2_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_3_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_4_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_5_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_6_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_7_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_8_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_9_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_10_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_11_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_12_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_13_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_14_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_15_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_16_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_17_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_18_neurons_fold_1_model_dev.h5 exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/2_folds_cross_validation_dev.jbl exists\n",
      "File /Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/NeuralNetwork/RawData_2_folds_1_inits_mapstd_norm_10_epochs_4_batch_size_tanh_activation_19_neurons_fold_1_model_dev.h5 exists\n"
     ]
    }
   ],
   "source": [
    "# load train parameters\n",
    "trn_params_folder='%s/NeuralNetwork/%s_trnparams.jbl'%(results_path,analysis_name)\n",
    "\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "\n",
    "n_folds = 2\n",
    "CVO = trnparams.ClassificationFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\n",
    "\n",
    "for ifold in range(len(CVO)):\n",
    "    for ineuron in range(1,20,1):\n",
    "        NeuralTrainFunction(data=all_data, trgt=all_trgt, ifold=ifold, n_folds=n_folds, \n",
    "                            n_neurons=ineuron, trn_params=trn_params, \n",
    "                            save_path=results_path,dev=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
