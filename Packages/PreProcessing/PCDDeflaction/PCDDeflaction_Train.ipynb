{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "# Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "# Laboratorio de Processamento de Sinais - UFRJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 4.6968460083e-05 seconds\n",
      "Time to read data file: 2.49076294899 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "outputpath = os.environ['OUTPUTDATAPATH']\n",
    "main_analysis_path = os.environ['SONAR_WORKSPACE']\n",
    "log_analysis_path = os.environ['PACKAGE_OUTPUT']\n",
    "result_analysis_path = os.environ['PACKAGE_OUTPUT']+'/PCDDeflaction'\n",
    "pict_results_path = os.environ['PACKAGE_OUTPUT']+'/PCDDeflaction/picts'\n",
    "files_results_path = os.environ['PACKAGE_OUTPUT']+'/PCDDeflaction/output_files'\n",
    "\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "\n",
    "subfolder = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "\n",
    "if(not os.path.exists(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))):\n",
    "    print outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "        subfolder,n_pts_fft,decimation_rate)+' doesnt exist...please create it'\n",
    "    exit()\n",
    "    \n",
    "#Read lofar data\n",
    "[data,class_labels] = joblib.load(outputpath+'/'+\n",
    "                                  'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))\n",
    "m_time = time.time()-m_time\n",
    "print 'Time to read data file: '+str(m_time)+' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/Users/natmourajr/.virtualenvs/kerasenv/lib/python2.7/site-packages/keras/utils/np_utils.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  Y = np.zeros((len(y), nb_classes))\n",
      "/Users/natmourajr/.virtualenvs/kerasenv/lib/python2.7/site-packages/keras/utils/np_utils.py:16: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  Y[i, y[i]] = 1.\n"
     ]
    }
   ],
   "source": [
    "# Get data in correct format\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# create a full data vector\n",
    "all_data = {};\n",
    "all_trgt = {};\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    for irun in range(len(data[iclass])):\n",
    "        if len(all_data) == 0:\n",
    "            all_data = data[iclass][irun]['Signal']\n",
    "            all_trgt = (iclass)*np.ones(data[iclass][irun]['Signal'].shape[1])\n",
    "        else:\n",
    "            all_data = np.append(all_data,data[iclass][irun]['Signal'],axis=1)\n",
    "            all_trgt = np.append(all_trgt,(iclass)*np.ones(data[iclass][irun]\n",
    "                                                           ['Signal'].shape[1]),axis=0)\n",
    "\n",
    "all_data = all_data.transpose()\n",
    "\n",
    "# turn targets in sparse mode\n",
    "trgt_sparse = np_utils.to_categorical(all_trgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train process\n",
    "### The train will modify one file and create three different files\n",
    "\n",
    "### Log File:\n",
    "\n",
    "This file will store basic information of all Package's trains and it will guide the analyses file to recognize which train information file should load. In each train this file should be appended with a new line contend the basic information to find the train information file (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n",
    "\n",
    "### Train Information File\n",
    "\n",
    "This file will store full information of the train performed (all parameters) in its name (each train information file will have a different name). And it will guide which train classifier file or which train result file should be open for analysis (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n",
    "\n",
    "### Train Classifier File\n",
    "\n",
    "This file will store the classifier after train, the folds information and the train evolution (when this exists) (NATIVE FORMAT) or (PYTHON FORMAT) - This file should not be access by all programs\n",
    "\n",
    "### Train Result File\n",
    "\n",
    "This file will store the classifier result for all data and classification target (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 of 2 - PCD: 1 of 3 - Init: 1 of 1\n",
      "Fold: 1 of 2 - PCD: 2 of 3 - Init: 1 of 1\n",
      "Fold: 1 of 2 - PCD: 3 of 3 - Init: 1 of 1\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 arrays but instead got the following list of 2 arrays: [array([[-0.1692743 , -0.28389391, -0.67418176, ..., -1.5969259 ,\n         0.31137802, -1.29025254],\n       [ 2.49231725,  5.06538424,  1.2520518 , ..., -0.41542442,\n        -0.41947787,  0.65544224],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-7f21149405a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrgt_sparse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                                 shuffle=True)\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_trn_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0mbest_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natmourajr/.virtualenvs/kerasenv/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/natmourajr/.virtualenvs/kerasenv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                                                            \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                                            \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m                                                            batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natmourajr/.virtualenvs/kerasenv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_dim, batch_size)\u001b[0m\n\u001b[1;32m    961\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                                    \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                                    exception_prefix='model input')\n\u001b[0m\u001b[1;32m    964\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[1;32m    965\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natmourajr/.virtualenvs/kerasenv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[1;32m     49\u001b[0m                                 \u001b[0;34m'the following list of '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                 \u001b[0;34m' arrays: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                 '...')\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 arrays but instead got the following list of 2 arrays: [array([[-0.1692743 , -0.28389391, -0.67418176, ..., -1.5969259 ,\n         0.31137802, -1.29025254],\n       [ 2.49231725,  5.06538424,  1.2520518 , ..., -0.41542442,\n        -0.41947787,  0.65544224],..."
     ]
    }
   ],
   "source": [
    "# Train Process\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Merge\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(package_name=\"PreProcessing\",analysis_name='PCDDeflaction')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_inits = 1\n",
    "n_pcds = 3\n",
    "norm = 'mapstd'\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_inits'] = n_inits\n",
    "train_info['n_pcds'] = n_pcds\n",
    "train_info['norm'] = norm\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+date+'_train_info.jbl'\n",
    "classifiers_name = result_analysis_path+'/classifiers_files'+'/'+date+'_classifiers'\n",
    "pdf_file_name = result_analysis_path+'/output_files'+'/'+date+'_pcds'\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "CVO = cross_validation.StratifiedKFold(all_trgt, train_info['n_folds'])\n",
    "CVO = list(CVO)\n",
    "train_info['CVO'] = CVO\n",
    "\n",
    "joblib.dump([train_info],train_info_name,compress=9)\n",
    "\n",
    "# train classifiers\n",
    "classifiers = {}\n",
    "trn_desc = {}\n",
    "pcds = {}\n",
    "\n",
    "# try to estimate time to be done...\n",
    "total_trains = train_info['n_folds']*train_info['n_inits']\n",
    "nn_trained = 0\n",
    "\n",
    "for ifold in range(train_info['n_folds']):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    # normalize data based in train set\n",
    "    if train_info['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif train_info['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif train_info['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        \n",
    "    norm_all_data = scaler.transform(all_data)\n",
    "       \n",
    "    classifiers[ifold] = {}\n",
    "    trn_desc[ifold] = {}\n",
    "    pcds[ifold] = {}\n",
    "    \n",
    "    for ipcd in range(train_info['n_pcds']):\n",
    "        best_init = 0\n",
    "        best_loss = 999\n",
    "        if ipcd == 0:\n",
    "            # first pcd: regular NN\n",
    "            for i_init in range(train_info['n_inits']):\n",
    "                print ('Fold: %i of %i - PCD: %i of %i - Init: %i of %i'\n",
    "                       %(ifold+1, train_info['n_folds'],\n",
    "                         ipcd+1, train_info['n_pcds'],\n",
    "                         i_init+1,train_info['n_inits']))\n",
    "                model = Sequential()\n",
    "                model.add(Dense(all_data.shape[1],\n",
    "                                input_dim=all_data.shape[1], \n",
    "                                init='identity',trainable=False))\n",
    "                model.add(Activation('linear'))\n",
    "                model.add(Dense(1, input_dim=all_data.shape[1], init='uniform'))\n",
    "                model.add(Activation('tanh'))\n",
    "                model.add(Dense(trgt_sparse.shape[1], init='uniform')) \n",
    "                model.add(Activation('tanh'))\n",
    "                \n",
    "                sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "                model.compile(loss='mean_squared_error', optimizer=sgd\n",
    "                      ,metrics=['accuracy'])\n",
    "                \n",
    "                earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=25, \n",
    "                                            verbose=0, mode='auto')\n",
    "                # Train model\n",
    "                init_trn_desc = model.fit(norm_all_data[train_id], trgt_sparse[train_id], \n",
    "                                nb_epoch=50, \n",
    "                                batch_size=8,\n",
    "                                callbacks=[earlyStopping], \n",
    "                                verbose=0,\n",
    "                                validation_data=(norm_all_data[test_id],\n",
    "                                                 trgt_sparse[test_id]),\n",
    "                                shuffle=True)\n",
    "                if np.min(init_trn_desc.history['val_loss']) < best_loss:\n",
    "                    best_init = i_init\n",
    "                    best_loss = np.min(init_trn_desc.history['val_loss'])\n",
    "                    classifiers[ifold][ipcd] = model\n",
    "                    trn_desc[ifold][ipcd] = init_trn_desc\n",
    "                    pcd_test = model.layers[2].get_weights()\n",
    "                    pcds[ifold][ipcd] = pcd_test[0]\n",
    "                \n",
    "        else:\n",
    "            # from second to end: freeze previous and train only last one \n",
    "            for i_init in range(train_info['n_inits']):\n",
    "                print ('Fold: %i of %i - PCD: %i of %i - Init: %i of %i'\n",
    "                       %(ifold+1, train_info['n_folds'],\n",
    "                         ipcd+1, train_info['n_pcds'],\n",
    "                         i_init+1,train_info['n_inits']))\n",
    "                model = Sequential()\n",
    "                \n",
    "                #freezing the first layer\n",
    "                freeze_layer = []\n",
    "                trn_data = []\n",
    "                tst_data = []\n",
    "                \n",
    "                for jpcd in range(ipcd):\n",
    "                    buffer_layer = Sequential()\n",
    "                    buffer_layer.add(Dense(1, \n",
    "                                           input_dim=norm_all_data.shape[1],\n",
    "                                           trainable=False))\n",
    "                    w = buffer_layer.get_weights()\n",
    "                    w[0] = pcds[ifold][jpcd]\n",
    "                    buffer_layer.set_weights(w)\n",
    "                    \n",
    "                    if jpcd == 0:\n",
    "                        freeze_layer = buffer_layer\n",
    "                        trn_data = norm_all_data[train_id]\n",
    "                        tst_data = norm_all_data[test_id]\n",
    "                    else:\n",
    "                        freeze_layer = Merge([freeze_layer, non_freeze_layer],\n",
    "                                             mode='concat')\n",
    "                        if jpcd == 1:\n",
    "                            trn_data = [trn_data, norm_all_data[train_id]]\n",
    "                            tst_data = [tst_data, norm_all_data[test_id]]\n",
    "                        else:\n",
    "                            trn_data = [trn_data[0], norm_all_data[train_id]]\n",
    "                            tst_data = [tst_data[0], norm_all_data[test_id]]\n",
    "                    \n",
    "                no_freeze_layer = Sequential()\n",
    "                no_freeze_layer.add(Dense(1,\n",
    "                                          input_dim=norm_all_data.shape[1],\n",
    "                                          trainable=True))\n",
    "                if ipcd == 1:\n",
    "                    trn_data= [trn_data, norm_all_data[train_id]]\n",
    "                    tst_data= [tst_data, norm_all_data[test_id]]\n",
    "                else:\n",
    "                    trn_data= [trn_data[0], norm_all_data[train_id]]\n",
    "                    tst_data= [tst_data[0], norm_all_data[test_id]]\n",
    "                    \n",
    "                first_layer = Merge([freeze_layer, no_freeze_layer], mode='concat')\n",
    "                model.add(first_layer)\n",
    "                model.add(Activation('tanh'))\n",
    "                model.add(Dense(trgt_sparse.shape[1], init='uniform')) \n",
    "                model.add(Activation('tanh'))\n",
    "                \n",
    "                \n",
    "                sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "                model.compile(loss='mean_squared_error',\n",
    "                              optimizer=sgd, \n",
    "                              metrics=['accuracy'])\n",
    "                earlyStopping = callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                        patience=25,\n",
    "                                                        verbose=0,\n",
    "                                                        mode='auto')\n",
    "                \n",
    "                # Train model\n",
    "                init_trn_desc = model.fit(trn_data, trgt_sparse[train_id], \n",
    "                                nb_epoch=50, \n",
    "                                batch_size=8,\n",
    "                                callbacks=[earlyStopping], \n",
    "                                verbose=0,\n",
    "                                validation_data=(tst_data,trgt_sparse[test_id]),\n",
    "                                shuffle=True)\n",
    "                if np.min(init_trn_desc.history['val_loss']) < best_loss:\n",
    "                    best_init = i_init\n",
    "                    best_loss = np.min(init_trn_desc.history['val_loss'])\n",
    "                    classifiers[ifold][ipcd] = model\n",
    "                    trn_desc[ifold][ipcd] = init_trn_desc\n",
    "                    \n",
    "                    # in keras, we cannot access directly the weights\n",
    "                    # one layer for weights and one for activation\n",
    "                    pcds[ifold][ipcd] = model.get_weights()[2*ipcd]\n",
    "#classifiers_file = open(classifiers_name+'.pickle', \"wb\")\n",
    "#pickle.dump([classifiers,trn_desc],classifiers_file)\n",
    "#classifiers_file.close()\n",
    "\n",
    "pdfs_file = open(pdf_file_name+'.pickle', \"wb\")\n",
    "pickle.dump([pcds],pdfs_file)\n",
    "pdfs_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 3.35462146,  2.91011893,  0.33016993, ..., -0.41542442,\n",
       "         -0.60947472, -1.45432777],\n",
       "        [-1.7833923 ,  4.42413157,  2.57619416, ..., -0.41542442,\n",
       "         -1.28977449, -1.06924898],\n",
       "        [-0.1692743 ,  5.79186593,  3.11790879, ...,  0.35023618,\n",
       "         -0.163121  , -1.38639632],\n",
       "        ..., \n",
       "        [-0.1692743 , -1.31937388,  0.79320779, ..., -0.08680988,\n",
       "         -0.41107446,  1.19151618],\n",
       "        [-0.1692743 , -0.28389391,  0.16178053, ..., -1.08024133,\n",
       "         -0.41107446, -1.27841941],\n",
       "        [-0.1692743 , -1.06496939,  0.70356775, ...,  0.18537754,\n",
       "         -1.59546807, -0.4316543 ]]),\n",
       " array([[ 3.35462146,  2.91011893,  0.33016993, ..., -0.41542442,\n",
       "         -0.60947472, -1.45432777],\n",
       "        [-1.7833923 ,  4.42413157,  2.57619416, ..., -0.41542442,\n",
       "         -1.28977449, -1.06924898],\n",
       "        [-0.1692743 ,  5.79186593,  3.11790879, ...,  0.35023618,\n",
       "         -0.163121  , -1.38639632],\n",
       "        ..., \n",
       "        [-0.1692743 , -1.31937388,  0.79320779, ..., -0.08680988,\n",
       "         -0.41107446,  1.19151618],\n",
       "        [-0.1692743 , -0.28389391,  0.16178053, ..., -1.08024133,\n",
       "         -0.41107446, -1.27841941],\n",
       "        [-0.1692743 , -1.06496939,  0.70356775, ...,  0.18537754,\n",
       "         -1.59546807, -0.4316543 ]])]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Process for 1 Fold of 2 Folds\n"
     ]
    }
   ],
   "source": [
    "# Train Process\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Merge\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(package_name=\"PreProcessing\",analysis_name='PCDDeflaction')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_inits = 1\n",
    "n_pcds = 2\n",
    "norm = 'mapstd'\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_inits'] = n_inits\n",
    "train_info['n_pcds'] = n_pcds\n",
    "train_info['norm'] = norm\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+date+'_train_info.jbl'\n",
    "classifiers_name = result_analysis_path+'/classifiers_files'+'/'+date+'_classifiers'\n",
    "pdf_file_name = result_analysis_path+'/output_files'+'/'+date+'_pcds'\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "CVO = cross_validation.StratifiedKFold(all_trgt, train_info['n_folds'])\n",
    "CVO = list(CVO)\n",
    "train_info['CVO'] = CVO\n",
    "\n",
    "joblib.dump([train_info],train_info_name,compress=9)\n",
    "\n",
    "# train classifiers\n",
    "classifiers = {}\n",
    "trn_desc = {}\n",
    "pcds = {}\n",
    "\n",
    "# try to estimate time to be done...\n",
    "total_trains = train_info['n_folds']*train_info['n_inits']\n",
    "nn_trained = 0\n",
    "\n",
    "for ifold in range(train_info['n_folds']):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    # normalize data based in train set\n",
    "    if train_info['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif train_info['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif train_info['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        \n",
    "    norm_all_data = scaler.transform(all_data)\n",
    "       \n",
    "    print 'Train Process for %i Fold of %i Folds'%(ifold+1,train_info['n_folds'] )\n",
    "    classifiers[ifold] = {}\n",
    "    trn_desc[ifold] = {}\n",
    "    pcds[ifold] = {}\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    freeze_layer = Sequential()\n",
    "    freeze_layer.add(Dense(1, input_dim=norm_all_data.shape[1]))\n",
    "\n",
    "    non_freeze_layer = Sequential()\n",
    "    non_freeze_layer.add(Dense(1, input_dim=norm_all_data.shape[1]))\n",
    "    \n",
    "    merged = Merge([freeze_layer, non_freeze_layer], mode='concat')\n",
    "    model.add(merged)\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(trgt_sparse.shape[1], init='uniform')) \n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=sgd, \n",
    "                  metrics=['accuracy'])\n",
    "    earlyStopping = callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                            patience=25,\n",
    "                                            verbose=0, \n",
    "                                            mode='auto')\n",
    "    # Train model\n",
    "    \n",
    "    train_data = [norm_all_data[train_id], norm_all_data[train_id]]\n",
    "    val_data = [norm_all_data[test_id], norm_all_data[test_id]]\n",
    "    \n",
    "    init_trn_desc = model.fit(train_data, \n",
    "                              trgt_sparse[train_id],\n",
    "                              nb_epoch=50, \n",
    "                              batch_size=8, \n",
    "                              callbacks=[earlyStopping], \n",
    "                              verbose=0, \n",
    "                              validation_data=(val_data,\n",
    "                                               trgt_sparse[test_id]),\n",
    "                              shuffle=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-6b3576e55562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/natmourajr/.virtualenvs/kerasenv/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mget_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;31m# support for legacy behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflattened_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natmourajr/.virtualenvs/kerasenv/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mflattened_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flattened_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Merge'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mmerge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
