{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "# Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "# Laboratorio de Processamento de Sinais - UFRJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 5.50746917725e-05 seconds\n",
      "Time to read data file: 2.58621692657 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "outputpath = os.environ['OUTPUTDATAPATH']\n",
    "main_analysis_path = os.environ['SONAR_WORKSPACE']\n",
    "log_analysis_path = os.environ['PACKAGE_OUTPUT']\n",
    "result_analysis_path = os.environ['PACKAGE_OUTPUT']+'/PCDDeflaction'\n",
    "pict_results_path = os.environ['PACKAGE_OUTPUT']+'/PCDDeflaction/picts'\n",
    "files_results_path = os.environ['PACKAGE_OUTPUT']+'/PCDDeflaction/output_files'\n",
    "\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "\n",
    "subfolder = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "\n",
    "if(not os.path.exists(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))):\n",
    "    print outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "        subfolder,n_pts_fft,decimation_rate)+' doesnt exist...please create it'\n",
    "    exit()\n",
    "    \n",
    "#Read lofar data\n",
    "[data,class_labels] = joblib.load(outputpath+'/'+\n",
    "                                  'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))\n",
    "m_time = time.time()-m_time\n",
    "print 'Time to read data file: '+str(m_time)+' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get data in correct format\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# create a full data vector\n",
    "all_data = {};\n",
    "all_trgt = {};\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    for irun in range(len(data[iclass])):\n",
    "        if len(all_data) == 0:\n",
    "            all_data = data[iclass][irun]['Signal']\n",
    "            all_trgt = (iclass)*np.ones(data[iclass][irun]['Signal'].shape[1])\n",
    "        else:\n",
    "            all_data = np.append(all_data,data[iclass][irun]['Signal'],axis=1)\n",
    "            all_trgt = np.append(all_trgt,(iclass)*np.ones(data[iclass][irun]\n",
    "                                                           ['Signal'].shape[1]),axis=0)\n",
    "\n",
    "all_data = all_data.transpose()\n",
    "\n",
    "# turn targets in sparse mode\n",
    "trgt_sparse = np_utils.to_categorical(all_trgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train process\n",
    "### The train will modify one file and create three different files\n",
    "\n",
    "### Log File:\n",
    "\n",
    "This file will store basic information of all Package's trains and it will guide the analyses file to recognize which train information file should load. In each train this file should be appended with a new line contend the basic information to find the train information file (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n",
    "\n",
    "### Train Information File\n",
    "\n",
    "This file will store full information of the train performed (all parameters) in its name (each train information file will have a different name). And it will guide which train classifier file or which train result file should be open for analysis (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n",
    "\n",
    "### Train Classifier File\n",
    "\n",
    "This file will store the classifier after train, the folds information and the train evolution (when this exists) (NATIVE FORMAT) or (PYTHON FORMAT) - This file should not be access by all programs\n",
    "\n",
    "### Train Result File\n",
    "\n",
    "This file will store the classifier result for all data and classification target (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Process for 0 Fold of 2 Folds\n",
      "Train Process for 1 Fold of 2 Folds\n"
     ]
    }
   ],
   "source": [
    "# Train Process\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(package_name=\"PreProcessing\",analysis_name='PCDDeflaction')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_inits = 1\n",
    "n_pcds = 10\n",
    "norm = 'mapstd'\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_inits'] = n_inits\n",
    "train_info['n_pcds'] = n_pcds\n",
    "train_info['norm'] = norm\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+date+'_train_info.jbl'\n",
    "classifiers_name = result_analysis_path+'/classifiers_files'+'/'+date+'_classifiers'\n",
    "pdf_file_name = result_analysis_path+'/output_files'+'/'+date+'_pcds'\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "CVO = cross_validation.StratifiedKFold(all_trgt, train_info['n_folds'])\n",
    "CVO = list(CVO)\n",
    "train_info['CVO'] = CVO\n",
    "\n",
    "joblib.dump([train_info],train_info_name,compress=9)\n",
    "\n",
    "# train classifiers\n",
    "classifiers = {}\n",
    "trn_desc = {}\n",
    "pcds = {}\n",
    "\n",
    "# try to estimate time to be done...\n",
    "total_trains = train_info['n_folds']*train_info['n_inits']\n",
    "nn_trained = 0\n",
    "\n",
    "for ifold in range(train_info['n_folds']):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    # normalize data based in train set\n",
    "    if train_info['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif train_info['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif train_info['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "        \n",
    "    norm_all_data = scaler.transform(all_data)\n",
    "       \n",
    "    print 'Train Process for %i Fold of %i Folds'%(ifold,train_info['n_folds'] )\n",
    "    pcds[ifold] = {}\n",
    "    for ipcd in range(train_info['n_pcds']):\n",
    "        best_init = 0\n",
    "        best_loss = 999\n",
    "        if ipcd == 0:\n",
    "            # first pcd: regular NN\n",
    "            for i_init in range(train_info['n_inits']):\n",
    "                #print 'Init: %i of %i'%(i_init,train_info['n_inits'])\n",
    "                model = Sequential()\n",
    "                model.add(Dense(all_data.shape[1],\n",
    "                                input_dim=all_data.shape[1], \n",
    "                                init='uniform'),)\n",
    "                model.add(Activation('linear'))\n",
    "                model.add(Dense(1, init='uniform'))\n",
    "                model.add(Activation('tanh'))\n",
    "                model.add(Dense(50, init='uniform'))\n",
    "                model.add(Activation('tanh'))\n",
    "                model.add(Dense(trgt_sparse.shape[1], init='uniform')) \n",
    "                model.add(Activation('tanh'))\n",
    "                \n",
    "                sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "                model.compile(loss='mean_squared_error', optimizer=sgd\n",
    "                      ,metrics=['accuracy'])\n",
    "                \n",
    "                earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=25, \n",
    "                                            verbose=0, mode='auto')\n",
    "                # Train model\n",
    "                init_trn_desc = model.fit(norm_all_data[train_id], trgt_sparse[train_id], \n",
    "                                nb_epoch=50, \n",
    "                                batch_size=8,\n",
    "                                callbacks=[earlyStopping], \n",
    "                                verbose=0,\n",
    "                                validation_data=(all_data[test_id],trgt_sparse[test_id]),\n",
    "                                shuffle=True)\n",
    "                if np.min(init_trn_desc.history['val_loss']) < best_loss:\n",
    "                    best_init = i_init\n",
    "                    best_loss = np.min(init_trn_desc.history['val_loss'])\n",
    "                    classifiers[ifold] = model\n",
    "                    trn_desc[ifold] = init_trn_desc\n",
    "                    pcd_test = model.layers[0].get_weights()\n",
    "                    pcds[ifold][ipcd] = pcd_test[0]\n",
    "                \n",
    "        else:\n",
    "            break\n",
    "            # from second to end: frezze previous and train only last one\n",
    "            for i_init in range(train_info['n_inits']):\n",
    "                #print 'Init: %i of %i'%(i_init,train_info['n_inits'])\n",
    "                \n",
    "classifiers_file = open(classifiers_name+'.pickle', \"wb\")\n",
    "pickle.dump([classifiers,trn_desc],classifiers_file)\n",
    "classifiers_file.close()\n",
    "\n",
    "pdfs_file = open(pdf_file_name+'.pickle', \"wb\")\n",
    "pickle.dump([pcds],pdfs_file)\n",
    "pdfs_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13671943350932139,\n",
       " 0.11903581799181444,\n",
       " 0.11816056331616881,\n",
       " 0.11798252544506592,\n",
       " 0.11767119092284531,\n",
       " 0.11756109150651162,\n",
       " 0.11679834748156499,\n",
       " 0.11457175052499967,\n",
       " 0.11311531101319085,\n",
       " 0.11245439042578953,\n",
       " 0.1121803049749749,\n",
       " 0.11204924891110997,\n",
       " 0.11182465142829989,\n",
       " 0.1116804906263891,\n",
       " 0.1116175043290913,\n",
       " 0.11141844525403924,\n",
       " 0.11130314698661682,\n",
       " 0.11112289180104536,\n",
       " 0.11099181282957146,\n",
       " 0.1110051277867493,\n",
       " 0.11088262431259523,\n",
       " 0.11076527807644466,\n",
       " 0.1107576062734001,\n",
       " 0.11045730272958858,\n",
       " 0.11058870252560248,\n",
       " 0.11053599257253657,\n",
       " 0.11043499493936149,\n",
       " 0.11047695068893672,\n",
       " 0.11039437994109695,\n",
       " 0.11012151630109664,\n",
       " 0.11016656437042688,\n",
       " 0.11016744411692349,\n",
       " 0.11020887540401796,\n",
       " 0.11007888760523943,\n",
       " 0.11003310899299583,\n",
       " 0.11008824100456979,\n",
       " 0.11001645948366801,\n",
       " 0.10999903945739116]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_desc[0].history['loss']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
