{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "## Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "### Autor: Vinícius dos Santos Mello <viniciusdsmello@poli.ufrj.br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating /home/vinicius.mello/Workspace/SonarAnalysis_2/Results/NoveltyDetection/StackedAutoEncoder/outputs/7bec0361cae6311c72f300bfe418d27f980780faaee26f3b6ee4ea9ceccaa265\n",
      "Creating /home/vinicius.mello/Workspace/SonarAnalysis_2/Results/NoveltyDetection/StackedAutoEncoder/outputs/7bec0361cae6311c72f300bfe418d27f980780faaee26f3b6ee4ea9ceccaa265/AnalysisFiles\n",
      "Creating /home/vinicius.mello/Workspace/SonarAnalysis_2/Results/NoveltyDetection/StackedAutoEncoder/outputs/7bec0361cae6311c72f300bfe418d27f980780faaee26f3b6ee4ea9ceccaa265/Pictures\n",
      "Saving /home/vinicius.mello/Workspace/SonarAnalysis_2/Results/NoveltyDetection/StackedAutoEncoder/outputs/7bec0361cae6311c72f300bfe418d27f980780faaee26f3b6ee4ea9ceccaa265/parameters.json\n",
      "[+] Time to read data file: 1.9878411293 seconds\n",
      "Qtd event of A is 12939\n",
      "Qtd event of B is 29352\n",
      "Qtd event of C is 11510\n",
      "Qtd event of D is 23760\n",
      "\n",
      "Biggest class is B with 29352 events\n",
      "Total of events in the dataset is 77561\n",
      "Balacing data...\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (12939, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (29352, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (11510, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (23760, 400)\n",
      "Reading from /home/vinicius.mello/Workspace/SonarAnalysis_2/Results/NoveltyDetection/4_folds_cross_validation_balanced_data.jbl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from noveltyDetectionConfig import CONFIG\n",
    "import numpy as np\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import pprint \n",
    "\n",
    "from Functions.email_utils import EmailConnection, Email\n",
    "from SAENoveltyDetectionAnalysis import SAENoveltyDetectionAnalysis\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "from Functions.telegrambot import Bot\n",
    "\n",
    "my_bot = Bot(\"lisa_thebot\")\n",
    "\n",
    "# Enviroment variables\n",
    "data_path = CONFIG['OUTPUTDATAPATH']\n",
    "results_path = CONFIG['PACKAGE_NAME']\n",
    "\n",
    "training_params = {\n",
    "    \"Technique\": \"StackedAutoEncoder\",\n",
    "    \"TechniqueParameters\": {\n",
    "        \"allow_change_weights\": True #False\n",
    "    },\n",
    "    \"DevelopmentMode\": False,\n",
    "    \"DevelopmentEvents\": 400,\n",
    "    \"NoveltyDetection\": True,\n",
    "    \"InputDataConfig\": {\n",
    "        \"database\": \"4classes\",\n",
    "        \"n_pts_fft\": 1024,\n",
    "        \"decimation_rate\": 3,\n",
    "        \"spectrum_bins_left\": 400,\n",
    "        \"n_windows\": 1,\n",
    "        \"balance_data\": True\n",
    "    },\n",
    "    \"OptmizerAlgorithm\": {\n",
    "        \"name\": \"Adam\",\n",
    "        \"parameters\": {\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"beta_1\": 0.90,\n",
    "            \"beta_2\": 0.999,\n",
    "            \"epsilon\": 1e-08,\n",
    "            \"learning_decay\": 1e-6,\n",
    "            \"momentum\": 0.3,\n",
    "            \"nesterov\": True\n",
    "        }\n",
    "    },\n",
    "    \"HyperParameters\": {\n",
    "        \"n_folds\": 4,#10, #4,\n",
    "        \"n_epochs\": 500,#500, #300,\n",
    "        \"n_inits\": 1,#1, #2,\n",
    "        \"batch_size\": 128,#128, #256,\n",
    "        \"kernel_initializer\": \"uniform\",\n",
    "        \"encoder_activation_function\": \"tanh\", #\"relu\",\n",
    "        \"decoder_activation_function\": \"linear\",\n",
    "        \"classifier_output_activation_function\": \"softmax\",\n",
    "        \"norm\": \"mapstd\",\n",
    "        \"metrics\": [\"accuracy\"],\n",
    "        \"loss\": \"mean_squared_error\",\n",
    "        \"dropout\": False,\n",
    "        \"dropout_parameter\": 0.00,\n",
    "        \"regularization\": None,\n",
    "        \"regularization_parameter\": 0.00\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"EarlyStopping\": {\n",
    "            \"patience\": 30,\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "analysis = SAENoveltyDetectionAnalysis(parameters=training_params,\n",
    "                                       model_hash=\"\",\n",
    "                                       load_hash=False, load_data=True, verbose=True)\n",
    "all_data, all_trgt, trgt_sparse = analysis.getData()\n",
    "\n",
    "SAE = analysis.createSAEModels()\n",
    "\n",
    "trn_data = analysis.trn_data\n",
    "trn_trgt = analysis.trn_trgt\n",
    "trn_trgt_sparse = analysis.trn_trgt_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7bec0361cae6311c72f300bfe418d27f980780faaee26f3b6ee4ea9ceccaa265\n",
      "/home/vinicius.mello/Workspace/SonarAnalysis_2/Results/NoveltyDetection/StackedAutoEncoder/outputs/7bec0361cae6311c72f300bfe418d27f980780faaee26f3b6ee4ea9ceccaa265\n",
      "{'DevelopmentEvents': 400,\n",
      " 'DevelopmentMode': False,\n",
      " 'HyperParameters': {'batch_size': 128,\n",
      "                     'classifier_output_activation_function': 'softmax',\n",
      "                     'decoder_activation_function': 'linear',\n",
      "                     'dropout': False,\n",
      "                     'dropout_parameter': 0.0,\n",
      "                     'encoder_activation_function': 'tanh',\n",
      "                     'kernel_initializer': 'uniform',\n",
      "                     'loss': 'mean_squared_error',\n",
      "                     'metrics': ['accuracy'],\n",
      "                     'n_epochs': 500,\n",
      "                     'n_folds': 4,\n",
      "                     'n_inits': 1,\n",
      "                     'norm': 'mapstd',\n",
      "                     'regularization': None,\n",
      "                     'regularization_parameter': 0.0},\n",
      " 'InputDataConfig': {'balance_data': True,\n",
      "                     'database': '4classes',\n",
      "                     'decimation_rate': 3,\n",
      "                     'n_pts_fft': 1024,\n",
      "                     'n_windows': 1,\n",
      "                     'spectrum_bins_left': 400},\n",
      " 'NoveltyDetection': True,\n",
      " 'OptmizerAlgorithm': {'name': 'Adam',\n",
      "                       'parameters': {'beta_1': 0.9,\n",
      "                                      'beta_2': 0.999,\n",
      "                                      'epsilon': 1e-08,\n",
      "                                      'learning_decay': 1e-06,\n",
      "                                      'learning_rate': 0.001,\n",
      "                                      'momentum': 0.3,\n",
      "                                      'nesterov': True}},\n",
      " 'Technique': 'StackedAutoEncoder',\n",
      " 'TechniqueParameters': {'allow_change_weights': True},\n",
      " 'callbacks': {'EarlyStopping': {'monitor': 'val_loss', 'patience': 30}}}\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "print analysis.model_hash\n",
    "print analysis.getBaseResultsPath()\n",
    "pp.pprint(analysis.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testar topologia\n",
    "1_inits_mapstd_norm_500_epochs_128_batch_size_tanh_hidden_activation_linear_output_activation_accuracy_metric_mean_squared_error_loss\n",
    "\n",
    "Trainable = True\n",
    "\n",
    "(n_inits=1,\n",
    "hidden_activation='tanh', # others tanh, relu, sigmoid, linear \n",
    "output_activation='linear',\n",
    "n_epochs=500, #500\n",
    "patience=30, # 30\n",
    "batch_size=128, #128\n",
    "verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = analysis.trn_data\n",
    "trn_trgt = analysis.trn_trgt\n",
    "trn_trgt_sparse = analysis.trn_trgt_sparse\n",
    "\n",
    "inovelty = 0\n",
    "\n",
    "data=trn_data[inovelty]\n",
    "trgt=trn_trgt[inovelty]\n",
    "\n",
    "ifold, classifier, trn_desc = SAE[inovelty].train_classifier(data  = data,\n",
    "                                                             trgt  = trgt,\n",
    "                                                             ifold = 0,\n",
    "                                                             hidden_neurons = [200,1],\n",
    "                                                             layer = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python sae_train.py --layer 1 --novelty 0 --finetunning 1 --threads 10 --type neuronSweep --hiddenNeurons 400 --neuronsVariationStep 100 --modelhash 7bec0361cae6311c72f300bfe418d27f980780faaee26f3b6ee4ea9ceccaa265\n"
     ]
    }
   ],
   "source": [
    "for inovelty in range(len(analysis.class_labels)):\n",
    "    startTime = time.time()\n",
    "    \n",
    "    data=trn_data[inovelty]\n",
    "    trgt=trn_trgt[inovelty]\n",
    "\n",
    "    analysis.train(layer=1,\n",
    "                   inovelty=inovelty,\n",
    "                   fineTuning=True,\n",
    "                   trainingType=\"neuronSweep\", #foldSweep, neuronSweep, normal\n",
    "                   hidden_neurons=[400],\n",
    "                   neurons_variation_step=100,\n",
    "                   numThreads=10,\n",
    "                   model_hash=analysis.model_hash)\n",
    "    \n",
    "    duration = str(timedelta(seconds=float(time.time() - startTime)))\n",
    "    message = \"Technique: Stacked Autoencoder\\n\"\n",
    "    message = message + \"Training Type: Neurons Sweep\\n\"\n",
    "    message = message + \"Novelty Class: {}\\n\".format(analysis.class_labels[inovelty])\n",
    "    message = message + \"Duration: {}\\n\".format(duration)\n",
    "    try:\n",
    "        my_bot.sendMessage(message)\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao enviar mensagem. Erro: \" + str(e))\n",
    "    print \"The training of the model for novelty class {0} took {1} to be performed\\n\".format(analysis.class_labels[inovelty], duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Neuron variation x MSE Divergence\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from Functions.StatisticalAnalysis import KLDiv, EstPDF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "# hidden_neurons = range(400,0,-50) + [2]\n",
    "hidden_neurons = [400,200]\n",
    "\n",
    "step = 100\n",
    "neurons_mat = [1] + range(step,hidden_neurons[layer-1]+step,step)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "analysis_name = 'mean_squared_error_%i_layer'%(layer)\n",
    "analysis_file = os.path.join(analysis.getBaseResultsPath(), \"AnalysisFiles\", analysis_name + \".jbl\")    \n",
    "\n",
    "# os.remove(analysis_file)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "figsize = (10,5)\n",
    "\n",
    "trn_desc = np.zeros([len(analysis.class_labels), analysis.parameters[\"HyperParameters\"][\"n_folds\"]])\n",
    "\n",
    "for inovelty in range(len(analysis.class_labels)):\n",
    "    def loadData(ifold):\n",
    "        return  SAE[inovelty].load_classifier(data  = analysis.trn_data[inovelty],\n",
    "                                              trgt  = analysis.trn_trgt[inovelty], \n",
    "                                              hidden_neurons = hidden_neurons[:layer],\n",
    "                                              layer = layer,\n",
    "                                              ifold = ifold\n",
    "                                              )\n",
    "\n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    if verbose:\n",
    "        print '[*] Loading training history...'\n",
    "    try:\n",
    "        results = p.map(loadData, range(analysis.parameters[\"HyperParameters\"][\"n_folds\"]))\n",
    "        p.close()\n",
    "        p.join()\n",
    "        for ifold in range(analysis.parameters[\"HyperParameters\"][\"n_folds\"]):\n",
    "            trn_desc[inovelty, results_path[ifold][0]] = results[2]\n",
    "    except Exception as e: \n",
    "        p.close()\n",
    "        p.join()          \n",
    "        print(\"Erro: {}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot results    \n",
    "fig = plt.subplots(figsize=figsize)\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "mean_acc = np.mean(trn_desc)\n",
    "error_acc = np.std(trn_desc)\n",
    "\n",
    "ax.plot(neurons_mat, mean_sp, color='b', alpha=0.7, linewidth=2.5, label='SP Index Test Data')\n",
    "\n",
    "ax.fill_between(neurons_mat, mean_sp+error_sp, mean_sp-error_sp, facecolor='blue', alpha=0.3)\n",
    "\n",
    "ax.set_title('MSE (Class {} as novelty) - Layer {}'.format(analysis.class_labels[inovelty], layer),\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Mean Squared Error', fontsize=20)\n",
    "ax.set_xlabel('Neurons', fontsize=20)\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error analysis for Pre-training Step with a neuron variation at autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron variation x MSE Divergence\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from Functions.StatisticalAnalysis import KLDiv, EstPDF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "# hidden_neurons = range(400,0,-50) + [2]\n",
    "hidden_neurons = [400,200]\n",
    "\n",
    "step = 100\n",
    "neurons_mat = [1] + range(step,hidden_neurons[layer-1]+step,step)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "analysis_name = 'mean_squared_error_%i_layer'%(layer)\n",
    "analysis_file = os.path.join(analysis.getBaseResultsPath(), \"AnalysisFiles\", analysis_name + \".jbl\")    \n",
    "\n",
    "# os.remove(analysis_file)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "figsize = (10,5)\n",
    "\n",
    "mse = {}\n",
    "mse_known = np.zeros([len(analysis.class_labels), analysis.n_folds, len(neurons_mat)])\n",
    "mse_novelty = np.zeros([len(analysis.class_labels), analysis.n_folds, len(neurons_mat)])\n",
    "\n",
    "if not os.path.exists(analysis_file):\n",
    "    for inovelty in range(len(analysis.class_labels)):\n",
    "        n_bins = 100\n",
    "        for ineuron in neurons_mat: \n",
    "            if ineuron == 0:\n",
    "                ineuron = 1\n",
    "            def getMSE(ifold):\n",
    "                train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "\n",
    "                # normalize known classes\n",
    "                if analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd\":\n",
    "                    scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "                elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd_rob\":\n",
    "                    scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "                elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapminmax\":\n",
    "                    scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "                known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "                novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "\n",
    "                model = SAE[inovelty].get_model(data=all_data, trgt=all_trgt,\n",
    "                                                hidden_neurons=hidden_neurons[:layer-1]+[ineuron],\n",
    "                                                layer=layer, ifold=ifold)\n",
    "\n",
    "                known_output = model.predict(known_data)\n",
    "                novelty_output = model.predict(novelty_data)\n",
    "\n",
    "                mseKnown = metrics.mean_squared_error(known_data, known_output)\n",
    "                mseNovelty = metrics.mean_squared_error(novelty_data, novelty_output)\n",
    "\n",
    "                return ifold, mseKnown, mseNovelty\n",
    "\n",
    "            # Start Parallel processing\n",
    "            p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "            folds = range(len(analysis.CVO[inovelty]))\n",
    "            if verbose:\n",
    "                print '[*] Calculating Mean Squared Error ...'\n",
    "            mse[ineuron] = p.map(getMSE, folds)\n",
    "\n",
    "            for ifold in range(analysis.n_folds):\n",
    "                mse_known[inovelty,:, neurons_mat.index(ineuron)] = mse[ineuron][ifold][1]\n",
    "                mse_novelty[inovelty,:, neurons_mat.index(ineuron)] = mse[ineuron][ifold][2]\n",
    "\n",
    "            p.close()\n",
    "            p.join()\n",
    "    \n",
    "    joblib.dump([neurons_mat,mse_known,mse_novelty],analysis_file,compress=9)\n",
    "else:\n",
    "    [neurons_mat, mse_known, mse_novelty] = joblib.load(analysis_file)\n",
    "\n",
    "for inovelty in range(len(analysis.class_labels)):\n",
    "    \n",
    "    # Plot results    \n",
    "    fig = plt.subplots(figsize=figsize)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "\n",
    "    ax.errorbar(neurons_mat, np.mean(mse_known[inovelty], axis=0),\n",
    "                np.std(mse_known[inovelty], axis=0),fmt='o-',\n",
    "                color='b',alpha=0.7,linewidth=2.5,\n",
    "                label='MSE Known Test Data')\n",
    "    ax.errorbar(neurons_mat, np.mean(mse_novelty[inovelty], axis=0),\n",
    "                np.std(mse_novelty[inovelty], axis=0),fmt='.--',\n",
    "                color='r',alpha=0.7,linewidth=2.5,\n",
    "                label='MSE Novelty Data')\n",
    "\n",
    "    ax.set_title('MSE x Neurons (Class {} as novelty) - Layer {}'.format(analysis.class_labels[inovelty], layer),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax.set_ylabel('Mean Squared Error', fontsize=20)\n",
    "    ax.set_xlabel('Neurons', fontsize=20)\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Save the figure\n",
    "# file_name = pict_results_path+'/'+current_analysis+'_%i_novelty_%s_neurons_'%(inovelty,neurons_str)+trn_params.get_params_str()+'.pdf'\n",
    "# plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information analysis for Pre-training Step with a neuron variation at autoencoder"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from Functions.mutual_info import mutual_information\n",
    "\n",
    "# Neuron variation x Mutual Information\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from Functions.StatisticalAnalysis import KLDiv, EstPDF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "# hidden_neurons = [200,200]\n",
    "\n",
    "step = 100\n",
    "neurons_mat = [1] + range(step,hidden_neurons[layer-1]+step,step)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "analysis_name = 'mutual_information_%i_layer_keras_class_weights'%(layer)\n",
    "analysis_file = os.path.join(analysis.getBaseResultsPath(), \"AnalysisFiles\", analysis_name + \".jbl\")    \n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "figsize = (10,5)\n",
    "\n",
    "mutual_info = {}\n",
    "mutual_info_known = np.zeros([len(analysis.class_labels), analysis.n_folds, len(neurons_mat)])\n",
    "mutual_info_novelty = np.zeros([len(analysis.class_labels), analysis.n_folds, len(neurons_mat)])\n",
    "\n",
    "k_value = 1\n",
    "\n",
    "if not os.path.exists(analysis_file):\n",
    "    for inovelty in range(len(analysis.class_labels)):\n",
    "        n_bins = 100\n",
    "        for ineuron in neurons_mat: \n",
    "            if ineuron == 0:\n",
    "                ineuron = 1\n",
    "            def getMI(ifold):\n",
    "                train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "\n",
    "                # normalize known classes\n",
    "                if analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd\":\n",
    "                    scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "                elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd_rob\":\n",
    "                    scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "                elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapminmax\":\n",
    "                    scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "                known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "                novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "\n",
    "                model = SAE[inovelty].get_model(data=all_data, trgt=all_trgt,\n",
    "                                                hidden_neurons=hidden_neurons[:layer-1]+[ineuron],\n",
    "                                                layer=layer, ifold=ifold)\n",
    "\n",
    "                known_output = model.predict(known_data)\n",
    "                novelty_output = model.predict(novelty_data)\n",
    "                \n",
    "                mutual_info_knownData = mutual_information((known_data, known_output), k=k_value)\n",
    "                mutual_info_noveltyData = mutual_information((novelty_data, novelty_output), k=k_value)\n",
    "                \n",
    "                return ifold, mutual_info_knownData, mutual_info_noveltyData\n",
    "\n",
    "            # Start Parallel processing\n",
    "            p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "            folds = range(len(analysis.CVO[inovelty]))\n",
    "            if verbose:\n",
    "                print '[*] Calculating Mutal Information ...'\n",
    "            mutual_info[ineuron] = p.map(getMI, folds)\n",
    "\n",
    "            for ifold in range(analysis.n_folds):\n",
    "                mutual_info_known[inovelty,:, neurons_mat.index(ineuron)] = mutual_info[ineuron][ifold][1]\n",
    "                mutual_info_novelty[inovelty,:, neurons_mat.index(ineuron)] = mutual_info[ineuron][ifold][2]\n",
    "\n",
    "            p.close()\n",
    "            p.join()\n",
    "    \n",
    "    joblib.dump([neurons_mat,mutual_info_known,mutual_info_novelty],analysis_file,compress=9)\n",
    "else:\n",
    "    [neurons_mat, mse_known, mse_novelty] = joblib.load(analysis_file)\n",
    "\n",
    "for inovelty in range(len(analysis.class_labels)):\n",
    "    \n",
    "    # Plot results    \n",
    "    fig = plt.subplots(figsize=figsize)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "\n",
    "    ax.errorbar(neurons_mat, np.mean(mutual_info_known[inovelty], axis=0),\n",
    "                np.std(mutual_info_known[inovelty], axis=0),fmt='o-',\n",
    "                color='b',alpha=0.7,linewidth=2.5,\n",
    "                label='Mutual Info Known Test Data')\n",
    "    ax.errorbar(neurons_mat, np.mean(mutual_info_novelty[inovelty], axis=0),\n",
    "                np.std(mutual_info_novelty[inovelty], axis=0),fmt='.--',\n",
    "                color='r',alpha=0.7,linewidth=2.5,\n",
    "                label='Mutual Info Novelty Data')\n",
    "\n",
    "    ax.set_title('Mutual Info x Neurons (Class {} as novelty) - Layer {}'.format(analysis.class_labels[inovelty], layer),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax.set_ylabel('Mutual Information', fontsize=20)\n",
    "    ax.set_xlabel('Neurons', fontsize=20)\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Save the figure\n",
    "# file_name = pict_results_path+'/'+current_analysis+'_%i_novelty_%s_neurons_'%(inovelty,neurons_str)+trn_params.get_params_str()+'.pdf'\n",
    "# plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot of Original data vs Reconstructed data for the features with the highest mean squared error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Neuron variation x MSE Divergence\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from Functions.StatisticalAnalysis import KLDiv, EstPDF\n",
    "from Functions import FunctionsDataVisualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "#hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "hidden_neurons = [200]\n",
    "\n",
    "neurons_mat = [1] + range(step,hidden_neurons[layer-1]+step,step)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "analysis_name = 'scatter_plot_%i_layer_keras_class_weights'%(layer)\n",
    "analysis_file = os.path.join(analysis.getBaseResultsPath(), \"AnalysisFiles\", analysis_name + \".jbl\")    \n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Choose model\n",
    "inovelty = 0\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "\n",
    "neurons_str = SAE[inovelty].get_neurons_str(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "models = {}\n",
    "known_outputs = {}\n",
    "novelty_outputs = {}\n",
    "mean = {}\n",
    "indexes = {}\n",
    "\n",
    "for ifold in range(analysis.parameters[\"HyperParameters\"][\"n_folds\"]):\n",
    "    train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "\n",
    "    # normalize known classes\n",
    "    if analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd\":\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd_rob\":\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapminmax\":\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "        \n",
    "    norm_data = scaler.transform(all_data)\n",
    "    known_data = scaler.transform(trn_data[inovelty][test_id,:])\n",
    "    novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "    \n",
    "    if ifold == 0:\n",
    "        diffSquared = np.zeros([analysis.parameters[\"HyperParameters\"][\"n_folds\"], all_data.shape[0], all_data.shape[1]])\n",
    "        \n",
    "    print 'Novelty class: %i - Topology: %s - fold %i'%(inovelty, neurons_str, ifold)\n",
    "    \n",
    "    models[ifold] = SAE[inovelty].get_model(data=trn_data[inovelty], trgt=trn_trgt[inovelty],\n",
    "                                            hidden_neurons=hidden_neurons,\n",
    "                                            layer=layer, ifold=ifold)\n",
    "    \n",
    "    outputs = models[ifold].predict(norm_data)\n",
    "    known_outputs[ifold] = models[ifold].predict(known_data)\n",
    "    novelty_outputs[ifold] = models[ifold].predict(novelty_data)\n",
    "    \n",
    "    diffSquared[ifold,:,:] = np.power((norm_data - outputs), 2)\n",
    "\n",
    "# Get the mean squared error on the folds    \n",
    "mean = np.mean(np.mean(diffSquared[:,test_id,:], axis=0), axis=0)\n",
    "\n",
    "# Sort the indexes that has the highest mean squared error\n",
    "indexes = np.argsort(mean)[::-1]\n",
    "\n",
    "for ifold in range(analysis.parameters[\"HyperParameters\"][\"n_folds\"]):\n",
    "    train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "\n",
    "    # normalize known classes\n",
    "    if analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd\":\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd_rob\":\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapminmax\":\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    norm_data = scaler.transform(all_data)\n",
    "    known_data = scaler.transform(trn_data[inovelty][test_id,:])\n",
    "    novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "    \n",
    "    known_points = known_data.shape[0]\n",
    "    novelty_points = novelty_data.shape[0]\n",
    "    \n",
    "    # Number of dimensions to analyse (even number is better!)\n",
    "    num_dim = 4\n",
    "    fig, m_ax = plt.subplots(figsize=(20,20),nrows=2, ncols=2)\n",
    "    for choose_index in range(num_dim):  \n",
    "        ax = plt.subplot(2,2,choose_index+1)\n",
    "        \n",
    "        # Plot novelty \n",
    "        ax.plot(novelty_data[:,indexes[choose_index]][0], \n",
    "                novelty_outputs[ifold][:,indexes[choose_index]][0],\n",
    "                \"r.\", label='Novelty Class', markersize=20)\n",
    "        ax.plot(novelty_data[:,indexes[choose_index]][:novelty_points], \n",
    "                novelty_outputs[ifold][:,indexes[choose_index]][:novelty_points],\n",
    "                \"r.\", alpha=0.3)\n",
    "        \n",
    "        # Plot known classes\n",
    "        ax.plot(known_data[:,indexes[choose_index]][0],\n",
    "                known_outputs[ifold][:,indexes[choose_index]][0],\n",
    "                \"b.\", label='Known Class', markersize=20)\n",
    "        ax.plot(known_data[:,indexes[choose_index]][:known_points],\n",
    "                known_outputs[ifold][:,indexes[choose_index]][:known_points],\n",
    "                \"b.\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "        ax.set_title('Input x Output - Layer - %i - Dim %i'%(layer,indexes[choose_index]),fontsize=22, fontweight='bold')\n",
    "        ax.set_xlim(-2, 7)\n",
    "        ax.set_ylim(-4, 8)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        # sort both labels and handles by labels\n",
    "        labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "        plt.grid() \n",
    "        \n",
    "        # Small plot\n",
    "        rect = [0.065, 0.55, 0.35, 0.4]\n",
    "        ax1 = FunctionsDataVisualization.add_subplot_axes(ax,rect)\n",
    "        \n",
    "        eq_known_reconstruction = np.power((known_data[:,indexes[choose_index]] - known_outputs[ifold][:,indexes[choose_index]]), 2)\n",
    "        eq_novelty_reconstruction = np.power((novelty_data[:,indexes[choose_index]] - novelty_outputs[ifold][:,indexes[choose_index]]), 2)\n",
    "        \n",
    "        mq_bins_known = np.linspace(np.min(eq_known_reconstruction), np.max(eq_known_reconstruction), 50)\n",
    "        mq_bins_novelty = np.linspace(np.min(eq_novelty_reconstruction), np.max(eq_novelty_reconstruction), 50)\n",
    "             \n",
    "        n, bins, patches = ax1.hist(eq_novelty_reconstruction,bins=mq_bins_novelty,\n",
    "                                    fc=\"r\",\n",
    "                                    alpha=0.5, normed=0)\n",
    "        \n",
    "        n, bins, patches = ax1.hist(eq_known_reconstruction,bins=mq_bins_known,\n",
    "                                    fc=\"b\",\n",
    "                                    alpha=0.8, normed=0)   \n",
    "        ax1.set_xlim(0, 0.06)\n",
    "        ax1.set_title(\"Squared Absolute Error\",fontsize=14, fontweight='bold')\n",
    "        ax1.grid() \n",
    "        \n",
    "        # Small plot\n",
    "        rect = [0.5, 0.05, 0.45, 0.4]\n",
    "        ax2 = FunctionsDataVisualization.add_subplot_axes(ax,rect)\n",
    "        \n",
    "        e_known_reconstruction = (known_data[:,indexes[choose_index]] - known_outputs[ifold][:,indexes[choose_index]])\n",
    "        e_novelty_reconstruction = (novelty_data[:,indexes[choose_index]] - novelty_outputs[ifold][:,indexes[choose_index]])\n",
    "        \n",
    "        m_bins_known = np.linspace(np.min(e_known_reconstruction), np.max(e_known_reconstruction), 50)\n",
    "        m_bins_novelty = np.linspace(np.min(e_novelty_reconstruction), np.max(e_novelty_reconstruction), 50)\n",
    "        \n",
    "        n, bins, patches = ax2.hist(e_novelty_reconstruction,bins=m_bins_novelty,\n",
    "                                    fc=\"r\",\n",
    "                                    alpha=0.6, normed=0)\n",
    "        \n",
    "        n, bins, patches = ax2.hist(e_known_reconstruction,bins=m_bins_known,\n",
    "                                    fc=\"b\",\n",
    "                                    alpha=0.8, normed=0)   \n",
    "        ax2.grid()\n",
    "        ax2.set_title(\"Absolute Error\",fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlim(-0.4,0.4) \n",
    "        \n",
    "        ax.legend(handles, labels, ncol=1, loc='upper right')\n",
    "        plt.legend()\n",
    "\n",
    "        mse = metrics.mean_squared_error(known_data[:,indexes[choose_index]], known_outputs[ifold][:,indexes[choose_index]])\n",
    "        ax.text(2, 6, 'MSE: %f'%mse, style='normal',fontsize=26, color='blue',\n",
    "        bbox={'alpha':0.0, 'pad':10})\n",
    "        \n",
    "        mse = metrics.mean_squared_error(novelty_data[:,indexes[choose_index]], novelty_outputs[ifold][:,indexes[choose_index]])\n",
    "        ax.text(2, 5.5, 'MSE: %f'%mse, style='normal',fontsize=26, color='red',\n",
    "        bbox={'alpha':0.0, 'pad':10})\n",
    "        \n",
    "        #Save the figure\n",
    "#         file_name = pict_results_path+'/'+current_analysis+'_%i_novelty_%i_layer_%s_neurons_%i_fold_'%(inovelty,\n",
    "#                                                                                                        layer,\n",
    "#                                                                                                        neurons_str,\n",
    "#                                                                                                        ifold)+trn_params.get_params_str()+'.pdf'\n",
    "#         plt.savefig(file_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction of the Lofargram of a novelty class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# LOFARGram for reconstructed input\n",
    "current_analysis = 'LOFARGram_reconstruction_first_layer'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "# Choose neurons topology\n",
    "ineuron = 400\n",
    "\n",
    "# Choose model\n",
    "inovelty = 1\n",
    "\n",
    "# Choose num of lines to plot\n",
    "points = 100\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "    \n",
    "for ifold in range(len(CVO[inovelty])):\n",
    "    train_id, test_id = CVO[inovelty][ifold]\n",
    "    \n",
    "    # normalize known classes\n",
    "    known_data = all_data[all_trgt!=inovelty]\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data)\n",
    "\n",
    "    # Get the model file\n",
    "    model_str = '%s/%s/%s_%i_novelty_%i_folds_%s_400x%i_neurons'%(results_path,analysis_str,\n",
    "                                                               model_prefix_str,inovelty,\n",
    "                                                               n_folds,params_str,\n",
    "                                                               ineuron)\n",
    "\n",
    "    if not development_flag:        \n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    print 'Loading %s'%file_name\n",
    "    model = load_model(file_name)\n",
    "    \n",
    "    all_output = model.predict(norm_data[all_trgt==inovelty])\n",
    "\n",
    "    m_fontsize = 12\n",
    "\n",
    "    fig, subplot_array = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\n",
    "    for i in range(2):\n",
    "        ax = plt.subplot(1,2,i+1)\n",
    "        if i == 0:\n",
    "            plt.imshow(norm_data[all_trgt==inovelty,:],\n",
    "               cmap=\"jet\",extent=[1, 400, norm_data[all_trgt==inovelty,:].shape[0],1],\n",
    "               aspect=\"auto\")\n",
    "            plt.xlabel('Frequencies (Hz)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "            plt.title('Lofar Analysis for Original Data of %s'%(class_labels[inovelty]), fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        if (i == 1):\n",
    "            plt.ylabel('Time (seconds)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "            plt.imshow(all_output,\n",
    "               cmap=\"jet\",extent=[1, 400, all_output.shape[0],1],\n",
    "               aspect=\"auto\")\n",
    "            plt.title('Lofar Analysis for Reconstructed Data of %s'%(class_labels[inovelty]), fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        \n",
    "        # Plot lines at frequencies with high errors\n",
    "        for index in indexes[:points]:\n",
    "            plt.axvline(index, color='r', alpha=0.7)\n",
    "            \n",
    "        plt.xticks(np.linspace(0,400,9),rotation=45)\n",
    "        cbar = plt.colorbar()\n",
    "        plt.clim(-2,9)\n",
    "        #if ((iclass == 1) or (iclass==3)):\n",
    "        cbar.ax.set_ylabel('dB',fontweight='bold') \n",
    "        #Save the figure\n",
    "        file_name = pict_results_path+'/'+current_analysis+'_%i_inovelty'%inovelty+'_%i'%ifold+'_fold'+'_%s'%neurons_str+'_neurons_'+trn_params.get_params_str()+'.pdf'\n",
    "        plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SP Index analysis for a Fine Tuning Step with a neuron variation at autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from Functions.StatisticalAnalysis import KLDiv, EstPDF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = [400]\n",
    "\n",
    "step = 100\n",
    "neurons_mat = [1] + range(step,hidden_neurons[layer-1]+step,step)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "analysis_name = 'sp_index_%i_layer'%(layer)\n",
    "analysis_file = os.path.join(analysis.getBaseResultsPath(), \"AnalysisFiles\", analysis_name + \".jbl\")    \n",
    "\n",
    "if os.path.exists(analysis_file):\n",
    "    os.remove(analysis_file)\n",
    "    \n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "figsize = (10,5)\n",
    "\n",
    "\n",
    "results = {}\n",
    "spIndex = np.zeros([len(analysis.class_labels), analysis.parameters[\"HyperParameters\"][\"n_folds\"], len(neurons_mat)])\n",
    "\n",
    "if not os.path.exists(analysis_file):\n",
    "    for inovelty in range(len(analysis.class_labels)):\n",
    "        folds = range(len(analysis.CVO[inovelty]))\n",
    "        for ifold in folds:    \n",
    "            class_eff_mat = np.zeros([analysis.parameters[\"HyperParameters\"][\"n_folds\"],len(np.unique(all_trgt))])\n",
    "            known_sp_mat = np.zeros([analysis.parameters[\"HyperParameters\"][\"n_folds\"]])\n",
    "\n",
    "            buff = np.zeros([len(np.unique(all_trgt))-1])\n",
    "            class_eff = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "            known_sp = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "\n",
    "            def getSP(ineuron):\n",
    "                train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "\n",
    "                # normalize known classes\n",
    "                if analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd\":\n",
    "                    scaler = preprocessing.StandardScaler().fit(analysis.trn_data[inovelty][train_id,:])\n",
    "                elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd_rob\":\n",
    "                    scaler = preprocessing.RobustScaler().fit(analysis.trn_data[inovelty][train_id,:])\n",
    "                elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapminmax\":\n",
    "                    scaler = preprocessing.MinMaxScaler().fit(analysis.trn_data[inovelty][train_id,:])\n",
    "\n",
    "                known_data = scaler.transform(analysis.trn_data[inovelty][test_id,:])\n",
    "                known_trgt = analysis.trn_trgt[inovelty][test_id]\n",
    "                \n",
    "                classifier = SAE[inovelty].load_classifier(data  = analysis.trn_data[inovelty],\n",
    "                                                           trgt  = analysis.trn_trgt[inovelty], \n",
    "                                                           hidden_neurons = hidden_neurons[:layer-1]+[ineuron],\n",
    "                                                           layer = layer,\n",
    "                                                           ifold = ifold)\n",
    "\n",
    "                output = classifier.predict(known_data)\n",
    "                \n",
    "                num_known_classes = trn_trgt_sparse[inovelty].shape[1]\n",
    "                thr_value = 0.2\n",
    "                for iclass, class_id in enumerate(np.unique(all_trgt)):\n",
    "                    if iclass == inovelty:\n",
    "                        continue\n",
    "                    output_of_class_events = output[known_trgt==iclass-(iclass>inovelty),:]\n",
    "                    correct_class_output = np.argmax(output_of_class_events,axis=1)==iclass-(iclass>inovelty)\n",
    "                    output_above_thr = output_of_class_events[correct_class_output,iclass-(iclass>inovelty)]>thr_value\n",
    "                    class_eff = float(np.sum(output_above_thr))/float(output_of_class_events.shape[0])\n",
    "                    buff[iclass-(iclass>inovelty)] = class_eff\n",
    "\n",
    "                sp_index = (np.sqrt(np.mean(buff,axis=0)*np.power(np.prod(buff),1./float(len(buff)))))\n",
    "\n",
    "                \n",
    "                return ineuron, sp_index\n",
    "\n",
    "            # Start Parallel processing\n",
    "            p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "            if verbose:\n",
    "                print '[*] Calculating SP Index ...'\n",
    "            try:\n",
    "                results = p.map(getSP, neurons_mat)\n",
    "                p.close()\n",
    "                p.join()\n",
    "                for ineuron_index in range(len(neurons_mat)):\n",
    "                    spIndex[inovelty, ifold, neurons_mat.index(results[ineuron_index][0])] = results[ineuron_index][1]\n",
    "            except Exception as e: \n",
    "                p.close()\n",
    "                p.join()          \n",
    "                print(\"Erro: {}\".format(str(e)))\n",
    "                break\n",
    "    joblib.dump([neurons_mat,spIndex],analysis_file,compress=9)\n",
    "else:\n",
    "    [neurons_mat, spIndex] = joblib.load(analysis_file)\n",
    "\n",
    "    \n",
    "for inovelty in range(len(analysis.class_labels)):\n",
    "    # Plot results    \n",
    "    fig = plt.subplots(figsize=figsize)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    mean_sp = np.mean(spIndex[inovelty,:], axis=0)\n",
    "    error_sp = np.std(spIndex[inovelty,:,:], axis=0)\n",
    "    \n",
    "    ax.plot(neurons_mat, mean_sp, color='b', alpha=0.7, linewidth=2.5, label='SP Index Test Data')\n",
    "    \n",
    "    ax.fill_between(neurons_mat, mean_sp+error_sp, mean_sp-error_sp, facecolor='blue', alpha=0.3)\n",
    "    \n",
    "    ax.set_title('SP Index x Neurons (Class {} as novelty)'.format(analysis.class_labels[inovelty]),\n",
    "                                  fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('SP Index', fontsize=22)\n",
    "    ax.set_xlabel('Neurons', fontsize=22)\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "inovelty = 0\n",
    "iclass = 1\n",
    "ifold = 2\n",
    "\n",
    "train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "\n",
    "# normalize known classes\n",
    "if analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd\":\n",
    "    scaler = preprocessing.StandardScaler().fit(analysis.trn_data[inovelty][train_id,:])\n",
    "elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd_rob\":\n",
    "    scaler = preprocessing.RobustScaler().fit(analysis.trn_data[inovelty][train_id,:])\n",
    "elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapminmax\":\n",
    "    scaler = preprocessing.MinMaxScaler().fit(analysis.trn_data[inovelty][train_id,:])\n",
    "\n",
    "known_data = scaler.transform(analysis.trn_data[inovelty][test_id,:])\n",
    "known_trgt = analysis.trn_trgt[inovelty][test_id]\n",
    "\n",
    "print np.unique(known_trgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inovelty = 0\n",
    "iclass = 2\n",
    "ifold = 2\n",
    "\n",
    "train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "\n",
    "# normalize known classes\n",
    "if analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd\":\n",
    "    scaler = preprocessing.StandardScaler().fit(analysis.trn_data[inovelty][train_id,:])\n",
    "elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd_rob\":\n",
    "    scaler = preprocessing.RobustScaler().fit(analysis.trn_data[inovelty][train_id,:])\n",
    "elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapminmax\":\n",
    "    scaler = preprocessing.MinMaxScaler().fit(analysis.trn_data[inovelty][train_id,:])\n",
    "\n",
    "known_data = scaler.transform(analysis.trn_data[inovelty][test_id,:])\n",
    "known_trgt = analysis.trn_trgt[inovelty][test_id]\n",
    "\n",
    "print known_trgt[known_trgt==iclass]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) Curve for SP/Trigger with Novelty Detection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "# Choose layer\n",
    "layer = 1\n",
    "inovelty = 1\n",
    "\n",
    "# Choose neurons topology for SAE\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "current_analysis = 'figures_of_merit'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_novelty_detection_%s_neurons_thr_sweep.jbl'%(results_path,analysis_str,analysis_name, neurons_str)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "\n",
    "    thr_mat = np.round(np.arange(-0.0,1.05,0.05),3)\n",
    "    thr_mat[thr_mat>-0.1] = abs(thr_mat[thr_mat>-0.1])\n",
    "\n",
    "    class_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    novelty_eff_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "    known_acc_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "    known_sp_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "    known_trig_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "\n",
    "    class_eff = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    novelty_eff = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    known_acc = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    known_sp = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    known_trig = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "\n",
    "    def getFiguresMetrics(inovelty):\n",
    "        n_folds = len(CVO[inovelty])  \n",
    "        print 'Novelty class: %01.0f - topology: %s'%(inovelty, neurons_str)\n",
    "        for ifold in range(n_folds):\n",
    "            classifier = SAE[inovelty].loadClassifier(data  = trn_data[inovelty],\n",
    "                                                      trgt  = trn_trgt[inovelty], \n",
    "                                                      hidden_neurons = hidden_neurons[:layer],\n",
    "                                                      layer = layer,\n",
    "                                                      ifold = ifold)\n",
    "\n",
    "            train_id, test_id = CVO[inovelty][ifold]\n",
    "            # normalize known classes\n",
    "            if trn_params.params['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(trn_data[inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(trn_data[inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(trn_data[inovelty][train_id,:])\n",
    "\n",
    "            known_data = scaler.transform(trn_data[inovelty][test_id,:])\n",
    "#             known_data = trn_data[inovelty][test_id,:]\n",
    "            known_trgt = trn_trgt[inovelty][test_id]\n",
    "\n",
    "            novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "#             novelty_data = all_data[all_trgt==inovelty]\n",
    "\n",
    "            output = classifier.predict(known_data)\n",
    "            novelty_output = classifier.predict(novelty_data)\n",
    "\n",
    "            for ithr,thr_value in enumerate(thr_mat): \n",
    "                buff = np.zeros([len(np.unique(all_trgt))-1])\n",
    "                for iclass, class_id in enumerate(np.unique(all_trgt)):\n",
    "                    if iclass == inovelty:\n",
    "                        continue\n",
    "                    output_of_class_events = output[known_trgt==iclass-(iclass>inovelty),:]\n",
    "                    correct_class_output = np.argmax(output_of_class_events,axis=1)==iclass-(iclass>inovelty)\n",
    "                    output_above_thr = output_of_class_events[correct_class_output,iclass-(iclass>inovelty)]>thr_value\n",
    "                    class_eff_mat[ifold, iclass, ithr] = float(sum(output_above_thr))/float(len(output_of_class_events))\n",
    "                    buff[iclass-(iclass>inovelty)] = class_eff_mat[ifold, iclass, ithr]\n",
    "                novelty_eff_mat[ifold, ithr] = float(sum(1-(novelty_output>thr_value).any(axis=1)))/float(len(novelty_output))\n",
    "                known_acc_mat[ifold, ithr] = np.mean(buff,axis=0)\n",
    "                known_sp_mat[ifold, ithr]= (np.sqrt(np.mean(buff,axis=0)\n",
    "                                                              *np.power(np.prod(buff),1./float(len(buff)))))\n",
    "                known_trig_mat[ifold, ithr]=float(sum(np.max(output,axis=1)>thr_value))/float(len(output))\n",
    "\n",
    "        return inovelty, class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, known_trig_mat\n",
    "\n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    results = p.map(getFiguresMetrics, class_labels.keys())\n",
    "\n",
    "    p.close()\n",
    "    p.join()         \n",
    "\n",
    "    for inovelty in class_labels.keys():\n",
    "        if inovelty == results[inovelty][0]:\n",
    "            class_eff[inovelty]   = results[inovelty][1]\n",
    "            novelty_eff[inovelty] = results[inovelty][2]\n",
    "            known_acc[inovelty]   = results[inovelty][3]\n",
    "            known_sp[inovelty]    = results[inovelty][4]\n",
    "            known_trig[inovelty]  = results[inovelty][5]\n",
    "\n",
    "    joblib.dump([class_eff, novelty_eff, known_acc, known_sp, known_trig, thr_mat],\n",
    "                analysis_file_name,compress=9)\n",
    "else:\n",
    "    print 'File exists'\n",
    "    [class_eff, novelty_eff, known_acc, known_sp, known_trig, thr_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "for inovelty, novelty_class in enumerate(np.unique(all_trgt)):\n",
    "    fig = plt.subplots(figsize=(12,6))\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    xdata1 = np.ones(np.mean(known_sp[inovelty], axis=0).shape) - np.mean(known_sp[inovelty], axis=0)\n",
    "    xdata2 = np.ones(np.mean(known_sp[inovelty], axis=0).shape) - np.mean(known_trig[inovelty], axis=0)\n",
    "        \n",
    "    ax.errorbar(xdata1, np.mean(novelty_eff[inovelty], axis=0),\n",
    "                np.std(novelty_eff[inovelty], axis=0),fmt='o-',\n",
    "                color='r',alpha=0.7,linewidth=2.5,\n",
    "                label='ROC SP')\n",
    "    ax.errorbar(xdata2, np.mean(novelty_eff[inovelty], axis=0),\n",
    "                np.std(novelty_eff[inovelty], axis=0),fmt='d-',\n",
    "                color='b',alpha=0.7,linewidth=2.5,\n",
    "                label='ROC Trigger')\n",
    "#     ax.errorbar(thr_mat,np.mean(known_acc[inovelty], axis=0),\n",
    "#                 np.std(known_acc[inovelty], axis=0),fmt='o--',\n",
    "#                 color='k',alpha=0.7,linewidth=2.5,\n",
    "#                 label='Known Acc.')\n",
    "    ax.set_xticks(np.arange(0,1.25,0.25))\n",
    "    ax.set_xticklabels(np.arange(0,1.25,0.25),rotation=45, fontsize=18)\n",
    "    ax.set_title('SAE com %i camada treinado com a classe %s como Novidade'%(layer,class_labels_txt[inovelty]),fontsize=18,weight='bold')\n",
    "    ax.set_xlim([np.min(thr_mat), np.max(thr_mat)])\n",
    "    \n",
    "    ax.set_ylim([0.0, 1.2])\n",
    "    y_ticks = np.arange(0.0,1.3,0.1)\n",
    "    ax.set_yticks(np.round(y_ticks,2))\n",
    "    ax.set_yticklabels(100*np.round(y_ticks,2)[np.round(y_ticks,2)<=1.0],fontsize=18)\n",
    "    \n",
    "    ax.grid()\n",
    "    \n",
    "    ax.set_xlabel('(1 - SP) (%s)',fontsize=20,weight='bold')\n",
    "    ax.set_ylabel('Detecção de Novidade (%)',fontsize=20,weight='bold')\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # sort both labels and handles by labels\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "    ax.legend(handles, labels, ncol=3, loc='upper center')\n",
    "    \n",
    "    plt.show()\n",
    "    #Save the figure\n",
    "    file_name = pict_results_path+'/'+\"ROC_sp_novelty_det\"+'_%i_novelty_%s_neurons_'%(inovelty,neurons_str)+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "print trn_params.get_params_str()\n",
    "print '%s neurons'%neurons_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures of Merit for a threshold sweep at output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresolds variation x Figures of Merit\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from Functions.StatisticalAnalysis import KLDiv, EstPDF\n",
    "from Functions import FunctionsDataVisualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = [200]\n",
    "\n",
    "analysis_name = 'figures_of_merit_%i_layer'%(layer)\n",
    "analysis_file = os.path.join(analysis.getBaseResultsPath(), \"AnalysisFiles\", analysis_name + \".jbl\")    \n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "figsize = (20,15)\n",
    "\n",
    "\n",
    "if not os.path.exists(analysis_file):\n",
    "    # Set the threshold to be analyzed\n",
    "    thr_mat = np.round(np.arange(0.0,1.1,0.1),3)\n",
    "    thr_mat[thr_mat>-0.1] = abs(thr_mat[thr_mat>-0.1])\n",
    "    \n",
    "    n_folds = analysis.n_folds\n",
    "    \n",
    "    class_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    novelty_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    known_acc_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    known_sp_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    known_trig_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    \n",
    "    for inovelty, novelty_class in enumerate(np.unique(analysis.all_trgt)):\n",
    "        for ifold in range(len(analysis.CVO[inovelty])):\n",
    "            train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "            \n",
    "            print 'Novelty class: %01.0f - Topology: %s - fold %i'%(novelty_class,\n",
    "                                                                    models[inovelty].get_neurons_str(data=trn_data[inovelty], hidden_neurons=hidden_neurons)+'x'+str(trn_trgt_sparse[inovelty].shape[1]),\n",
    "                                                                    ifold)\n",
    "            classifier = SAE[inovelty].load_classifier(data  = analysis.trn_data[inovelty],\n",
    "                                                       trgt  = analysis.trn_trgt[inovelty], \n",
    "                                                       hidden_neurons = hidden_neurons[:layer-1]+[ineuron],\n",
    "                                                       layer = layer,\n",
    "                                                       ifold = ifold)\n",
    "            \n",
    "            # normalize known classes\n",
    "            if analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd\":\n",
    "                scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "            elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd_rob\":\n",
    "                scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "            elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapminmax\":\n",
    "                scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "            known_data = scaler.transform(analysis.trn_data[inovelty][test_id,:])\n",
    "            known_trgt = analysis.trn_trgt[inovelty][test_id]\n",
    "            \n",
    "            novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "            \n",
    "            output = classifier.predict(known_data)\n",
    "            novelty_output = classifier.predict(novelty_data)\n",
    "            \n",
    "            for ithr,thr_value in enumerate(thr_mat): \n",
    "                buff = np.zeros([len(np.unique(all_trgt))-1])\n",
    "                for iclass, class_id in enumerate(np.unique(all_trgt)):\n",
    "                    if iclass == inovelty:\n",
    "                        continue\n",
    "                    output_of_class_events = output[known_trgt==iclass-(iclass>inovelty),:]\n",
    "                    correct_class_output = np.argmax(output_of_class_events,axis=1)==iclass-(iclass>inovelty)\n",
    "                    output_above_thr = output_of_class_events[correct_class_output,iclass-(iclass>inovelty)]>thr_value\n",
    "                    class_eff_mat[ifold, inovelty, iclass, ithr] = float(sum(output_above_thr))/float(len(output_of_class_events))\n",
    "                    buff[iclass-(iclass>inovelty)] = class_eff_mat[ifold, inovelty, iclass, ithr]\n",
    "                novelty_eff_mat[ifold, inovelty, ithr] = float(sum(1-(novelty_output>thr_value).any(axis=1)))/float(len(novelty_output))\n",
    "                known_acc_mat[ifold, inovelty, ithr] = np.mean(buff,axis=0)\n",
    "                known_sp_mat[ifold, inovelty, ithr]= (np.sqrt(np.mean(buff,axis=0)\n",
    "                                                              *np.power(np.prod(buff),1./float(len(buff)))))\n",
    "                known_trig_mat[ifold, inovelty, ithr]=float(sum(np.max(output,axis=1)>thr_value))/float(len(output))\n",
    "    joblib.dump([class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, known_trig_mat, thr_mat],\n",
    "                analysis_file,compress=9)\n",
    "else:\n",
    "    print 'file exists'\n",
    "    [class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, known_trig_mat, thr_mat] = joblib.load(analysis_file) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.subplots(figsize=figsize)\n",
    "\n",
    "for inovelty, novelty_class in enumerate(np.unique(all_trgt)):\n",
    "    ax = plt.subplot(2,2,inovelty+1)\n",
    "    for iclass, m_class in enumerate(np.unique(all_trgt)):\n",
    "        if novelty_class == m_class:\n",
    "            #a = 0\n",
    "            ax.errorbar(thr_mat,np.mean(novelty_eff_mat[:,int(novelty_class),:],axis=0),\n",
    "                        np.std(novelty_eff_mat[:,int(novelty_class),:],axis=0),fmt='o-',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Novel Det.')\n",
    "            ax.errorbar(thr_mat,np.mean(known_acc_mat[:,int(novelty_class),:],axis=0),\n",
    "                        np.std(known_acc_mat[:,int(novelty_class),:],axis=0),fmt='o--',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Known Acc.')\n",
    "            ax.errorbar(thr_mat,np.mean(known_sp_mat[:,int(novelty_class),:],axis=0),\n",
    "                        np.std(known_sp_mat[:,int(novelty_class),:],axis=0),fmt='o:',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Known SP')\n",
    "#             ax.errorbar(thr_mat,np.mean(known_trig_mat[:,int(novelty_class),:],axis=0),\n",
    "#                         np.std(known_trig_mat[:,int(novelty_class),:],axis=0),fmt='o-.',\n",
    "#                         color='k',alpha=0.7,linewidth=2.5,\n",
    "#                         label='Known Trig.')\n",
    "        else:\n",
    "            ax.errorbar(thr_mat,np.mean(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),\n",
    "                        np.std(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),fmt='o-',\n",
    "                        color=m_colors[int(m_class)],alpha=0.7,linewidth=2.5,\n",
    "                       label='C%i Eff.'%(int(m_class)+1))\n",
    "    ax.set_xticks(thr_mat)\n",
    "    ax.set_xticklabels(thr_mat,rotation=45, fontsize=18)\n",
    "    ax.set_title('Eff per Known Class',fontsize=18,weight='bold')\n",
    "    ax.set_xlim([np.min(thr_mat), np.max(thr_mat)])\n",
    "    \n",
    "    ax.set_ylim([0.0, 1.3])\n",
    "    y_ticks = np.arange(0.0,1.3,0.1)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    y_tick_labels = 100*y_ticks[y_ticks<=1.0]\n",
    "    y_tick_labels = y_tick_labels.astype(int)\n",
    "    ax.set_yticklabels(y_tick_labels,fontsize=18)\n",
    "    \n",
    "    ax.grid()\n",
    "    \n",
    "    if inovelty > 1:\n",
    "        ax.set_xlabel('Threshold',fontsize=18,weight='bold')\n",
    "    if inovelty == 0 or inovelty == 2:\n",
    "        ax.set_ylabel('Figures-of-Merit (%)',fontsize=18,weight='bold')\n",
    "        \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # sort both labels and handles by labels\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "    ax.legend(handles, labels, ncol=3, loc='upper center')\n",
    "   \n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
