{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "## Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "### Autor: Vinícius dos Santos Mello <viniciusdsmello@poli.ufrj.br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Time to read data file: 1.09258794785 seconds\n",
      "Qtd event of A is 12939\n",
      "Qtd event of B is 29352\n",
      "Qtd event of C is 11510\n",
      "Qtd event of D is 23760\n",
      "\n",
      "Biggest class is B with 29352 events\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (12939, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (29352, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (11510, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (23760, 400)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from noveltyDetectionConfig import CONFIG\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from SAENoveltyDetectionAnalysis import SAENoveltyDetectionAnalysis\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "analysis_name = 'StackedAutoEncoder'\n",
    "\n",
    "# Enviroment variables\n",
    "data_path = CONFIG['OUTPUTDATAPATH']\n",
    "results_path = CONFIG['PACKAGE_NAME']\n",
    "\n",
    "analysis = SAENoveltyDetectionAnalysis(analysis_name=\"StackedAutoEncoder\", development_flag = False, development_events=400)\n",
    "all_data, all_trgt, trgt_sparse = analysis.getData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the train parameters file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating /home/vinicius.mello/Workspace/SonarAnalysis/Results/NoveltyDetection/StackedAutoEncoder/Adam_optmizer/tanh_sae_hidden_activation/linear_sae_output_actvation/softmax_classifier_output_activation/2_init_10_folds_mapstd_norm_300_epochs_256_batch_size_accuracy_metric_mean_squared_error_loss\n",
      "Reading from /home/vinicius.mello/Workspace/SonarAnalysis/Results/NoveltyDetection/10_folds_cross_validation.jbl\n",
      "\n",
      "Results path: /home/vinicius.mello/Workspace/SonarAnalysis/Results/NoveltyDetection/StackedAutoEncoder/Adam_optmizer/tanh_sae_hidden_activation/linear_sae_output_actvation/softmax_classifier_output_activation/2_init_10_folds_mapstd_norm_300_epochs_256_batch_size_accuracy_metric_mean_squared_error_loss\n"
     ]
    }
   ],
   "source": [
    "analysis.setTrainParameters(n_inits=2,\n",
    "                        hidden_activation='tanh',\n",
    "                        output_activation='linear',\n",
    "                        n_epochs=300,\n",
    "                        n_folds=10,\n",
    "                        patience=30,\n",
    "                        batch_size=256,\n",
    "                        verbose=False,\n",
    "                        optmizerAlgorithm='Adam',\n",
    "                        metrics=['accuracy'],\n",
    "                        loss='mean_squared_error')\n",
    "\n",
    "print (\"\\nResults path: \" + analysis.getBaseResultsPath())\n",
    "\n",
    "# trn_params = analysis.getTrainParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize objects of StackedAutoEncoder class for all models with its corresponding novelty class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initializing SAE Class for class A\n",
      "[*] Initializing SAE Class for class B\n",
      "[*] Initializing SAE Class for class C\n",
      "[*] Initializing SAE Class for class D\n"
     ]
    }
   ],
   "source": [
    "analysis.createSAEModels()\n",
    "SAE, trn_data, trn_trgt, trn_trgt_sparse = analysis.getSAEModels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python modelTrain.py --layer 1 --novelty 0 --finetunning 1 --threads 8 --type neuronSweep --hiddenNeurons 400 --neuronsVariationStep 25\n"
     ]
    }
   ],
   "source": [
    "for inovelty in range(len(analysis.class_labels)):\n",
    "    startTime = time.time()\n",
    "    analysis.train(layer=1,\n",
    "                   inovelty=inovelty,\n",
    "                   ifold=10,\n",
    "                   fineTuning=True,\n",
    "                   trainingType=\"neuronSweep\", #foldSweep, neuronSweep, normal\n",
    "                   hidden_neurons=[400],\n",
    "                   neurons_variation_step=25,\n",
    "                   numThreads=8)\n",
    "    print \"The training of the model for novelty {0} took {1} seconds to be performed\\n\".format(analysis.class_labels[inovelty], time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler Divergence analysis for a neuron variation at autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron variation x KL Divergence\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_mat = [10,20] + range(50,450,50)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 0\n",
    "regularizer = \"dropout\"\n",
    "regularizer_param = 0.500000\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'klDivergence_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "if regularizer != None or len(regularizer) != 0:\n",
    "    analysis_file_name='%s/%s/%s_%s_regularizer(%f)_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,regularizer,\n",
    "                                                                                 regularizer_param,current_analysis)\n",
    "else:\n",
    "    analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "n_folds = analysis.n_folds\n",
    "\n",
    "klDivergenceFreq = {}\n",
    "klDivergenceKnown = np.zeros([n_folds, len(neurons_mat)], dtype=object)\n",
    "klDivergenceKnownFreq = {}\n",
    "klDivergenceNovelty = np.zeros([n_folds, len(neurons_mat)], dtype=object)\n",
    "klDivergenceNoveltyFreq = {}\n",
    "\n",
    "# if os.path.exists(analysis_file_name):\n",
    "#     os.remove(analysis_file_name)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    for ineuron in neurons_mat: \n",
    "        if ineuron == 0:\n",
    "            ineuron = 1\n",
    "        neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer-1]+[ineuron])\n",
    "\n",
    "        models = {}\n",
    "        outputs = {}\n",
    "        norm_data = {}\n",
    "        reconstructed_known_data = {}\n",
    "        reconstructed_novelty_data = {}\n",
    "        if verbose: \n",
    "            print '[*] Layer: %i - Topology: %s'%(layer, neurons_str)\n",
    "\n",
    "        n_bins = 100\n",
    "\n",
    "        def getKlDiv(ifold):\n",
    "            train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "\n",
    "            # normalize known classes\n",
    "            if trn_params.params['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "            known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "            novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "\n",
    "            model = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                           hidden_neurons=hidden_neurons[:layer-1]+[ineuron],\n",
    "                                           layer=layer, ifold=ifold, regularizer=regularizer, regularizer_param=regularizer_param)\n",
    "\n",
    "            known_output = model.predict(known_data)\n",
    "            novelty_output = model.predict(novelty_data)\n",
    "\n",
    "            klKnown = np.zeros([all_data.shape[1]], dtype=object)\n",
    "            klNovelty = np.zeros([all_data.shape[1]], dtype=object)\n",
    "            \n",
    "            for ifrequency in range(0,400):\n",
    "                # Calculate KL Div for known data reconstruction\n",
    "                known_data_freq = known_data[:,ifrequency]\n",
    "                reconstructed_known_data = known_output[:,ifrequency]\n",
    "\n",
    "                m_bins = np.linspace(known_data_freq.min(), known_data_freq.max(), n_bins)\n",
    "                \n",
    "                klKnown[ifrequency] = KLDiv(known_data_freq.reshape(-1,1), reconstructed_known_data.reshape(-1,1),\n",
    "                                       bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                                       kernel_bw=0.1, verbose=False)\n",
    "\n",
    "                klKnown[ifrequency] = klKnown[ifrequency][0]\n",
    "                \n",
    "                # Calculate KL Div for novelty data reconstruction\n",
    "                novelty_data_freq = novelty_data[:,ifrequency]\n",
    "                reconstructed_novelty_data = novelty_output[:,ifrequency]\n",
    "\n",
    "                m_bins = np.linspace(novelty_data_freq.min(), novelty_data_freq.max(), n_bins)\n",
    "                \n",
    "                klNovelty[ifrequency] = KLDiv(novelty_data_freq.reshape(-1,1), reconstructed_novelty_data.reshape(-1,1),\n",
    "                                       bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                                       kernel_bw=0.1, verbose=False)\n",
    "\n",
    "                klNovelty[ifrequency] = klNovelty[ifrequency][0]\n",
    "                \n",
    "            return ifold, klKnown, klNovelty\n",
    "\n",
    "        # Start Parallel processing\n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "        folds = range(n_folds)\n",
    "        if verbose:\n",
    "            print '[*] Calculating KL Div for all frequencies...'\n",
    "        # Calculate the KL Div at all frequencies\n",
    "        klDivergenceFreq[ineuron] = p.map(getKlDiv, folds)\n",
    "        \n",
    "        p.close()\n",
    "        p.join()\n",
    "        \n",
    "        index = neurons_mat.index(ineuron)\n",
    "        for ifold in range(n_folds):\n",
    "            klDivergenceKnownFreq = klDivergenceFreq[ineuron][ifold][1]\n",
    "            klDivergenceNoveltyFreq = klDivergenceFreq[ineuron][ifold][2]\n",
    "            \n",
    "            klDivergenceKnown[ifold, index] = np.sum(klDivergenceKnownFreq)\n",
    "            klDivergenceNovelty[ifold, index] = np.sum(klDivergenceNoveltyFreq)\n",
    "\n",
    "        joblib.dump([neurons_mat,klDivergenceKnown,klDivergenceNovelty],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [neurons_mat, klDivergenceKnown, klDivergenceNovelty] = joblib.load(analysis_file_name)\n",
    "# Plot results    \n",
    "fig, m_ax = plt.subplots(figsize=(15,30),nrows=5, ncols=2)\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    irow = int(ifold/2)\n",
    "    if (ifold % 2 == 0):\n",
    "        icolumn = 0\n",
    "    else: \n",
    "        icolumn = 1\n",
    "\n",
    "    m_ax[irow, icolumn].plot(neurons_mat, klDivergenceKnown[ifold,:], 'b-o', label='Known Test Data')\n",
    "    m_ax[irow, icolumn].plot(neurons_mat, klDivergenceNovelty[ifold,:], 'r--o', label='Novelty Data')\n",
    "    m_ax[irow, icolumn].set_title('KL Divergence x Neurons - Layer %i - Fold %i - Novelty Class %i '%(layer,ifold+1, inovelty), fontsize=14, fontweight='bold')\n",
    "    m_ax[irow, icolumn].set_ylabel('Kullback-Leibler Divergence', fontsize=14)\n",
    "    m_ax[irow, icolumn].set_xlabel('Neurons', fontsize=14)\n",
    "    m_ax[irow, icolumn].grid()\n",
    "    m_ax[irow, icolumn].legend()\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error analysis for Pre-training Step with a neuron variation at autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron variation x KL Divergence\n",
    "%matplotlib inline \n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_mat = [10, 20] + range(50,450,50)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 1\n",
    "regularizer = \"l2\"\n",
    "regularizer_param = 0.500000\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'mean_squared_error_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "if regularizer != None or len(regularizer) != 0:\n",
    "    analysis_file_name='%s/%s/%s_%s_regularizer(%f)_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,regularizer,\n",
    "                                                                                 regularizer_param,current_analysis)\n",
    "else:\n",
    "    analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = analysis.trn_params.get_params_str()\n",
    "\n",
    "n_folds = analysis.n_folds\n",
    "\n",
    "mse = {}\n",
    "mse_known = np.zeros([n_folds, len(neurons_mat)])\n",
    "mse_novelty = np.zeros([n_folds, len(neurons_mat)])\n",
    "\n",
    "# if os.path.exists(analysis_file_name):\n",
    "#     os.remove(analysis_file_name)\n",
    "    \n",
    "if not os.path.exists(analysis_file_name):\n",
    "    for ineuron in neurons_mat: \n",
    "        if ineuron == 0:\n",
    "            ineuron = 1\n",
    "        neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer-1]+[ineuron])\n",
    "\n",
    "        models = {}\n",
    "        outputs = {}\n",
    "        norm_data = {}\n",
    "        reconstructed_known_data = {}\n",
    "        reconstructed_novelty_data = {}\n",
    "        if verbose: \n",
    "            print '[*] Layer: %i - Topology: %s'%(layer, neurons_str)\n",
    "\n",
    "        n_bins = 100\n",
    "\n",
    "        def getMSE(ifold):\n",
    "            train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "\n",
    "            # normalize known classes\n",
    "            if trn_params.params['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "            known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "            novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "\n",
    "            model = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                           hidden_neurons=hidden_neurons[:layer-1]+[ineuron],\n",
    "                                           layer=layer, ifold=ifold, regularizer=regularizer, regularizer_param=regularizer_param)\n",
    "\n",
    "            known_output = model.predict(known_data)\n",
    "            novelty_output = model.predict(novelty_data)\n",
    "            \n",
    "            mseKnown = metrics.mean_squared_error(known_data, known_output)\n",
    "            mseNovelty = metrics.mean_squared_error(novelty_data, novelty_output)\n",
    "            \n",
    "                \n",
    "            return ifold, mseKnown, mseNovelty\n",
    "\n",
    "        # Start Parallel processing\n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "        folds = range(n_folds)\n",
    "        if verbose:\n",
    "            print '[*] Calculating Mean Squared Error ...'\n",
    "        mse[ineuron] = p.map(getMSE, folds)\n",
    "        \n",
    "        for ifold in range(n_folds):\n",
    "            mse_known[:, neurons_mat.index(ineuron)] = mse[ineuron][ifold][1]\n",
    "            mse_novelty[:, neurons_mat.index(ineuron)] = mse[ineuron][ifold][2]\n",
    "        \n",
    "        p.close()\n",
    "        p.join()\n",
    "    \n",
    "        joblib.dump([neurons_mat,mse_known,mse_novelty],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [neurons_mat, mse_known, mse_novelty] = joblib.load(analysis_file_name)\n",
    "# Plot results    \n",
    "fig, m_ax = plt.subplots(figsize=(20,30),nrows=5, ncols=2)\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    irow = int(ifold/2)\n",
    "    if (ifold % 2 == 0):\n",
    "        icolumn = 0\n",
    "    else: \n",
    "        icolumn = 1\n",
    "\n",
    "    m_ax[irow, icolumn].plot(neurons_mat, mse_known[ifold, :], 'b-o', label='Known Test Data')\n",
    "    m_ax[irow, icolumn].plot(neurons_mat, mse_novelty[ifold, :], 'r--o', label='Novelty Data')\n",
    "    m_ax[irow, icolumn].set_title('MSE x Neurons - Layer %i - Fold %i - Novelty Class %i '%(layer,ifold+1, inovelty), fontsize=16, fontweight='bold')\n",
    "    m_ax[irow, icolumn].set_ylabel('Mean Squared Error', fontsize=22)\n",
    "    m_ax[irow, icolumn].set_xlabel('Neurons', fontsize=22)\n",
    "    m_ax[irow, icolumn].grid()\n",
    "    m_ax[irow, icolumn].legend()\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "#Save the figure\n",
    "file_name = pict_results_path+'/'+current_analysis+'_%i_novelty_%s_neurons_'%(inovelty,neurons_str)+trn_params.get_params_str()+'.pdf'\n",
    "plt.savefig(file_name)\n",
    "print trn_params.get_params_str()\n",
    "print '%s neurons'%neurons_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) Curve for SP/Trigger with Novelty Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "# Choose layer\n",
    "layer = 1\n",
    "inovelty = 1\n",
    "\n",
    "# Choose neurons topology for SAE\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "current_analysis = 'figures_of_merit'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_novelty_detection_%s_neurons_thr_sweep.jbl'%(results_path,analysis_str,analysis_name, neurons_str)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "\n",
    "    thr_mat = np.round(np.arange(-0.0,1.05,0.05),3)\n",
    "    thr_mat[thr_mat>-0.1] = abs(thr_mat[thr_mat>-0.1])\n",
    "\n",
    "    class_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    novelty_eff_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "    known_acc_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "    known_sp_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "    known_trig_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "\n",
    "    class_eff = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    novelty_eff = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    known_acc = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    known_sp = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    known_trig = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "\n",
    "    def getFiguresMetrics(inovelty):\n",
    "        n_folds = len(CVO[inovelty])  \n",
    "        print 'Novelty class: %01.0f - topology: %s'%(inovelty, neurons_str)\n",
    "        for ifold in range(n_folds):\n",
    "            classifier = SAE[inovelty].loadClassifier(data  = trn_data[inovelty],\n",
    "                                                      trgt  = trn_trgt[inovelty], \n",
    "                                                      hidden_neurons = hidden_neurons[:layer],\n",
    "                                                      layer = layer,\n",
    "                                                      ifold = ifold)\n",
    "\n",
    "            train_id, test_id = CVO[inovelty][ifold]\n",
    "            # normalize known classes\n",
    "            if trn_params.params['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(trn_data[inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(trn_data[inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(trn_data[inovelty][train_id,:])\n",
    "\n",
    "            known_data = scaler.transform(trn_data[inovelty][test_id,:])\n",
    "#             known_data = trn_data[inovelty][test_id,:]\n",
    "            known_trgt = trn_trgt[inovelty][test_id]\n",
    "\n",
    "            novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "#             novelty_data = all_data[all_trgt==inovelty]\n",
    "\n",
    "            output = classifier.predict(known_data)\n",
    "            novelty_output = classifier.predict(novelty_data)\n",
    "\n",
    "            for ithr,thr_value in enumerate(thr_mat): \n",
    "                buff = np.zeros([len(np.unique(all_trgt))-1])\n",
    "                for iclass, class_id in enumerate(np.unique(all_trgt)):\n",
    "                    if iclass == inovelty:\n",
    "                        continue\n",
    "                    output_of_class_events = output[known_trgt==iclass-(iclass>inovelty),:]\n",
    "                    correct_class_output = np.argmax(output_of_class_events,axis=1)==iclass-(iclass>inovelty)\n",
    "                    output_above_thr = output_of_class_events[correct_class_output,iclass-(iclass>inovelty)]>thr_value\n",
    "                    class_eff_mat[ifold, iclass, ithr] = float(sum(output_above_thr))/float(len(output_of_class_events))\n",
    "                    buff[iclass-(iclass>inovelty)] = class_eff_mat[ifold, iclass, ithr]\n",
    "                novelty_eff_mat[ifold, ithr] = float(sum(1-(novelty_output>thr_value).any(axis=1)))/float(len(novelty_output))\n",
    "                known_acc_mat[ifold, ithr] = np.mean(buff,axis=0)\n",
    "                known_sp_mat[ifold, ithr]= (np.sqrt(np.mean(buff,axis=0)\n",
    "                                                              *np.power(np.prod(buff),1./float(len(buff)))))\n",
    "                known_trig_mat[ifold, ithr]=float(sum(np.max(output,axis=1)>thr_value))/float(len(output))\n",
    "\n",
    "        return inovelty, class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, known_trig_mat\n",
    "\n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    results = p.map(getFiguresMetrics, class_labels.keys())\n",
    "\n",
    "    p.close()\n",
    "    p.join()         \n",
    "\n",
    "    for inovelty in class_labels.keys():\n",
    "        if inovelty == results[inovelty][0]:\n",
    "            class_eff[inovelty]   = results[inovelty][1]\n",
    "            novelty_eff[inovelty] = results[inovelty][2]\n",
    "            known_acc[inovelty]   = results[inovelty][3]\n",
    "            known_sp[inovelty]    = results[inovelty][4]\n",
    "            known_trig[inovelty]  = results[inovelty][5]\n",
    "\n",
    "    joblib.dump([class_eff, novelty_eff, known_acc, known_sp, known_trig, thr_mat],\n",
    "                analysis_file_name,compress=9)\n",
    "else:\n",
    "    print 'File exists'\n",
    "    [class_eff, novelty_eff, known_acc, known_sp, known_trig, thr_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "for inovelty, novelty_class in enumerate(np.unique(all_trgt)):\n",
    "    fig = plt.subplots(figsize=(12,6))\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    xdata1 = np.ones(np.mean(known_sp[inovelty], axis=0).shape) - np.mean(known_sp[inovelty], axis=0)\n",
    "    xdata2 = np.ones(np.mean(known_sp[inovelty], axis=0).shape) - np.mean(known_trig[inovelty], axis=0)\n",
    "        \n",
    "    ax.errorbar(xdata1, np.mean(novelty_eff[inovelty], axis=0),\n",
    "                np.std(novelty_eff[inovelty], axis=0),fmt='o-',\n",
    "                color='r',alpha=0.7,linewidth=2.5,\n",
    "                label='ROC SP')\n",
    "    ax.errorbar(xdata2, np.mean(novelty_eff[inovelty], axis=0),\n",
    "                np.std(novelty_eff[inovelty], axis=0),fmt='d-',\n",
    "                color='b',alpha=0.7,linewidth=2.5,\n",
    "                label='ROC Trigger')\n",
    "#     ax.errorbar(thr_mat,np.mean(known_acc[inovelty], axis=0),\n",
    "#                 np.std(known_acc[inovelty], axis=0),fmt='o--',\n",
    "#                 color='k',alpha=0.7,linewidth=2.5,\n",
    "#                 label='Known Acc.')\n",
    "    ax.set_xticks(np.arange(0,1.25,0.25))\n",
    "    ax.set_xticklabels(np.arange(0,1.25,0.25),rotation=45, fontsize=18)\n",
    "    ax.set_title('SAE com %i camada treinado com a classe %s como Novidade'%(layer,class_labels_txt[inovelty]),fontsize=18,weight='bold')\n",
    "    ax.set_xlim([np.min(thr_mat), np.max(thr_mat)])\n",
    "    \n",
    "    ax.set_ylim([0.0, 1.2])\n",
    "    y_ticks = np.arange(0.0,1.3,0.1)\n",
    "    ax.set_yticks(np.round(y_ticks,2))\n",
    "    ax.set_yticklabels(100*np.round(y_ticks,2)[np.round(y_ticks,2)<=1.0],fontsize=18)\n",
    "    \n",
    "    ax.grid()\n",
    "    \n",
    "    ax.set_xlabel('(1 - SP) (%s)',fontsize=20,weight='bold')\n",
    "    ax.set_ylabel('Detecção de Novidade (%)',fontsize=20,weight='bold')\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # sort both labels and handles by labels\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "    ax.legend(handles, labels, ncol=3, loc='upper center')\n",
    "    \n",
    "    plt.show()\n",
    "    #Save the figure\n",
    "    file_name = pict_results_path+'/'+\"ROC_sp_novelty_det\"+'_%i_novelty_%s_neurons_'%(inovelty,neurons_str)+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "print trn_params.get_params_str()\n",
    "print '%s neurons'%neurons_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
