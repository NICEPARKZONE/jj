{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "## Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "### Autor: Vinícius dos Santos Mello <viniciusdsmello@poli.ufrj.br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 5.91278076172e-05 seconds\n",
      "Time to read data file: 15.9682421684 seconds\n",
      "Qtd event of 0 is 12939\n",
      "Qtd event of 1 is 29352\n",
      "Qtd event of 2 is 11510\n",
      "Qtd event of 3 is 23760\n",
      "\n",
      "Biggest class is 1 with 29352 events\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (12939, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (29352, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (11510, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (23760, 400)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Functions import TrainParameters as trnparams\n",
    "from Functions import TrainFunctions\n",
    "from Functions.StackedAutoEncoders import StackedAutoEncoders\n",
    "from Functions import FunctionsDataVisualization\n",
    "from Functions.StatisticalAnalysis import KLDiv, EstPDF\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "analysis_name = 'StackedAutoEncoder'\n",
    "\n",
    "# Enviroment variables\n",
    "data_path = os.getenv('OUTPUTDATAPATH')\n",
    "results_path = os.getenv('PACKAGE_NAME')\n",
    "\n",
    "# paths to export results\n",
    "base_results_path = '%s/%s'%(results_path,analysis_name)\n",
    "pict_results_path = '%s/pictures_files'%(base_results_path)\n",
    "files_results_path = '%s/output_files'%(base_results_path)\n",
    "\n",
    "# For multiprocessing purpose\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# Read data\n",
    "m_time = time.time()\n",
    "\n",
    "# Database caracteristics\n",
    "database = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "spectrum_bins_left = 400\n",
    "development_flag = False\n",
    "development_events = 400\n",
    "\n",
    "# Check if LofarData has created...\n",
    "if not os.path.exists('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                      (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left)):\n",
    "    print 'No Files in %s/%s\\n'%(data_path,database)\n",
    "else:\n",
    "    #Read lofar data\n",
    "    [data,trgt,class_labels] = joblib.load('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                                           (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left))\n",
    "\n",
    "\n",
    "    m_time = time.time()-m_time\n",
    "    print 'Time to read data file: '+str(m_time)+' seconds'\n",
    "\n",
    "    # correct format\n",
    "    all_data = data\n",
    "    all_trgt = trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))\n",
    "\n",
    "    # Process data\n",
    "    # unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "    # Same number of events in each class\n",
    "    qtd_events_biggest_class = 0\n",
    "    biggest_class_label = ''\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "            qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "            biggest_class_label = class_label\n",
    "        print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "    print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "    balanced_data = {}\n",
    "    balanced_trgt = {}\n",
    "\n",
    "    from Functions import DataHandler as dh\n",
    "    m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if development_flag:\n",
    "            class_events = all_data[all_trgt==iclass,:]\n",
    "            if len(balanced_data) == 0:\n",
    "                balanced_data = class_events[0:development_events,:]\n",
    "                balanced_trgt = (iclass)*np.ones(development_events)\n",
    "            else:\n",
    "                balanced_data = np.append(balanced_data,\n",
    "                                          class_events[0:development_events,:],\n",
    "                                          axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,(iclass)*np.ones(development_events))\n",
    "        else:\n",
    "            if len(balanced_data) == 0:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                balanced_data = m_datahandler.CreateEventsForClass(\n",
    "                    class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "                balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "            else:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                created_events = (m_datahandler.CreateEventsForClass(all_data[all_trgt==iclass,:],\n",
    "                                                                     qtd_events_biggest_class-\n",
    "                                                                     (len(class_events))))\n",
    "                balanced_data = np.append(balanced_data,created_events,axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,\n",
    "                                          (iclass)*np.ones(created_events.shape[0]),axis=0)\n",
    "\n",
    "    all_data = balanced_data\n",
    "    all_trgt = balanced_trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar parâmetros de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train parameters\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if os.path.exists(trn_params_folder):\n",
    "    os.remove(trn_params_folder)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.SAENoveltyDetectionTrnParams(n_inits=1,\n",
    "                                                       hidden_activation='tanh', # others tanh, relu, sigmoid, linear \n",
    "                                                       output_activation='linear',\n",
    "                                                       n_epochs=300,\n",
    "                                                       patience=30,\n",
    "                                                       batch_size=256,\n",
    "                                                       verbose=False,\n",
    "                                                       optmizerAlgorithm='Adam',\n",
    "                                                       metrics=['accuracy'], #mean_squared_error\n",
    "                                                       loss='mean_squared_error') #kullback_leibler_divergence\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.SAENoveltyDetectionTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "    \n",
    "# Choose how many fold to be used in Cross Validation\n",
    "n_folds = 10\n",
    "CVO = trnparams.NoveltyDetectionFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag, verbose=True)\n",
    "print '\\n'+trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do Stacked Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "SAE = {}\n",
    "trn_data = {}\n",
    "trn_trgt = {}\n",
    "trn_trgt_sparse = {}\n",
    "for inovelty, novelty_class in enumerate(np.unique(trgt)):\n",
    "    trn_data[inovelty] = all_data[all_trgt!=novelty_class]\n",
    "    trn_trgt[inovelty] = all_trgt[all_trgt!=novelty_class]\n",
    "    trn_trgt[inovelty][trn_trgt[inovelty]>novelty_class] = trn_trgt[inovelty][trn_trgt[inovelty]>novelty_class]-1\n",
    "    trn_trgt_sparse[inovelty] = np_utils.to_categorical(trn_trgt[inovelty].astype(int))\n",
    "    # Initialize an SAE object for all novelties\n",
    "    SAE[inovelty] = StackedAutoEncoders(params           = trn_params,\n",
    "                                        development_flag = development_flag,\n",
    "                                        n_folds          = n_folds,\n",
    "                                        save_path        = results_path,\n",
    "                                        CVO              = CVO,\n",
    "                                        noveltyDetection = True,\n",
    "                                        inovelty         = inovelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### Train Process #####\n",
    "inovelty = 1\n",
    "novelty_train = [inovelty]\n",
    "print 'Novelty to Train: %i'%inovelty\n",
    "\n",
    "# Choose layer to be trained\n",
    "layer = 1\n",
    "\n",
    "n_folds = len(CVO[inovelty])\n",
    "\n",
    "# hidden_neurons = range(400,0,-50) + [2]\n",
    "hidden_neurons = [20]\n",
    "\n",
    "print \"Neurons Topology: \", hidden_neurons\n",
    "print \"Layer: %i\"%layer\n",
    "print \"N. of Folds: %i\"%n_folds\n",
    "\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "if len(regularizer) != 0:    \n",
    "    print \"Regularizer: %s\"%regularizer\n",
    "    print \"Parameter Value: %i\"%regularizer_param\n",
    "\n",
    "# Functions defined to be used by multiprocessing.Pool()\n",
    "def trainNeuron(ineuron):\n",
    "    for ifold in range(n_folds):\n",
    "        SAE[inovelty].trainLayer(data = trn_data[inovelty],\n",
    "                                 trgt = trn_trgt[inovelty],\n",
    "                                 ifold = ifold,\n",
    "                                 hidden_neurons = hidden_neurons + [ineuron],\n",
    "                                 layer = layer,\n",
    "                                 regularizer = regularizer,\n",
    "                                 regularizer_param = regularizer_param)\n",
    "\n",
    "def trainFold(ifold):\n",
    "    return  SAE[inovelty].trainLayer(data = trn_data[inovelty],\n",
    "                                     trgt = trn_trgt[inovelty],\n",
    "                                     ifold = ifold,\n",
    "                                     hidden_neurons = hidden_neurons,\n",
    "                                     layer = layer,\n",
    "                                     regularizer = regularizer,\n",
    "                                     regularizer_param = regularizer_param)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if K.backend() == 'theano':\n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    ####################### SAE LAYERS ############################\n",
    "    # It is necessary to choose the layer to be trained\n",
    "\n",
    "    # To train on multiple cores sweeping the number of folds\n",
    "    folds = range(len(CVO[inovelty]))\n",
    "    results = p.map(trainFold, folds)\n",
    "\n",
    "    # To train multiple topologies sweeping the number of neurons\n",
    "    # neurons_mat = range(0,400,50) (start,final,step)\n",
    "    # results = p.map(trainNeuron, neurons_mat)\n",
    "\n",
    "    p.close()\n",
    "    p.join()\n",
    "else: \n",
    "    for ifold in range(len(CVO[inovelty])):\n",
    "        result = trainFold(ifold)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimação de PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# Reconstruction Analysis\n",
    "%matplotlib inline \n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'pdf_estimation'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 0\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "models = {}\n",
    "know_outputs = {}\n",
    "novelty_outputs = {}\n",
    "\n",
    "reconstructed_data = {}\n",
    "klDivergenceFreq = {}\n",
    "\n",
    "n_bins = 100\n",
    "\n",
    "def getPDF(ifold):\n",
    "    print '[*] Processing %i fold...'%ifold\n",
    "    train_id, test_id = CVO[inovelty][ifold]\n",
    "    \n",
    "    # normalize known classes\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "        \n",
    "    known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "    novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "    \n",
    "    models[ifold] = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                           hidden_neurons=hidden_neurons[:layer],\n",
    "                                           layer=layer, ifold=ifold)\n",
    "    \n",
    "    know_outputs[ifold] = models[ifold].predict(known_data)\n",
    "    novelty_outputs[ifold] = models[ifold].predict(novelty_data)\n",
    "    \n",
    "    pdf_freq = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    for ifrequency in range(0,400):\n",
    "        data = known_data[:,ifrequency]\n",
    "        reconstructed_data = know_outputs[ifold][:,ifrequency]\n",
    "        m_bins = np.linspace(data.min(), data.max(), n_bins)\n",
    "        \n",
    "        pdf_freq[ifrequency] = EstPDF(data.reshape(-1,1),\n",
    "                               bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                               kernel_bw=0.1, verbose=False)\n",
    "    return ifold, pdf_freq\n",
    "\n",
    "# Start Parallel processing\n",
    "p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "folds = range(len(CVO[inovelty]))\n",
    "pdfEstimated = p.map(getPDF, folds)\n",
    "\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimação de PDF através de Espectro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ifrequency = 399\n",
    "for ifold in range(n_folds):\n",
    "    fig, m_ax = plt.subplots(figsize=(10,10),nrows=1, ncols=1)\n",
    "    m_ax.plot(pdfEstimated[ifold][1][ifrequency][1], pdfEstimated[ifold][1][ifrequency][0])\n",
    "    m_ax.set_title('PDF x Frequency(%i) - Layer %i - Fold %i - Novelty Class %i'%(ifrequency,layer, ifold, inovelty), fontweight='bold', fontsize=16)\n",
    "    m_ax.set_xlabel('Amplitude', fontsize=18)\n",
    "    m_ax.set_ylabel('Estimated Probability Density', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inovelty = 0\n",
    "layer = 1\n",
    "ifold = 0\n",
    "\n",
    "iwindow = 3\n",
    "\n",
    "freqs = range(0,400)\n",
    "\n",
    "known_outputs = {}\n",
    "novelty_outputs = {}\n",
    "\n",
    "print '[*] Processing %i fold...'%ifold\n",
    "train_id, test_id = CVO[inovelty][ifold]\n",
    "\n",
    "# normalize known classes\n",
    "if trn_params.params['norm'] == 'mapstd':\n",
    "    scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "    scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "elif trn_params.params['norm'] == 'mapminmax':\n",
    "    scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "\n",
    "models[ifold] = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                       hidden_neurons=hidden_neurons[:layer],\n",
    "                                       layer=layer, ifold=ifold)\n",
    "\n",
    "known_outputs[ifold] = models[ifold].predict(known_data)\n",
    "novelty_outputs[ifold] = models[ifold].predict(novelty_data)\n",
    "\n",
    "pdfEstimated = known_data[iwindow]/metrics.auc(freqs, known_data[iwindow])\n",
    "pdfEstimated_output = known_outputs[ifold][iwindow]/metrics.auc(freqs, known_outputs[ifold][iwindow])\n",
    "\n",
    "def KLDiv(p_pdf, q_pdf):\n",
    "    kl_values = []\n",
    "    for i in range(len(p_pdf)):\n",
    "        if p_pdf[i] == 0 or q_pdf[i] == 0 :\n",
    "            kl_values = np.append(kl_values,0)\n",
    "        else:\n",
    "            kl_value = np.abs(p_pdf[i]*np.log10(p_pdf[i]/q_pdf[i]))\n",
    "            if np.isnan(kl_value):\n",
    "                kl_values = np.append(kl_values,0)\n",
    "            else:\n",
    "                kl_values = np.append(kl_values,kl_value)\n",
    "    return [np.sum(kl_values),kl_values]\n",
    "\n",
    "kl_novelty = np.zeros(novelty_data.shape[1])\n",
    "for iwindow in range(novelty_data.shape[1]):\n",
    "    pdfEstimated_novelty = novelty_data[iwindow]/metrics.auc(freqs, novelty_data[iwindow])\n",
    "    pdfEstimated_noveltyoutput = novelty_outputs[ifold][iwindow]/metrics.auc(freqs, novelty_outputs[ifold][iwindow])\n",
    "    [kl_novelty[iwindow], klVector] = KLDiv(pdfEstimated_novelty, pdfEstimated_noveltyoutput)\n",
    "    \n",
    "plt.figure(figsize=[8,8])\n",
    "\n",
    "m_bins = np.linspace(-0, 20, 50)\n",
    "kl_novelty.shape\n",
    "n, bins, patches = plt.hist(kl_novelty, bins=m_bins, alpha=0.8, normed=1)\n",
    "plt.ylabel('Frequency', fontweight='bold',fontsize=15)\n",
    "plt.title('Histogram of KL Divergence of Novelty Class', fontweight='bold',fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDiv(p_pdf, q_pdf):\n",
    "    kl_values = []\n",
    "    for i in range(len(p_pdf)):\n",
    "        if p_pdf[i] == 0 or q_pdf[i] == 0 :\n",
    "            kl_values = np.append(kl_values,0)\n",
    "        else:\n",
    "            kl_value = np.abs(p_pdf[i]*np.log10(p_pdf[i]/q_pdf[i]))\n",
    "            if np.isnan(kl_value):\n",
    "                kl_values = np.append(kl_values,0)\n",
    "            else:\n",
    "                kl_values = np.append(kl_values,kl_value)\n",
    "    return [np.sum(kl_values),kl_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram of KL Divergence\n",
    "\n",
    "# Choose layer \n",
    "layer = 4\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 1\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "for ifold in range(len(CVO[inovelty])):\n",
    "    class_outputs = {}\n",
    "    class_data = {}\n",
    "    kl_class = {}\n",
    "\n",
    "    freqs = range(0,400)\n",
    "\n",
    "    irow = -1\n",
    "    icolumn = 0\n",
    "\n",
    "    rows = 2\n",
    "    columns = 2\n",
    "    fig, m_ax = plt.subplots(figsize=(12,12),nrows=rows, ncols=columns)\n",
    "\n",
    "    m_bins = np.linspace(-0, 20, 50)\n",
    "    itarget = 0\n",
    "    ilabel = 0\n",
    "\n",
    "    labels = range(trgt_sparse.shape[1])\n",
    "    labels.remove(inovelty)\n",
    "\n",
    "    print \"Layer: %i\"%layer\n",
    "    print \"Novelty Class: %i\"%inovelty\n",
    "    print \"Neurons Topology: %s\"%neurons_str\n",
    "    print \"Fold: %i\"%ifold\n",
    "    for iclass in range(len(class_labels)):\n",
    "        class_data[iclass] = np.zeros(n_folds, dtype=object)\n",
    "        class_outputs[iclass] = np.zeros(n_folds, dtype=object)\n",
    "\n",
    "        print '[*] Processing class %i...'%(iclass)\n",
    "        train_id, test_id = CVO[inovelty][ifold]\n",
    "\n",
    "        # normalize known classes\n",
    "        if trn_params.params['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "        if iclass == inovelty:\n",
    "            class_data[iclass][ifold] = scaler.transform(all_data[all_trgt==inovelty])\n",
    "        else:\n",
    "            tmp = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "            class_data[iclass][ifold] = tmp[all_trgt[test_id]==itarget]\n",
    "            itarget = itarget + 1\n",
    "\n",
    "        model = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                               hidden_neurons=hidden_neurons[:layer],\n",
    "                                               layer=layer, ifold=ifold)\n",
    "\n",
    "        class_outputs[iclass][ifold] = model.predict(class_data[iclass][ifold])\n",
    "        kl_class[iclass] = np.zeros(class_data[iclass][ifold].shape[1])\n",
    "\n",
    "        pdfEstimated = []\n",
    "        pdfEstimated_output = []\n",
    "        for iwindow in range(class_data[iclass][ifold].shape[1]):\n",
    "            pdfEstimated = class_data[iclass][ifold][iwindow]/metrics.auc(freqs, class_data[iclass][ifold][iwindow])\n",
    "            pdfEstimated_output = class_outputs[iclass][ifold][iwindow]/metrics.auc(freqs, class_outputs[iclass][ifold][iwindow])\n",
    "            [kl_class[iclass][iwindow], klVector] = KLDiv(pdfEstimated, pdfEstimated_output)\n",
    "        if iclass % columns == 0:\n",
    "            irow = irow + 1\n",
    "            icolumn = 0\n",
    "        else: \n",
    "            icolumn = icolumn + 1\n",
    "\n",
    "        m_ax[irow, icolumn].set_ylabel('Frequency', fontweight='bold',fontsize=15)\n",
    "        if iclass == inovelty:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'blue'\n",
    "        n, bins, patches = m_ax[irow,icolumn].hist(kl_class[iclass], bins=m_bins, alpha=0.8, normed=1, color=color)\n",
    "        m_ax[irow,icolumn].set_title('KL Divergence of Class %i'%(iclass), fontweight='bold',fontsize=15)\n",
    "        m_ax[irow,icolumn].grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograma dos Máximos de KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Max KL Divergence\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 0\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "for ifold in range(len(CVO[inovelty])):\n",
    "    class_outputs = {}\n",
    "    class_data = {}\n",
    "    kl_class = {}\n",
    "\n",
    "    freqs = range(0,400)\n",
    "\n",
    "    irow = -1\n",
    "    icolumn = 0\n",
    "\n",
    "    rows = 2\n",
    "    columns = 2\n",
    "    fig, m_ax = plt.subplots(figsize=(12,12),nrows=rows, ncols=columns)\n",
    "\n",
    "    m_bins = np.linspace(-0, 20, 50)\n",
    "    itarget = 0\n",
    "\n",
    "    labels = range(trgt_sparse.shape[1])\n",
    "    labels.remove(inovelty)\n",
    "\n",
    "    print \"Layer: %i\"%layer\n",
    "    print \"Novelty Class: %i\"%inovelty\n",
    "    print \"Neurons Topology: %s\"%neurons_str\n",
    "    print \"Fold: %i\"%ifold\n",
    "    for iclass in range(len(class_labels)):\n",
    "        class_data[iclass] = np.zeros(n_folds, dtype=object)\n",
    "        class_outputs[iclass] = np.zeros(n_folds, dtype=object)\n",
    "\n",
    "        print '[*] Processing class %i...'%(iclass)\n",
    "        train_id, test_id = CVO[inovelty][ifold]\n",
    "\n",
    "        # normalize known classes\n",
    "        if trn_params.params['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "        if iclass == inovelty:\n",
    "            class_data[iclass][ifold] = scaler.transform(all_data[all_trgt==inovelty])\n",
    "        else:\n",
    "            tmp = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "            class_data[iclass][ifold] = tmp[all_trgt[test_id]==itarget]\n",
    "            itarget = itarget + 1\n",
    "\n",
    "        model = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                               hidden_neurons=hidden_neurons[:layer],\n",
    "                                               layer=layer, ifold=ifold)\n",
    "\n",
    "        class_outputs[iclass][ifold] = model.predict(class_data[iclass][ifold])\n",
    "        kl_class[iclass] = np.zeros(class_data[iclass][ifold].shape[0])\n",
    "\n",
    "        pdfEstimated = []\n",
    "        pdfEstimated_output = []\n",
    "        for iwindow in range(class_data[iclass][ifold].shape[0]):\n",
    "            pdfEstimated = class_data[iclass][ifold][iwindow,:]/metrics.auc(freqs, class_data[iclass][ifold][iwindow,:])\n",
    "            pdfEstimated_output = class_outputs[iclass][ifold][iwindow,:]/metrics.auc(freqs, class_outputs[iclass][ifold][iwindow,:])\n",
    "            [kl_class[iclass][iwindow], klVector] = KLDiv(pdfEstimated, pdfEstimated_output)\n",
    "        if iclass % columns == 0:\n",
    "            irow = irow + 1\n",
    "            icolumn = 0\n",
    "        else: \n",
    "            icolumn = icolumn + 1\n",
    "\n",
    "        m_ax[irow, icolumn].set_ylabel('Frequency', fontweight='bold',fontsize=15)\n",
    "        if iclass == inovelty:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'blue'\n",
    "        n, bins, patches = m_ax[irow,icolumn].hist(np.max(kl_class[iclass], axis=1), bins=m_bins, alpha=0.8, normed=1, color=color)\n",
    "        m_ax[irow,icolumn].set_title('Maximum KL Divergence of Class %i'%(iclass), fontweight='bold',fontsize=15)\n",
    "        m_ax[irow,icolumn].grid()\n",
    "    plt.show()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfEstimated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergência KL - LOFARGRAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOFARGram for reconstructed input \n",
    "current_analysis = 'LOFARGram_reconstruction_first_layer'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "# Choose neurons topology\n",
    "ineuron = 400\n",
    "\n",
    "# Choose model\n",
    "inovelty = 1\n",
    "\n",
    "# Choose num of lines to plot\n",
    "points = 100\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "    \n",
    "for ifold in range(len(CVO[inovelty])):\n",
    "    train_id, test_id = CVO[inovelty][ifold]\n",
    "    \n",
    "    # normalize known classes\n",
    "    known_data = all_data[all_trgt!=inovelty]\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data)\n",
    "\n",
    "    # Get the model file\n",
    "    model_str = '%s/%s/%s_%i_novelty_%i_folds_%s_400x%i_neurons'%(results_path,analysis_str,\n",
    "                                                               model_prefix_str,inovelty,\n",
    "                                                               n_folds,params_str,\n",
    "                                                               ineuron)\n",
    "\n",
    "    if not development_flag:        \n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    print 'Loading %s'%file_name\n",
    "    model = load_model(file_name)\n",
    "    \n",
    "    all_output = model.predict(norm_data[all_trgt==inovelty])\n",
    "\n",
    "    m_fontsize = 12\n",
    "\n",
    "    fig, subplot_array = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\n",
    "    for i in range(2):\n",
    "        ax = plt.subplot(1,2,i+1)\n",
    "        if i == 0:\n",
    "            plt.imshow(norm_data[all_trgt==inovelty,:],\n",
    "               cmap=\"jet\",extent=[1, 400, norm_data[all_trgt==inovelty,:].shape[0],1],\n",
    "               aspect=\"auto\")\n",
    "            plt.xlabel('Frequencies (Hz)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "            plt.title('Lofar Analysis for Original Data of %s'%(class_labels[inovelty]), fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        if (i == 1):\n",
    "            plt.ylabel('Time (seconds)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "            plt.imshow(all_output,\n",
    "               cmap=\"jet\",extent=[1, 400, all_output.shape[0],1],\n",
    "               aspect=\"auto\")\n",
    "            plt.title('Lofar Analysis for Reconstructed Data of %s'%(class_labels[inovelty]), fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        \n",
    "        # Plot lines at frequencies with high errors\n",
    "        for index in indexes[:points]:\n",
    "            plt.axvline(index, color='r', alpha=0.7)\n",
    "            \n",
    "        plt.xticks(np.linspace(0,400,9),rotation=45)\n",
    "        cbar = plt.colorbar()\n",
    "        plt.clim(-2,9)\n",
    "        #if ((iclass == 1) or (iclass==3)):\n",
    "        cbar.ax.set_ylabel('dB',fontweight='bold') \n",
    "        #Save the figure\n",
    "        file_name = pict_results_path+'/'+current_analysis+'_%i_inovelty'%inovelty+'_%i'%ifold+'_fold'+'_%s'%neurons_str+'_neurons_'+trn_params.get_params_str()+'.pdf'\n",
    "        plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergência KL - Frequência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# Reconstruction Analysis\n",
    "%matplotlib inline \n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'kl_divergence_frequency'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 0\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "models = {}\n",
    "know_outputs = {}\n",
    "novelty_outputs = {}\n",
    "\n",
    "reconstructed_data = {}\n",
    "klDivergenceFreq = {}\n",
    "\n",
    "n_bins = 100\n",
    "\n",
    "def getKlDiv(ifold):\n",
    "    print '[*] Processing %i fold...'%ifold\n",
    "    train_id, test_id = CVO[inovelty][ifold]\n",
    "    \n",
    "    # normalize known classes\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "        \n",
    "    known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "    novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "    \n",
    "    models[ifold] = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                           hidden_neurons=hidden_neurons[:layer],\n",
    "                                           layer=layer, ifold=ifold)\n",
    "    \n",
    "    know_outputs[ifold] = models[ifold].predict(known_data)\n",
    "    novelty_outputs[ifold] = models[ifold].predict(novelty_data)\n",
    "    \n",
    "    kl = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    for ifrequency in range(0,400):\n",
    "        data = known_data[:,ifrequency]\n",
    "        reconstructed_data = know_outputs[ifold][:,ifrequency]\n",
    "        m_bins = np.linspace(data.min(), data.max(), n_bins)\n",
    "        kl[ifrequency] = KLDiv(data.reshape(-1,1), reconstructed_data.reshape(-1,1),\n",
    "                               bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                               kernel_bw=0.1, verbose=False)\n",
    "        kl[ifrequency] = kl[ifrequency][0]\n",
    "    return ifold, kl\n",
    "\n",
    "# Start Parallel processing\n",
    "p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "folds = range(len(CVO[inovelty]))\n",
    "klDivergenceFreq = p.map(getKlDiv, folds)\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "freqs = range(0,400)\n",
    "for ifold in range(n_folds):\n",
    "    fig, m_ax = plt.subplots(figsize=(10,10),nrows=1, ncols=1)\n",
    "    m_ax.plot(freqs, klDivergenceFreq[ifold][1])\n",
    "    m_ax.set_title('KL Divergence x Frequency - Layer %i - Fold %i - Novelty Class %i'%(layer, ifold, inovelty), fontweight='bold', fontsize=16)\n",
    "    m_ax.set_xlabel('Frequency', fontsize=18)\n",
    "    m_ax.set_ylabel('Kullback-Leibler Divergence', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergência KL Naive - Supondo independência nas features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# Reconstruction Analysis\n",
    "%matplotlib inline \n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'klDivergence_naive'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 0\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "reconstructed_data = {}\n",
    "klDivergenceFreq = {}\n",
    "\n",
    "n_bins = 100\n",
    "\n",
    "def KLDivCustom(p_pdf, q_pdf):   \n",
    "    kl_values = []\n",
    "    for i in range(len(p_pdf)):\n",
    "        if p_pdf[i] == 0 or q_pdf[i] == 0 :\n",
    "            kl_values = np.append(kl_values,0)\n",
    "        else:\n",
    "            kl_value = np.abs(p_pdf[i]*np.log10(p_pdf[i]/q_pdf[i]))\n",
    "            if np.isnan(kl_value):\n",
    "                kl_values = np.append(kl_values,0)\n",
    "            else:\n",
    "                kl_values = np.append(kl_values,kl_value)\n",
    "    return [np.sum(kl_values),kl_values]\n",
    "\n",
    "def getKL(ifold):\n",
    "    print '[*] Processing %i fold...'%ifold\n",
    "    train_id, test_id = CVO[inovelty][ifold]\n",
    "    \n",
    "    # normalize known classes\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "        \n",
    "    known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "    novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "    \n",
    "    model = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                           hidden_neurons=hidden_neurons[:layer],\n",
    "                                           layer=layer, ifold=ifold)\n",
    "    \n",
    "    known_output = model.predict(known_data)\n",
    "    novelty_output = model.predict(novelty_data)\n",
    "    \n",
    "    pdf_known = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    pdf_known_output = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    \n",
    "    pdf_novelty = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    pdf_novelty_output = np.zeros([all_data.shape[1]], dtype=object)\n",
    "\n",
    "    joint_pdf_known = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    joint_pdf_known_output = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    \n",
    "    joint_pdf_novelty = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    joint_pdf_novelty_output = np.zeros([all_data.shape[1]], dtype=object)\n",
    "    # Suppose independence in frequency, so the joint probability function becomes the product of marginal PDFs\n",
    "    for ifrequency in range(0,400):\n",
    "        # Estimate PDF for known data reconstruction\n",
    "        data = known_data[:,ifrequency]\n",
    "        reconstructed_data = known_output[:,ifrequency]\n",
    "        m_bins = np.linspace(data.min(), data.max(), n_bins)\n",
    "        \n",
    "        [p_pdf,p_bins] = EstPDF(data.reshape(-1,1),\n",
    "                                bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                                kernel_bw=0.1, verbose=False)\n",
    "        pdf_known[ifrequency]  = p_pdf\n",
    "        \n",
    "        [p_pdf,p_bins] = EstPDF(reconstructed_data.reshape(-1,1),\n",
    "                                bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                                kernel_bw=0.1, verbose=False)\n",
    "        pdf_known_output[ifrequency]  = p_pdf\n",
    "        \n",
    "        # Estimate PDF for novelty data reconstruction\n",
    "        data = novelty_data[:,ifrequency]\n",
    "        reconstructed_novelty_data = novelty_output[:,ifrequency]\n",
    "        m_bins = np.linspace(data.min(), data.max(), n_bins)\n",
    "        \n",
    "        [p_pdf,p_bins] = EstPDF(data.reshape(-1,1),\n",
    "                                 bins=m_bins, mode='hist', kernel='epanechnikov',\n",
    "                                 kernel_bw=0.1, verbose=False)\n",
    "        pdf_novelty[ifrequency] = p_pdf\n",
    "        \n",
    "        [p_pdf,p_bins] = EstPDF(reconstructed_novelty_data.reshape(-1,1),\n",
    "                                bins=m_bins, mode='hist', kernel='epanechnikov',\n",
    "                                kernel_bw=0.1, verbose=False)\n",
    "        pdf_novelty_output[ifrequency] = p_pdf\n",
    "\n",
    "        if ifrequency == 0:\n",
    "            joint_pdf_known = pdf_known[ifrequency]\n",
    "            joint_pdf_known_output = pdf_known_output[ifrequency]\n",
    "            joint_pdf_novelty = pdf_novelty[ifrequency]\n",
    "            joint_pdf_novelty_output = pdf_novelty_output[ifrequency]\n",
    "        else:\n",
    "            joint_pdf_known = joint_pdf_known * pdf_known[ifrequency]\n",
    "            joint_pdf_known_output = joint_pdf_known_output * pdf_known_output[ifrequency]\n",
    "            joint_pdf_novelty = joint_pdf_novelty * pdf_novelty[ifrequency]\n",
    "            joint_pdf_novelty_output = joint_pdf_novelty_output * pdf_novelty_output[ifrequency]\n",
    "#     # Calculate KL Div for known data reconstruction\n",
    "#     klKnown = KLDivCustom(joint_pdf_known, joint_pdf_known_output)\n",
    "#     klKnown = klKnown[0] \n",
    "    \n",
    "#     # Calculate KL Div for novelty data reconstruction\n",
    "#     klNovelty = KLDivCustom(joint_pdf_known, joint_pdf_known_output)\n",
    "#     klNovelty = klNovelty[0]         \n",
    "        \n",
    "    return ifold #, klKnown, klNovelty\n",
    "\n",
    "# Start Parallel processing\n",
    "p = multiprocessing.Pool(processes=num_processes+2)\n",
    "\n",
    "folds = range(len(CVO[inovelty]))\n",
    "klDivergence = p.map(getKL, folds)\n",
    "\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifold = 0\n",
    "print '[*] Processing %i fold...'%ifold\n",
    "train_id, test_id = CVO[inovelty][ifold]\n",
    "\n",
    "# normalize known classes\n",
    "if trn_params.params['norm'] == 'mapstd':\n",
    "    scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "    scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "elif trn_params.params['norm'] == 'mapminmax':\n",
    "    scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "\n",
    "model = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                       hidden_neurons=hidden_neurons[:layer],\n",
    "                                       layer=layer, ifold=ifold)\n",
    "\n",
    "known_output = model.predict(known_data)\n",
    "novelty_output = model.predict(novelty_data)\n",
    "\n",
    "pdf_known = np.zeros([all_data.shape[1]], dtype=object)\n",
    "pdf_known_output = np.zeros([all_data.shape[1]], dtype=object)\n",
    "\n",
    "pdf_novelty = np.zeros([all_data.shape[1]], dtype=object)\n",
    "pdf_novelty_output = np.zeros([all_data.shape[1]], dtype=object)\n",
    "\n",
    "# Suppose independence in frequency, so the joint probability function becomes the product of marginal PDFs\n",
    "for ifrequency in range(0,400):\n",
    "    # Estimate PDF for known data reconstruction\n",
    "    data = known_data[:,ifrequency]\n",
    "    reconstructed_data = known_output[:,ifrequency]\n",
    "    m_bins = np.linspace(data.min(), data.max(), n_bins)\n",
    "\n",
    "    [p_pdf,p_bins] = EstPDF(data.reshape(-1,1),\n",
    "                            bins=m_bins, mode='hist', kernel='epanechnikov',\n",
    "                            kernel_bw=0.1, verbose=False)\n",
    "    pdf_known[ifrequency]  = p_pdf\n",
    "\n",
    "    [p_pdf,p_bins] = EstPDF(reconstructed_data.reshape(-1,1),\n",
    "                            bins=m_bins, mode='hist', kernel='epanechnikov',\n",
    "                            kernel_bw=0.1, verbose=False)\n",
    "    pdf_known_output[ifrequency]  = p_pdf\n",
    "\n",
    "    # Estimate PDF for novelty data reconstruction\n",
    "    data = novelty_data[:,ifrequency]\n",
    "    reconstructed_novelty_data = novelty_output[:,ifrequency]\n",
    "    m_bins = np.linspace(data.min(), data.max(), n_bins)\n",
    "\n",
    "    [p_pdf,p_bins] = EstPDF(data.reshape(-1,1),\n",
    "                            bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                            kernel_bw=0.1, verbose=False)\n",
    "    pdf_novelty[ifrequency] = p_pdf\n",
    "\n",
    "    [p_pdf,p_bins] = EstPDF(reconstructed_novelty_data.reshape(-1,1),\n",
    "                            bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                            kernel_bw=0.1, verbose=False)\n",
    "    pdf_novelty_output[ifrequency] = p_pdf\n",
    "for ifrequency in range(0,400):\n",
    "    if ifrequency == 0:\n",
    "        joint_pdf_known = pdf_known[ifrequency]\n",
    "        joint_pdf_known_output = pdf_known_output[ifrequency]\n",
    "        joint_pdf_novelty = pdf_novelty[ifrequency]\n",
    "        joint_pdf_novelty_output = pdf_novelty_output[ifrequency]\n",
    "    else:\n",
    "        joint_pdf_known = joint_pdf_known * pdf_known[ifrequency]\n",
    "        joint_pdf_known_output = joint_pdf_known_output * pdf_known_output[ifrequency]\n",
    "        joint_pdf_novelty = joint_pdf_novelty * pdf_novelty[ifrequency]\n",
    "        joint_pdf_novelty_output = joint_pdf_novelty_output * pdf_novelty_output[ifrequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifrequency = 0\n",
    "fig, m_ax = plt.subplots(figsize=(10,10),nrows=1, ncols=1)\n",
    "m_ax.plot(p_bins, p_pdf*(pdf_known[ifrequency])\n",
    "m_ax.set_title('PDF x Frequency(%i) - Layer %i - Fold %i - Novelty Class %i'%(ifrequency,layer, ifold, inovelty), fontweight='bold', fontsize=16)\n",
    "m_ax.set_xlabel('Amplitude', fontsize=18)\n",
    "m_ax.set_ylabel('Estimated Probability Density', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_pdf_known = np.ones([n_bins])\n",
    "joint_pdf_known_output = np.ones([n_bins])\n",
    "joint_pdf_novelty = np.ones([n_bins])\n",
    "joint_pdf_novelty_output = np.ones([n_bins])\n",
    "\n",
    "for ifrequency in range(0,10):\n",
    "    joint_pdf_known = joint_pdf_known*pdf_known[ifrequency]\n",
    "    joint_pdf_known_output = joint_pdf_known_output * pdf_known_output[ifrequency]\n",
    "    joint_pdf_novelty = joint_pdf_novelty * pdf_novelty[ifrequency]\n",
    "    joint_pdf_novelty_output = joint_pdf_novelty_output * pdf_novelty_output[ifrequency]\n",
    "print joint_pdf_known\n",
    "print joint_pdf_known_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_known[ifrequency]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergência KL - Variação de Neurônios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neuron variation x KL Divergence\n",
    "%matplotlib inline \n",
    "\n",
    "# Choose layer \n",
    "layer = 4\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_mat = [10, 20] + range(50,450,50)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 3\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'klDivergence_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "n_folds = len(CVO[inovelty])\n",
    "\n",
    "klDivergenceFreq = {}\n",
    "klDivergenceKnown = np.zeros([n_folds, len(neurons_mat)], dtype=object)\n",
    "klDivergenceKnownFreq = {}\n",
    "klDivergenceNovelty = np.zeros([n_folds, len(neurons_mat)], dtype=object)\n",
    "klDivergenceNoveltyFreq = {}\n",
    "\n",
    "# if os.path.exists(analysis_file_name):\n",
    "#     os.remove(analysis_file_name)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    for ineuron in neurons_mat: \n",
    "        if ineuron == 0:\n",
    "            ineuron = 1\n",
    "        neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer-1]+[ineuron])\n",
    "\n",
    "        models = {}\n",
    "        outputs = {}\n",
    "        norm_data = {}\n",
    "        reconstructed_known_data = {}\n",
    "        reconstructed_novelty_data = {}\n",
    "        if verbose: \n",
    "            print '[*] Layer: %i - Topology: %s'%(layer, neurons_str)\n",
    "\n",
    "        n_bins = 100\n",
    "\n",
    "        def getKlDiv(ifold):\n",
    "            train_id, test_id = CVO[inovelty][ifold]\n",
    "\n",
    "            # normalize known classes\n",
    "            if trn_params.params['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "            known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "            novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "\n",
    "            model = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                           hidden_neurons=hidden_neurons[:layer-1]+[ineuron],\n",
    "                                           layer=layer, ifold=ifold)\n",
    "\n",
    "            known_output = model.predict(known_data)\n",
    "            novelty_output = model.predict(novelty_data)\n",
    "\n",
    "            klKnown = np.zeros([all_data.shape[1]], dtype=object)\n",
    "            klNovelty = np.zeros([all_data.shape[1]], dtype=object)\n",
    "            \n",
    "            for ifrequency in range(0,400):\n",
    "                # Calculate KL Div for known data reconstruction\n",
    "                known_data_freq = known_data[:,ifrequency]\n",
    "                reconstructed_known_data = known_output[:,ifrequency]\n",
    "\n",
    "                m_bins = np.linspace(known_data_freq.min(), known_data_freq.max(), n_bins)\n",
    "                \n",
    "                klKnown[ifrequency] = KLDiv(known_data_freq.reshape(-1,1), reconstructed_known_data.reshape(-1,1),\n",
    "                                       bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                                       kernel_bw=0.1, verbose=False)\n",
    "\n",
    "                klKnown[ifrequency] = klKnown[ifrequency][0]\n",
    "                \n",
    "                # Calculate KL Div for novelty data reconstruction\n",
    "                novelty_data_freq = novelty_data[:,ifrequency]\n",
    "                reconstructed_novelty_data = novelty_output[:,ifrequency]\n",
    "\n",
    "                m_bins = np.linspace(novelty_data_freq.min(), novelty_data_freq.max(), n_bins)\n",
    "                \n",
    "                klNovelty[ifrequency] = KLDiv(novelty_data_freq.reshape(-1,1), reconstructed_novelty_data.reshape(-1,1),\n",
    "                                       bins=m_bins, mode='kernel', kernel='epanechnikov',\n",
    "                                       kernel_bw=0.1, verbose=False)\n",
    "\n",
    "                klNovelty[ifrequency] = klNovelty[ifrequency][0]\n",
    "                \n",
    "            return ifold, klKnown, klNovelty\n",
    "\n",
    "        # Start Parallel processing\n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "        folds = range(len(CVO[inovelty]))\n",
    "        if verbose:\n",
    "            print '[*] Calculating KL Div for all frequencies...'\n",
    "        # Calculate the KL Div at all frequencies\n",
    "        klDivergenceFreq[ineuron] = p.map(getKlDiv, folds)\n",
    "        \n",
    "        p.close()\n",
    "        p.join()\n",
    "        \n",
    "        index = neurons_mat.index(ineuron)\n",
    "        for ifold in range(n_folds):\n",
    "            klDivergenceKnownFreq = klDivergenceFreq[ineuron][ifold][1]\n",
    "            klDivergenceNoveltyFreq = klDivergenceFreq[ineuron][ifold][2]\n",
    "            \n",
    "            klDivergenceKnown[ifold, index] = np.sum(klDivergenceKnownFreq)\n",
    "            klDivergenceNovelty[ifold, index] = np.sum(klDivergenceNoveltyFreq)\n",
    "\n",
    "        joblib.dump([neurons_mat,klDivergenceKnown,klDivergenceNovelty],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [neurons_mat, klDivergenceKnown, klDivergenceNovelty] = joblib.load(analysis_file_name)\n",
    "# Plot results    \n",
    "fig, m_ax = plt.subplots(figsize=(20,30),nrows=5, ncols=2)\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    irow = int(ifold/2)\n",
    "    if (ifold % 2 == 0):\n",
    "        icolumn = 0\n",
    "    else: \n",
    "        icolumn = 1\n",
    "\n",
    "    m_ax[irow, icolumn].plot(neurons_mat, klDivergenceKnown[ifold,:], 'b-o', label='Known Test Data')\n",
    "    m_ax[irow, icolumn].plot(neurons_mat, klDivergenceNovelty[ifold,:], 'r--o', label='Novelty Data')\n",
    "    m_ax[irow, icolumn].set_title('KL Divergence x Neurons - Layer %i - Fold %i - Novelty Class %i '%(layer,ifold+1, inovelty), fontsize=16, fontweight='bold')\n",
    "    m_ax[irow, icolumn].set_ylabel('Kullback-Leibler Divergence', fontsize=22)\n",
    "    m_ax[irow, icolumn].set_xlabel('Neurons', fontsize=22)\n",
    "    m_ax[irow, icolumn].grid()\n",
    "    m_ax[irow, icolumn].legend()\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE - Variação de Neurônios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neuron variation x KL Divergence\n",
    "%matplotlib inline \n",
    "\n",
    "# Choose layer \n",
    "layer = 4\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_mat = [10, 20] + range(50,450,50)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 3\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'mean_squared_error_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "n_folds = len(CVO[inovelty])\n",
    "\n",
    "mse = {}\n",
    "mse_known = np.zeros([n_folds, len(neurons_mat)])\n",
    "mse_novelty = np.zeros([n_folds, len(neurons_mat)])\n",
    "\n",
    "# if os.path.exists(analysis_file_name):\n",
    "#     os.remove(analysis_file_name)\n",
    "    \n",
    "if not os.path.exists(analysis_file_name):\n",
    "    for ineuron in neurons_mat: \n",
    "        if ineuron == 0:\n",
    "            ineuron = 1\n",
    "        neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer-1]+[ineuron])\n",
    "\n",
    "        models = {}\n",
    "        outputs = {}\n",
    "        norm_data = {}\n",
    "        reconstructed_known_data = {}\n",
    "        reconstructed_novelty_data = {}\n",
    "        if verbose: \n",
    "            print '[*] Layer: %i - Topology: %s'%(layer, neurons_str)\n",
    "\n",
    "        n_bins = 100\n",
    "\n",
    "        def getMSE(ifold):\n",
    "            train_id, test_id = CVO[inovelty][ifold]\n",
    "\n",
    "            # normalize known classes\n",
    "            if trn_params.params['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "            known_data = scaler.transform(all_data[all_trgt!=inovelty][test_id,:])\n",
    "            novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "\n",
    "            model = SAE[inovelty].getModel(data=all_data, trgt=all_trgt,\n",
    "                                           hidden_neurons=hidden_neurons[:layer-1]+[ineuron],\n",
    "                                           layer=layer, ifold=ifold)\n",
    "\n",
    "            known_output = model.predict(known_data)\n",
    "            novelty_output = model.predict(novelty_data)\n",
    "            \n",
    "            mseKnown = metrics.mean_squared_error(known_data, known_output)\n",
    "            mseNovelty = metrics.mean_squared_error(novelty_data, novelty_output)\n",
    "            \n",
    "                \n",
    "            return ifold, mseKnown, mseNovelty\n",
    "\n",
    "        # Start Parallel processing\n",
    "        p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "        folds = range(len(CVO[inovelty]))\n",
    "        if verbose:\n",
    "            print '[*] Calculating Mean Squared Error ...'\n",
    "        mse[ineuron] = p.map(getMSE, folds)\n",
    "        \n",
    "        for ifold in range(n_folds):\n",
    "            mse_known[:, neurons_mat.index(ineuron)] = mse[ineuron][ifold][1]\n",
    "            mse_novelty[:, neurons_mat.index(ineuron)] = mse[ineuron][ifold][2]\n",
    "        \n",
    "        p.close()\n",
    "        p.join()\n",
    "    \n",
    "        joblib.dump([neurons_mat,mse_known,mse_novelty],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [neurons_mat, mse_known, mse_novelty] = joblib.load(analysis_file_name)\n",
    "# Plot results    \n",
    "fig, m_ax = plt.subplots(figsize=(20,30),nrows=5, ncols=2)\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    irow = int(ifold/2)\n",
    "    if (ifold % 2 == 0):\n",
    "        icolumn = 0\n",
    "    else: \n",
    "        icolumn = 1\n",
    "\n",
    "    m_ax[irow, icolumn].plot(neurons_mat, mse_known[ifold, :], 'b-o', label='Known Test Data')\n",
    "    m_ax[irow, icolumn].plot(neurons_mat, mse_novelty[ifold, :], 'r--o', label='Novelty Data')\n",
    "    m_ax[irow, icolumn].set_title('MSE x Neurons - Layer %i - Fold %i - Novelty Class %i '%(layer,ifold+1, inovelty), fontsize=16, fontweight='bold')\n",
    "    m_ax[irow, icolumn].set_ylabel('Mean Squared Error', fontsize=22)\n",
    "    m_ax[irow, icolumn].set_xlabel('Neurons', fontsize=22)\n",
    "    m_ax[irow, icolumn].grid()\n",
    "    m_ax[irow, icolumn].legend()\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergência KL e MSE - Variação de Neurônios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Neuron variation x KL Divergence\n",
    "%matplotlib inline \n",
    "\n",
    "# Choose layer \n",
    "layer = 4\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_mat = [10, 20] + range(50,450,50)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 1\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "current_analysis = 'mean_squared_error_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "mse_analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "current_analysis = 'klDivergence_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "klDiv_analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "\n",
    "# For picture filename\n",
    "current_analysis = 'klDivergence_mean_squared_error_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "n_folds = len(CVO[inovelty])\n",
    "\n",
    "if os.path.exists(mse_analysis_file_name) and  os.path.exists(klDiv_analysis_file_name):\n",
    "    [neurons_mat, mse_known, mse_novelty] = joblib.load(mse_analysis_file_name)\n",
    "    [neurons_mat, klDivergenceKnown, klDivergenceNovelty] = joblib.load(klDiv_analysis_file_name)\n",
    "    \n",
    "    # Plot results    \n",
    "    for ifold in range(n_folds):\n",
    "        fig, m_ax = plt.subplots(figsize=(10,8),nrows=1, ncols=1)\n",
    "        irow = int(ifold/2)\n",
    "        if (ifold % 2 == 0):\n",
    "            icolumn = 0\n",
    "        else: \n",
    "            icolumn = 1\n",
    "        linewidth = 4.0\n",
    "        ln1 = m_ax.plot(neurons_mat, mse_known[ifold, :], 'b--v', label='MSE Known', markersize=10, linewidth=linewidth)\n",
    "        ln2 = m_ax.plot(neurons_mat, mse_novelty[ifold, :], 'b-^', label='MSE Novelty', markersize=10, linewidth=linewidth)\n",
    "        m_ax.set_title('Layer %i - Fold %i - Novelty Class %i '%(layer,ifold+1, inovelty), fontsize=16, fontweight='bold')\n",
    "        m_ax.set_ylabel(\"Mean Squared Error\", fontsize=22, color='b', fontweight='bold')\n",
    "        m_ax.tick_params('y', colors='b')\n",
    "        ax2 = m_ax.twinx()\n",
    "        ln3 = ax2.plot(neurons_mat, klDivergenceKnown[ifold,:], 'r--v', label='KL Known', markersize=10, linewidth=linewidth)\n",
    "        ln4 = ax2.plot(neurons_mat, klDivergenceNovelty[ifold,:], 'r-^', label='KL Novelty', markersize=10, linewidth=linewidth)\n",
    "        ax2.set_ylabel('KL Divergence', fontsize=22, color='r', fontweight='bold')\n",
    "        ax2.tick_params('y', colors='r')\n",
    "        ax2.set_yticks(range(0,280,40))\n",
    "        ax2.set_ylim([0,280])\n",
    "        ax2.grid()\n",
    "        \n",
    "        m_ax.set_ylim([0,1.4])\n",
    "        m_ax.set_yticks(np.arange(0,1.4,0.2))\n",
    "        m_ax.set_xlabel('Neurons', fontsize=22, fontweight='bold')\n",
    "        m_ax.grid()\n",
    "        \n",
    "        lns = [ln1[0], ln2[0], ln3[0], ln4[0]]\n",
    "        labs = [l.get_label() for l in lns]\n",
    "        plt.legend(lns, labs, ncol=2, loc='upper center')\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        filename = pict_results_path + '/' + current_analysis + '_' + params_str + '.png'\n",
    "        plt.savefig(filename)\n",
    "else: \n",
    "    if not os.path.exists(klDiv_analysis_file_name):\n",
    "        print \"[-] KL Divergence analysis not done.\"\n",
    "    if not os.path.exists(mse_analysis_file_name):\n",
    "        print \"[-] MSE analysis not done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Divergência KL e MSE (média nos Folds) - Variação de Neurônios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron variation x KL Divergence X MSE - Mean Value at Folds\n",
    "%matplotlib inline \n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_mat = [10, 20] + range(50,450,50)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "\n",
    "# Choose model\n",
    "inovelty = 1\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "current_analysis = 'mean_squared_error_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "mse_analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "current_analysis = 'klDivergence_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "klDiv_analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "\n",
    "# For picture filename\n",
    "current_analysis = 'klDivergence_mean_squared_error_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "n_folds = len(CVO[inovelty])\n",
    "\n",
    "if os.path.exists(mse_analysis_file_name) and  os.path.exists(klDiv_analysis_file_name):\n",
    "    [neurons_mat, mse_known, mse_novelty] = joblib.load(mse_analysis_file_name)\n",
    "    [neurons_mat, klDivergenceKnown, klDivergenceNovelty] = joblib.load(klDiv_analysis_file_name)\n",
    "    \n",
    "    # Plot results    \n",
    "    fig, m_ax = plt.subplots(figsize=(10,8),nrows=1, ncols=1)\n",
    "    \n",
    "    linewidth = 3.5\n",
    "    \n",
    "    ln1 = m_ax.errorbar(neurons_mat, np.mean(mse_known, axis=0),\n",
    "                        np.var(mse_known, axis=0), color='b', linestyle='dashed',  marker='v',\n",
    "                        label='MSE Known', markersize=10, linewidth=linewidth)\n",
    "    \n",
    "    ln2 = m_ax.errorbar(neurons_mat, np.mean(mse_novelty, axis=0),\n",
    "                        np.var(mse_novelty, axis=0), color='b', linestyle='solid',  marker='^',\n",
    "                        label='MSE Novelty', markersize=10, linewidth=linewidth)\n",
    "    \n",
    "    m_ax.set_title('Layer %i - Novelty Class %i '%(layer, inovelty), fontsize=16, fontweight='bold')\n",
    "    m_ax.set_ylabel(\"Mean Squared Error\", fontsize=22, color='b', fontweight='bold')\n",
    "    m_ax.tick_params('y', colors='b')\n",
    "    ax2 = m_ax.twinx()\n",
    "    \n",
    "    ln3 = ax2.errorbar(neurons_mat, np.mean(klDivergenceKnown, axis=0),\n",
    "                       np.var(klDivergenceKnown, axis=0), color='r', linestyle='dashed', marker='v',\n",
    "                       label='KL Known', markersize=10, linewidth=linewidth)\n",
    "    \n",
    "    ln4 = ax2.errorbar(neurons_mat, np.mean(klDivergenceNovelty,axis=0),\n",
    "                        np.var(klDivergenceNovelty, axis=0), color='r', linestyle='solid', marker='^',\n",
    "                        label='KL Novelty', markersize=10, linewidth=linewidth)\n",
    "    \n",
    "    ax2.set_ylabel('KL Divergence', fontsize=22, color='r', fontweight='bold')\n",
    "    ax2.tick_params('y', colors='r')\n",
    "    ax2.set_yticks(range(0,280,40))\n",
    "    ax2.set_ylim([0,280])\n",
    "    ax2.grid()\n",
    "\n",
    "    m_ax.set_ylim([0,1.4])\n",
    "    m_ax.set_yticks(np.arange(0,1.4,0.2))\n",
    "    m_ax.set_xlabel('Neurons', fontsize=22, fontweight='bold')\n",
    "    m_ax.grid()\n",
    "\n",
    "    lns = [ln1, ln2, ln3, ln4]\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    plt.legend(lns, labs, ncol=2, loc='upper center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else: \n",
    "    if not os.path.exists(klDiv_analysis_file_name):\n",
    "        print \"[-] KL Divergence analysis not done.\"\n",
    "    if not os.path.exists(mse_analysis_file_name):\n",
    "        print \"[-] MSE analysis not done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction of Known Classes vs Reconstruction of Novelty - novelty detection for neural network\n",
    "%matplotlib inline \n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'reconstruction_known_x_novelty'\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 1\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "\n",
    "models = {}\n",
    "known_outputs = {}\n",
    "novelty_outputs = {}\n",
    "mean = {}\n",
    "indexes = {}\n",
    "\n",
    "n_folds = len(CVO[inovelty])\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    train_id, test_id = CVO[inovelty][ifold]\n",
    "    \n",
    "    # normalize known classes\n",
    "    known_data = all_data[all_trgt!=inovelty]\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "    norm_data = scaler.transform(all_data)\n",
    "    known_data = scaler.transform(known_data[test_id,:])\n",
    "    novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "    \n",
    "    if ifold == 0:\n",
    "        diffSquared = np.zeros([len(CVO[inovelty]), all_data.shape[0], all_data.shape[1]])\n",
    "        \n",
    "    print 'Novelty class: %i - Topology: %s - fold %i'%(inovelty, neurons_str, ifold)\n",
    "    \n",
    "    models[ifold] = SAE[inovelty].getModel(data=trn_data[inovelty], trgt=trn_trgt[inovelty],\n",
    "                                           hidden_neurons=hidden_neurons,\n",
    "                                           layer=layer, ifold=ifold)\n",
    "    \n",
    "    outputs = models[ifold].predict(norm_data)\n",
    "    known_outputs[ifold] = models[ifold].predict(known_data)\n",
    "    novelty_outputs[ifold] = models[ifold].predict(novelty_data)\n",
    "    \n",
    "    diffSquared[ifold] = np.power((norm_data - outputs), 2)\n",
    "      \n",
    "mean = np.mean(np.mean(diffSquared[:,test_id,:], axis=0), axis=0)\n",
    "indexes = np.argsort(mean)[::-1]\n",
    "print \"Topology (%s)\"%trn_params.get_params_str()\n",
    "for ifold in range(len(CVO[inovelty])):\n",
    "    train_id, test_id = CVO[inovelty][ifold]\n",
    "    \n",
    "    # normalize known classes\n",
    "    known_data = all_data[all_trgt!=inovelty]\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "    norm_data = scaler.transform(all_data)\n",
    "    known_data = scaler.transform(known_data[test_id,:])\n",
    "    novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "    \n",
    "    known_points = known_data.shape[0]\n",
    "    novelty_points = novelty_data.shape[0]\n",
    "    \n",
    "    # Number of dimensions to analyse (even number is better!)\n",
    "    num_dim = 4\n",
    "    fig, m_ax = plt.subplots(figsize=(20,20),nrows=2, ncols=2)\n",
    "    for choose_index in range(num_dim):  \n",
    "        ax = plt.subplot(2,2,choose_index+1)\n",
    "        \n",
    "        # Plot novelty \n",
    "        ax.plot(novelty_data[:,indexes[choose_index]][0], \n",
    "                novelty_outputs[ifold][:,indexes[choose_index]][0],\n",
    "                \"r.\", label='Novelty Class', markersize=20)\n",
    "        ax.plot(novelty_data[:,indexes[choose_index]][:novelty_points], \n",
    "                novelty_outputs[ifold][:,indexes[choose_index]][:novelty_points],\n",
    "                \"r.\", alpha=0.3)\n",
    "        \n",
    "        # Plot known classes\n",
    "        ax.plot(known_data[:,indexes[choose_index]][0],\n",
    "                known_outputs[ifold][:,indexes[choose_index]][0],\n",
    "                \"b.\", label='Known Class', markersize=20)\n",
    "        ax.plot(known_data[:,indexes[choose_index]][:known_points],\n",
    "                known_outputs[ifold][:,indexes[choose_index]][:known_points],\n",
    "                \"b.\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "        ax.set_title('Input x Output - Layer - %i - Dim %i'%(layer,indexes[choose_index]),fontsize=22, fontweight='bold')\n",
    "        ax.set_xlim(-2, 7)\n",
    "        ax.set_ylim(-4, 8)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        # sort both labels and handles by labels\n",
    "        labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "        plt.grid() \n",
    "        \n",
    "        # Small plot\n",
    "        rect = [0.065, 0.55, 0.35, 0.4]\n",
    "        ax1 = FunctionsDataVisualization.add_subplot_axes(ax,rect)\n",
    "        \n",
    "        eq_known_reconstruction = np.power((known_data[:,indexes[choose_index]] - known_outputs[ifold][:,indexes[choose_index]]), 2)\n",
    "        eq_novelty_reconstruction = np.power((novelty_data[:,indexes[choose_index]] - novelty_outputs[ifold][:,indexes[choose_index]]), 2)\n",
    "        \n",
    "        mq_bins_known = np.linspace(np.min(eq_known_reconstruction), np.max(eq_known_reconstruction), 50)\n",
    "        mq_bins_novelty = np.linspace(np.min(eq_novelty_reconstruction), np.max(eq_novelty_reconstruction), 50)\n",
    "             \n",
    "        n, bins, patches = ax1.hist(eq_novelty_reconstruction,bins=mq_bins_novelty,\n",
    "                                    fc=\"r\",\n",
    "                                    alpha=0.5, normed=0)\n",
    "        \n",
    "        n, bins, patches = ax1.hist(eq_known_reconstruction,bins=mq_bins_known,\n",
    "                                    fc=\"b\",\n",
    "                                    alpha=0.8, normed=0)   \n",
    "        ax1.set_xlim(0, 0.06)\n",
    "        ax1.set_title(\"Squared Absolute Error\",fontsize=14, fontweight='bold')\n",
    "        ax1.grid() \n",
    "        \n",
    "        # Small plot\n",
    "        rect = [0.5, 0.05, 0.45, 0.4]\n",
    "        ax2 = FunctionsDataVisualization.add_subplot_axes(ax,rect)\n",
    "        \n",
    "        e_known_reconstruction = (known_data[:,indexes[choose_index]] - known_outputs[ifold][:,indexes[choose_index]])\n",
    "        e_novelty_reconstruction = (novelty_data[:,indexes[choose_index]] - novelty_outputs[ifold][:,indexes[choose_index]])\n",
    "        \n",
    "        m_bins_known = np.linspace(np.min(e_known_reconstruction), np.max(e_known_reconstruction), 50)\n",
    "        m_bins_novelty = np.linspace(np.min(e_novelty_reconstruction), np.max(e_novelty_reconstruction), 50)\n",
    "        \n",
    "        n, bins, patches = ax2.hist(e_novelty_reconstruction,bins=m_bins_novelty,\n",
    "                                    fc=\"r\",\n",
    "                                    alpha=0.6, normed=0)\n",
    "        \n",
    "        n, bins, patches = ax2.hist(e_known_reconstruction,bins=m_bins_known,\n",
    "                                    fc=\"b\",\n",
    "                                    alpha=0.8, normed=0)   \n",
    "        ax2.grid()\n",
    "        ax2.set_title(\"Absolute Error\",fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlim(-0.4,0.4) \n",
    "        \n",
    "        ax.legend(handles, labels, ncol=1, loc='upper right')\n",
    "        plt.legend()\n",
    "\n",
    "        mse = metrics.mean_squared_error(known_data[:,indexes[choose_index]], known_outputs[ifold][:,indexes[choose_index]])\n",
    "        ax.text(2, 6, 'MSE: %f'%mse, style='normal',fontsize=26, color='blue',\n",
    "        bbox={'alpha':0.0, 'pad':10})\n",
    "        \n",
    "        mse = metrics.mean_squared_error(novelty_data[:,indexes[choose_index]], novelty_outputs[ifold][:,indexes[choose_index]])\n",
    "        ax.text(2, 5.5, 'MSE: %f'%mse, style='normal',fontsize=26, color='red',\n",
    "        bbox={'alpha':0.0, 'pad':10})\n",
    "        \n",
    "        #Save the figure\n",
    "        file_name = pict_results_path+'/'+current_analysis+'_%i_novelty_%i_layer_%s_neurons_%i_fold_'%(inovelty,\n",
    "                                                                                                       layer,\n",
    "                                                                                                       neurons_str,\n",
    "                                                                                                       ifold)+trn_params.get_params_str()+'.pdf'\n",
    "        plt.savefig(file_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ineuron in [20]:\n",
    "    hidden_neurons = [ineuron]\n",
    "    layer = 1\n",
    "    for inovelty in range(len(CVO)):\n",
    "        print \"Novelty class: %i\"%inovelty\n",
    "        # Train classifiers to their corresponding folds\n",
    "        def trainClassifierFold(ifold): \n",
    "            return SAE[inovelty].trainClassifier(data=all_data,\n",
    "                                trgt = all_trgt,\n",
    "                                ifold = ifold,\n",
    "                                hidden_neurons=hidden_neurons,\n",
    "                                layer = layer)\n",
    "\n",
    "        # Train classifier sweeping the number of layer\n",
    "        def trainClassifierLayer(ilayer):\n",
    "            for ifold in range(len(CVO)):\n",
    "                SAE.trainClassifier(data=all_data,\n",
    "                                    trgt = all_trgt,\n",
    "                                    ifold = ifold,\n",
    "                                    hidden_neurons=hidden_neurons,\n",
    "                                    layer = ilayer)\n",
    "        start_time = time.time()\n",
    "\n",
    "        p = multiprocessing.Pool(num_processes)\n",
    "\n",
    "        folds = range(len(CVO[inovelty]))\n",
    "        results = p.map(trainClassifierFold, folds)\n",
    "\n",
    "        # layers = range(1,11)\n",
    "        # results = p.map(trainClassifierLayer, layers)\n",
    "\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "        print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Classificação - Indíce SP por Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron variation x SO Index\n",
    "%matplotlib inline \n",
    "\n",
    "# Choose layer \n",
    "layer = 2\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_mat = [10, 20] + range(50,450,50)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 1\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'sp_index_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "n_folds = len(CVO[inovelty])\n",
    "\n",
    "results = {}\n",
    "spIndex = np.zeros([n_folds, len(neurons_mat)])\n",
    "\n",
    "if os.path.exists(analysis_file_name):\n",
    "    os.remove(analysis_file_name)\n",
    "    \n",
    "if not os.path.exists(analysis_file_name):\n",
    "    for ineuron in neurons_mat[:len(neurons_mat)-layer+2]: \n",
    "        if ineuron == 0:\n",
    "            ineuron = 1\n",
    "        neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer-1]+[ineuron])\n",
    "        \n",
    "\n",
    "        class_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt))])\n",
    "        known_sp_mat = np.zeros([n_folds])\n",
    "\n",
    "        buff = np.zeros([len(np.unique(all_trgt))-1])\n",
    "        class_eff = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "        known_sp = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "        if verbose: \n",
    "            print '[*] Layer: %i - Topology: %s'%(layer, neurons_str)\n",
    "        \n",
    "        def getSP(ifold):\n",
    "            train_id, test_id = CVO[inovelty][ifold]\n",
    "\n",
    "            # normalize known classes\n",
    "            if trn_params.params['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(trn_data[inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(trn_data[inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(trn_data[inovelty][train_id,:])\n",
    "\n",
    "            known_data = scaler.transform(trn_data[inovelty][test_id,:])\n",
    "            known_trgt = trn_trgt[inovelty][test_id]\n",
    "            classifier = SAE[inovelty].loadClassifier(data  = trn_data[inovelty],\n",
    "                                                      trgt  = trn_trgt[inovelty], \n",
    "                                                      hidden_neurons = hidden_neurons[:layer-1]+[ineuron],\n",
    "                                                      layer = layer,\n",
    "                                                      ifold = ifold)\n",
    "\n",
    "            output = classifier.predict(known_data)\n",
    "            \n",
    "            num_known_classes = trn_trgt_sparse[inovelty].shape[1]\n",
    "            thr_value = 0.2\n",
    "            for iclass, class_id in enumerate(np.unique(all_trgt)):\n",
    "                    if iclass == inovelty:\n",
    "                        continue\n",
    "                    output_of_class_events = output[known_trgt==iclass-(iclass>inovelty),:]\n",
    "                    correct_class_output = np.argmax(output_of_class_events,axis=1)==iclass-(iclass>inovelty)\n",
    "                    output_above_thr = output_of_class_events[correct_class_output,iclass-(iclass>inovelty)]>thr_value\n",
    "                    class_eff = float(sum(output_above_thr))/float(len(output_of_class_events))\n",
    "                    buff[iclass-(iclass>inovelty)] = class_eff\n",
    "                \n",
    "            sp_index = (np.sqrt(np.mean(buff,axis=0)*np.power(np.prod(buff),1./float(len(buff)))))\n",
    "\n",
    "            \n",
    "            return ifold, sp_index\n",
    "\n",
    "        # Start Parallel processing\n",
    "        p = multiprocessing.Pool(processes=num_processes+2)\n",
    "\n",
    "        folds = range(len(CVO[inovelty]))\n",
    "        if verbose:\n",
    "            print '[*] Calculating SP Index ...'\n",
    "        results[ineuron] = p.map(getSP, folds)\n",
    "        \n",
    "        for ifold in range(n_folds):\n",
    "            spIndex[ifold, neurons_mat.index(ineuron)] = results[ineuron][ifold][1]\n",
    "        \n",
    "        p.close()\n",
    "        p.join()\n",
    "    \n",
    "        joblib.dump([neurons_mat,spIndex],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [neurons_mat, spIndex] = joblib.load(analysis_file_name)\n",
    "# Plot results    \n",
    "fig, m_ax = plt.subplots(figsize=(20,30),nrows=5, ncols=2)\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    irow = int(ifold/2)\n",
    "    if (ifold % 2 == 0):\n",
    "        icolumn = 0\n",
    "    else: \n",
    "        icolumn = 1\n",
    "\n",
    "    m_ax[irow, icolumn].plot(neurons_mat, spIndex[ifold, :], 'b-o', label='Known data')\n",
    "    m_ax[irow, icolumn].set_title('SP x Neurons - Layer %i - Fold %i - Novelty Class %i '%(layer,ifold+1, inovelty), fontsize=16, fontweight='bold')\n",
    "    m_ax[irow, icolumn].set_ylabel('SP Index', fontsize=22)\n",
    "    m_ax[irow, icolumn].set_xlabel('Neurons', fontsize=22)\n",
    "    m_ax[irow, icolumn].grid()\n",
    "    m_ax[irow, icolumn].legend()\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron variation x SP Index (Mean at Folds)\n",
    "%matplotlib inline \n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "# hidden_neurons = [20,15,10,5]\n",
    "neurons_mat = [10, 20] + range(50,450,50)\n",
    "# neurons_mat = [5,10,15]\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 2\n",
    "regularizer = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "current_analysis = 'sp_index_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "n_folds = len(CVO[inovelty])\n",
    "\n",
    "results = {}\n",
    "spIndex = np.zeros([n_folds, len(neurons_mat)])\n",
    "\n",
    "if os.path.exists(analysis_file_name):\n",
    "    os.remove(analysis_file_name)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    for ineuron in neurons_mat[:len(neurons_mat)-layer+2]: \n",
    "        if ineuron == 0:\n",
    "            ineuron = 1\n",
    "        neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer-1]+[ineuron])\n",
    "        \n",
    "\n",
    "        class_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt))])\n",
    "        known_sp_mat = np.zeros([n_folds])\n",
    "\n",
    "        buff = np.zeros([len(np.unique(all_trgt))-1])\n",
    "        class_eff = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "        known_sp = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "        if verbose: \n",
    "            print '[*] Layer: %i - Topology: %s'%(layer, neurons_str)\n",
    "        \n",
    "        def getSP(ifold):\n",
    "            train_id, test_id = CVO[inovelty][ifold]\n",
    "\n",
    "            # normalize known classes\n",
    "            if trn_params.params['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(trn_data[inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(trn_data[inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(trn_data[inovelty][train_id,:])\n",
    "\n",
    "            known_data = scaler.transform(trn_data[inovelty][test_id,:])\n",
    "            known_trgt = trn_trgt[inovelty][test_id]\n",
    "            classifier = SAE[inovelty].loadClassifier(data  = trn_data[inovelty],\n",
    "                                                      trgt  = trn_trgt[inovelty], \n",
    "                                                      hidden_neurons = hidden_neurons[:layer-1]+[ineuron],\n",
    "                                                      layer = layer,\n",
    "                                                      ifold = ifold)\n",
    "\n",
    "            output = classifier.predict(known_data)\n",
    "            \n",
    "            num_known_classes = trn_trgt_sparse[inovelty].shape[1]\n",
    "            thr_value = 0.2\n",
    "            for iclass, class_id in enumerate(np.unique(all_trgt)):\n",
    "                    if iclass == inovelty:\n",
    "                        continue\n",
    "                    output_of_class_events = output[known_trgt==iclass-(iclass>inovelty),:]\n",
    "                    correct_class_output = np.argmax(output_of_class_events,axis=1)==iclass-(iclass>inovelty)\n",
    "                    output_above_thr = output_of_class_events[correct_class_output,iclass-(iclass>inovelty)]>thr_value\n",
    "                    class_eff = float(sum(output_above_thr))/float(len(output_of_class_events))\n",
    "                    buff[iclass-(iclass>inovelty)] = class_eff\n",
    "                \n",
    "            sp_index = (np.sqrt(np.mean(buff,axis=0)*np.power(np.prod(buff),1./float(len(buff)))))\n",
    "\n",
    "            \n",
    "            return ifold, sp_index\n",
    "\n",
    "        # Start Parallel processing\n",
    "        p = multiprocessing.Pool(processes=num_processes+2)\n",
    "\n",
    "        folds = range(len(CVO[inovelty]))\n",
    "        if verbose:\n",
    "            print '[*] Calculating SP Index ...'\n",
    "        results[ineuron] = p.map(getSP, folds)\n",
    "        \n",
    "        for ifold in range(n_folds):\n",
    "            spIndex[ifold, neurons_mat.index(ineuron)] = results[ineuron][ifold][1]\n",
    "        \n",
    "        p.close()\n",
    "        p.join()\n",
    "    \n",
    "        joblib.dump([neurons_mat,spIndex],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [neurons_mat, spIndex] = joblib.load(analysis_file_name)\n",
    "\n",
    "# Plot results    \n",
    "fig, m_ax = plt.subplots(figsize=(10,5),nrows=1, ncols=1)\n",
    "m_ax.errorbar(neurons_mat, np.mean(spIndex, axis=0), np.var(spIndex, axis=0), label='Known data',\n",
    "             linewidth=3.0, linestyle='-')\n",
    "m_ax.set_title('SP x Neurons - Layer %i - Novelty Class %i '%(layer, inovelty), fontsize=16, fontweight='bold')\n",
    "m_ax.set_ylabel('SP Index', fontsize=22)\n",
    "m_ax.set_xlabel('Neurons', fontsize=22)\n",
    "m_ax.grid()\n",
    "m_ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron variation x SP Index and KL Divergence - Mean Value at Folds\n",
    "%matplotlib inline \n",
    "\n",
    "# Choose layer \n",
    "layer = 1\n",
    "# Choose neurons topology\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_mat = [10, 20] + range(50,450,50)\n",
    "neurons_mat = neurons_mat[:len(neurons_mat)-layer+2]\n",
    "\n",
    "# Choose model\n",
    "inovelty = 1\n",
    "regularize = \"\"\n",
    "regularizer_param = 0.5\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "current_analysis = 'sp_index_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "sp_index_analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "current_analysis = 'klDivergence_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "klDiv_analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "# For picture filename\n",
    "current_analysis = 'klDivergence_sp_index_%i_layer_%i_novelty'%(layer, inovelty)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "\n",
    "n_folds = len(CVO[inovelty])\n",
    "\n",
    "if os.path.exists(sp_index_analysis_file_name) and  os.path.exists(klDiv_analysis_file_name):\n",
    "    [neurons_mat, spIndex] = joblib.load(sp_index_analysis_file_name)\n",
    "    [neurons_mat2, klDivergenceKnown, klDivergenceNovelty] = joblib.load(klDiv_analysis_file_name)\n",
    "  \n",
    "    # Plot results    \n",
    "    fig, m_ax = plt.subplots(figsize=(10,8),nrows=1, ncols=1)\n",
    "    \n",
    "    linewidth = 3.5\n",
    "    \n",
    "    ln1 = m_ax.errorbar(neurons_mat, np.mean(spIndex, axis=0),\n",
    "                        np.var(spIndex, axis=0), color='b', linestyle='dashed', marker='o',\n",
    "                        label='SP Index', markersize=10, linewidth=linewidth)\n",
    "    \n",
    "    m_ax.set_title('First Autoencoder - Novelty Class %i '%(inovelty), fontsize=16, fontweight='bold')\n",
    "    m_ax.set_ylabel(\"SP Index\", fontsize=22, color='b', fontweight='bold')\n",
    "    m_ax.tick_params('y', colors='b')\n",
    "    ax2 = m_ax.twinx()\n",
    "    \n",
    "    ln3 = ax2.errorbar(neurons_mat, np.mean(klDivergenceKnown, axis=0)[:len(neurons_mat)],\n",
    "                       np.var(klDivergenceKnown, axis=0)[:len(neurons_mat)], color='r', linestyle='dashed', marker='v',\n",
    "                       label='KL Known', markersize=10, linewidth=linewidth)\n",
    "    \n",
    "    ln4 = ax2.errorbar(neurons_mat, np.mean(klDivergenceNovelty,axis=0)[:len(neurons_mat)],\n",
    "                        np.var(klDivergenceNovelty, axis=0)[:len(neurons_mat)], color='r', linestyle='solid', marker='^',\n",
    "                        label='KL Novelty', markersize=10, linewidth=linewidth)\n",
    "    \n",
    "    ax2.set_ylabel('KL Divergence', fontsize=22, color='r', fontweight='bold')\n",
    "    ax2.tick_params('y', colors='r')\n",
    "    ax2.set_yticks(range(0,270,45))\n",
    "    ax2.set_ylim([0,270])\n",
    "    ax2.grid()\n",
    "\n",
    "    m_ax.set_ylim([0.8,1.1])\n",
    "    m_ax.set_yticks(np.arange(0.8,1.05,0.05))\n",
    "    m_ax.set_xlabel('Neurons', fontsize=22, fontweight='bold')\n",
    "    m_ax.grid()\n",
    "\n",
    "    lns = [ln1, ln3, ln4]\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    plt.legend(lns, labs, ncol=2, loc='upper center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "    #Save the figure\n",
    "    file_name = pict_results_path+'/'+current_analysis+'_%i_novelty_%i_layer_%s_neurons_%i_fold_'%(inovelty,\n",
    "                                                                                                   layer,\n",
    "                                                                                                   neurons_str,\n",
    "                                                                                                   ifold)+trn_params.get_params_str()+'.pdf'\n",
    "    plt.savefig(file_name)\n",
    "else: \n",
    "    if not os.path.exists(klDiv_analysis_file_name):\n",
    "        print \"[-] KL Divergence analysis not done.\"\n",
    "    if not os.path.exists(sp_index_analysis_file_name):\n",
    "        print \"[-] SP Index analysis not done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Variação de Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# novelty detection for SAE\n",
    "# thr. sweep\n",
    "\n",
    "# Choose layer\n",
    "layer = 2\n",
    "\n",
    "# Choose neurons topology for SAE\n",
    "hidden_neurons = range(400,0,-50) + [2]\n",
    "\n",
    "neurons_str = SAE[inovelty].getNeuronsString(all_data, hidden_neurons=hidden_neurons[:layer])\n",
    "    \n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "current_analysis = 'figures_of_merit'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_novelty_detection_%s_neurons_thr_sweep.jbl'%(results_path,analysis_str,analysis_name, neurons_str)\n",
    "\n",
    "if os.path.exists(analysis_file_name):\n",
    "        os.remove(analysis_file_name)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "    \n",
    "    thr_mat = np.round(np.arange(-0.0,1.1,0.05),3)\n",
    "    thr_mat[thr_mat>-0.1] = abs(thr_mat[thr_mat>-0.1])\n",
    "\n",
    "    class_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    novelty_eff_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "    known_acc_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "    known_sp_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "    known_trig_mat = np.zeros([n_folds,len(thr_mat)])\n",
    "    \n",
    "    class_eff = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    novelty_eff = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    known_acc = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    known_sp = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    known_trig = np.zeros([len(np.unique(all_trgt))], dtype=object)\n",
    "    \n",
    "    def getFiguresMetrics(inovelty):\n",
    "        n_folds = len(CVO[inovelty])  \n",
    "        print 'Novelty class: %01.0f - topology: %s'%(inovelty, neurons_str)\n",
    "        for ifold in range(n_folds):\n",
    "            classifier = SAE[inovelty].loadClassifier(data  = trn_data[inovelty],\n",
    "                                                      trgt  = trn_trgt[inovelty], \n",
    "                                                      hidden_neurons = hidden_neurons[:layer],\n",
    "                                                      layer = layer,\n",
    "                                                      ifold = ifold)\n",
    "            \n",
    "            train_id, test_id = CVO[inovelty][ifold]\n",
    "            # normalize known classes\n",
    "            if trn_params.params['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(trn_data[inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(trn_data[inovelty][train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(trn_data[inovelty][train_id,:])\n",
    "\n",
    "            known_data = scaler.transform(trn_data[inovelty][test_id,:])\n",
    "#             known_data = trn_data[inovelty][test_id,:]\n",
    "            known_trgt = trn_trgt[inovelty][test_id]\n",
    "        \n",
    "            novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "#             novelty_data = all_data[all_trgt==inovelty]\n",
    "        \n",
    "            output = classifier.predict(known_data)\n",
    "            novelty_output = classifier.predict(novelty_data)\n",
    "\n",
    "            for ithr,thr_value in enumerate(thr_mat): \n",
    "                buff = np.zeros([len(np.unique(all_trgt))-1])\n",
    "                for iclass, class_id in enumerate(np.unique(all_trgt)):\n",
    "                    if iclass == inovelty:\n",
    "                        continue\n",
    "                    output_of_class_events = output[known_trgt==iclass-(iclass>inovelty),:]\n",
    "                    correct_class_output = np.argmax(output_of_class_events,axis=1)==iclass-(iclass>inovelty)\n",
    "                    output_above_thr = output_of_class_events[correct_class_output,iclass-(iclass>inovelty)]>thr_value\n",
    "                    class_eff_mat[ifold, iclass, ithr] = float(sum(output_above_thr))/float(len(output_of_class_events))\n",
    "                    buff[iclass-(iclass>inovelty)] = class_eff_mat[ifold, iclass, ithr]\n",
    "                novelty_eff_mat[ifold, ithr] = float(sum(1-(novelty_output>thr_value).any(axis=1)))/float(len(novelty_output))\n",
    "                known_acc_mat[ifold, ithr] = np.mean(buff,axis=0)\n",
    "                known_sp_mat[ifold, ithr]= (np.sqrt(np.mean(buff,axis=0)\n",
    "                                                              *np.power(np.prod(buff),1./float(len(buff)))))\n",
    "                known_trig_mat[ifold, ithr]=float(sum(np.max(output,axis=1)>thr_value))/float(len(output))\n",
    "\n",
    "        return inovelty, class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, known_trig_mat\n",
    "    \n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    results = p.map(getFiguresMetrics, class_labels.keys())\n",
    "\n",
    "    p.close()\n",
    "    p.join()         \n",
    "    \n",
    "    for inovelty in class_labels.keys():\n",
    "        if inovelty == results[inovelty][0]:\n",
    "            class_eff[inovelty]   = results[inovelty][1]\n",
    "            novelty_eff[inovelty] = results[inovelty][2]\n",
    "            known_acc[inovelty]   = results[inovelty][3]\n",
    "            known_sp[inovelty]    = results[inovelty][4]\n",
    "            known_trig[inovelty]  = results[inovelty][5]\n",
    "        \n",
    "    joblib.dump([class_eff, novelty_eff, known_acc, known_sp, known_trig, thr_mat],\n",
    "                analysis_file_name,compress=9)\n",
    "else:\n",
    "    print 'File exists'\n",
    "    [class_eff, novelty_eff, known_acc, known_sp, known_trig, thr_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.subplots(figsize=(20,15))\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "for inovelty, novelty_class in enumerate(np.unique(all_trgt)):\n",
    "    ax = plt.subplot(2,2,inovelty+1)\n",
    "    for iclass, m_class in enumerate(np.unique(all_trgt)):\n",
    "        if novelty_class == m_class:\n",
    "            ax.errorbar(thr_mat,np.mean(novelty_eff[inovelty], axis=0),\n",
    "                        np.std(novelty_eff[inovelty], axis=0),fmt='o-',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Novel Det.')\n",
    "            ax.errorbar(thr_mat,np.mean(known_acc[inovelty], axis=0),\n",
    "                        np.std(known_acc[inovelty], axis=0),fmt='o--',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Known Acc.')\n",
    "            ax.errorbar(thr_mat,np.mean(known_sp[inovelty], axis=0),\n",
    "                        np.std(known_sp[inovelty], axis=0),fmt='o:',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Known SP')\n",
    "#             ax.errorbar(thr_mat,np.mean(known_trig[inovelty], axis=0),\n",
    "#                         np.std(known_trig[inovelty], axis=0),fmt='o-.',\n",
    "#                         color='k',alpha=0.7,linewidth=2.5,\n",
    "#                         label='Known Trig.')\n",
    "        else:\n",
    "            ax.errorbar(thr_mat,np.mean(class_eff[inovelty][:, iclass, :], axis=0),\n",
    "                        np.std(class_eff[inovelty][:, iclass, :], axis=0),fmt='o-',\n",
    "                        color=m_colors[int(m_class)],alpha=0.7,linewidth=2.5,\n",
    "                       label='C%i Eff.'%(int(iclass)))\n",
    "    ax.set_xticks(thr_mat)\n",
    "    ax.set_xticklabels(thr_mat,rotation=45, fontsize=18)\n",
    "    ax.set_title('Class %i as the novelty'%inovelty,fontsize=18,weight='bold')\n",
    "    ax.set_xlim([np.min(thr_mat), np.max(thr_mat)])\n",
    "    \n",
    "    ax.set_ylim([0.0, 1.3])\n",
    "    y_ticks = np.arange(0.0,1.3,0.1)\n",
    "    ax.set_yticks(np.round(y_ticks,2))\n",
    "    ax.set_yticklabels(100*np.round(y_ticks,2)[np.round(y_ticks,2)<=1.0],fontsize=18)\n",
    "    \n",
    "    ax.grid()\n",
    "    \n",
    "    if inovelty > 1:\n",
    "        ax.set_xlabel('Threshold',fontsize=18,weight='bold')\n",
    "    if inovelty == 0 or inovelty == 2:\n",
    "        ax.set_ylabel('Figures-of-Merit (%)',fontsize=18,weight='bold')\n",
    "        \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # sort both labels and handles by labels\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "    ax.legend(handles, labels, ncol=3, loc='upper center')\n",
    "    \n",
    "    #Save the figure\n",
    "    file_name = pict_results_path+'/'+current_analysis+'_%s_neurons_'%(neurons_str)+trn_params.get_params_str()+'.png'\n",
    "    plt.savefig(file_name)\n",
    "print trn_params.get_params_str()\n",
    "print '%s neurons'%neurons_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
