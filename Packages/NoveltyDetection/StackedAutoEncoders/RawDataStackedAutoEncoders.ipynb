{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de Classificação para Marinha do Brasil\n",
    "\n",
    "## Autor: Vinícius dos Santos Mello (viniciusdsmello@poli.ufrj.br)\n",
    "\n",
    "Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "Laboratório de Tecnologia Sonar\n",
    "\n",
    "Instituto de Pesquisas da Marinha - IPqM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/vinicius.mello/.virtualenvs/sonarenvAnnecy/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 2.38418579102e-05 seconds\n",
      "Time to read data file: 1.07007789612 seconds\n",
      "Qtd event of 0 is 12939\n",
      "Qtd event of 1 is 29352\n",
      "Qtd event of 2 is 11510\n",
      "Qtd event of 3 is 23760\n",
      "\n",
      "Biggest class is 1 with 29352 events\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (12939, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (29352, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (11510, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (23760, 400)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Functions import TrainParameters as trnparams\n",
    "from Functions import TrainFunctions\n",
    "\n",
    "import multiprocessing \n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "analysis_name = 'StackedAutoEncoder'\n",
    "\n",
    "# Enviroment variables\n",
    "data_path = os.getenv('OUTPUTDATAPATH')\n",
    "results_path = os.getenv('PACKAGE_NAME')\n",
    "\n",
    "# paths to export results\n",
    "base_results_path = '%s/%s'%(results_path,analysis_name)\n",
    "pict_results_path = '%s/pictures_files'%(base_results_path)\n",
    "files_results_path = '%s/output_files'%(base_results_path)\n",
    "\n",
    "# For multiprocessing purpose\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# Read data\n",
    "m_time = time.time()\n",
    "\n",
    "# Database caracteristics\n",
    "database = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "spectrum_bins_left = 400\n",
    "development_flag = False\n",
    "development_events = 400\n",
    "\n",
    "# Check if LofarData has created...\n",
    "if not os.path.exists('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                      (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left)):\n",
    "    print 'No Files in %s/%s\\n'%(data_path,database)\n",
    "else:\n",
    "    #Read lofar data\n",
    "    [data,trgt,class_labels] = joblib.load('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                                           (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left))\n",
    "\n",
    "\n",
    "    m_time = time.time()-m_time\n",
    "    print 'Time to read data file: '+str(m_time)+' seconds'\n",
    "\n",
    "    # correct format\n",
    "    all_data = data\n",
    "    all_trgt = trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))\n",
    "    \n",
    "    # Process data\n",
    "    # unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "    # Same number of events in each class\n",
    "    qtd_events_biggest_class = 0\n",
    "    biggest_class_label = ''\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "            qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "            biggest_class_label = class_label\n",
    "        print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "    print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "    balanced_data = {}\n",
    "    balanced_trgt = {}\n",
    "\n",
    "    from Functions import DataHandler as dh\n",
    "    m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if development_flag:\n",
    "            class_events = all_data[all_trgt==iclass,:]\n",
    "            if len(balanced_data) == 0:\n",
    "                balanced_data = class_events[0:development_events,:]\n",
    "                balanced_trgt = (iclass)*np.ones(development_events)\n",
    "            else:\n",
    "                balanced_data = np.append(balanced_data,\n",
    "                                          class_events[0:development_events,:], \n",
    "                                          axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,(iclass)*np.ones(development_events))\n",
    "        else:\n",
    "            if len(balanced_data) == 0:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                balanced_data = m_datahandler.CreateEventsForClass(\n",
    "                    class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "                balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "            else:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                created_events = (m_datahandler.CreateEventsForClass(all_data[all_trgt==iclass,:],\n",
    "                                                                     qtd_events_biggest_class-\n",
    "                                                                     (len(class_events))))\n",
    "                balanced_data = np.append(balanced_data,created_events,axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,\n",
    "                                          (iclass)*np.ones(created_events.shape[0]),axis=0)\n",
    "        \n",
    "    all_data = balanced_data\n",
    "    all_trgt = balanced_trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 5.01 µs\n",
      "1_inits_mapstd_norm_500_epochs_128_batch_size_tanh_hidden_activation_linear_output_activation\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "# Load train parameters\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "os.remove(trn_params_folder)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.SAENoveltyDetectionTrnParams(n_inits=1,\n",
    "                                                       hidden_activation='tanh', # others tanh, relu, sigmoid, linear \n",
    "                                                       output_activation='linear',\n",
    "                                                       n_epochs=500,\n",
    "                                                       patience=30,\n",
    "                                                       batch_size=128,\n",
    "                                                       verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.SAENoveltyDetectionTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "    \n",
    "# Choose how many fold to be used in Cross Validation\n",
    "n_folds = 2\n",
    "CVO = trnparams.NoveltyDetectionFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\n",
    "print trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento da 1 ª Camada - Variação de Neurônios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation\n",
      "Novelty class: 0\n",
      "\n",
      "Novelty class: 1\n",
      "\n",
      "Novelty class: 2\n",
      "\n",
      "Novelty class: 3\n",
      "It took 5.950 seconds to perform the training\n"
     ]
    }
   ],
   "source": [
    "# Train example\n",
    "\n",
    "# Choose neurons topology\n",
    "max_n_neurons = 475\n",
    "min_n_neurons = 0\n",
    "neurons_step = 75\n",
    "\n",
    "verbose = False\n",
    "\n",
    "# Create neurons vector to be used in multiprocessing.Pool()\n",
    "neurons_mat = [1,100,200,250,300,350,400,450]  #range(min_n_neurons, max_n_neurons, neurons_step)\n",
    "print trn_params.get_params_str()\n",
    "start_time = time.time()\n",
    "for inovelty, novelty_class in enumerate(np.unique(trgt)):\n",
    "    trn_data = all_data[all_trgt!=novelty_class]\n",
    "    trn_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "\n",
    "    trn_trgt[trn_trgt>novelty_class] = trn_trgt[trn_trgt>novelty_class]-1\n",
    "    \n",
    "    if inovelty != 0:\n",
    "        print ''\n",
    "    print 'Novelty class: %i'%inovelty\n",
    "    \n",
    "    def trainNeuron(ineuron):\n",
    "        n_folds = len(CVO[inovelty])\n",
    "        for ifold in range(n_folds):\n",
    "            #print 'Neuron value: %i - fold %i'%(ineuron, ifold)\n",
    "            TrainFunctions.SAENoveltyTrainFunction(data=trn_data,\n",
    "                                                       trgt=trn_data,\n",
    "                                                       inovelty=inovelty,\n",
    "                                                       ifold=ifold,\n",
    "                                                       n_folds=n_folds,\n",
    "                                                       n_neurons=ineuron,\n",
    "                                                       trn_params=trn_params,\n",
    "                                                       save_path=results_path,\n",
    "                                                       layer = 1,\n",
    "                                                       verbose=verbose,\n",
    "                                                       dev=development_flag)\n",
    "    \n",
    "    \n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "    \n",
    "    # To train on multiple cores sweeping the number of neurons\n",
    "    results = p.map(trainNeuron, neurons_mat)\n",
    "            \n",
    "    p.close()\n",
    "    p.join()        \n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento da 1ª Camada - Número de Neurônios definido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_inits_mapstd_norm_500_epochs_128_batch_size_tanh_hidden_activation_linear_output_activation\n",
      "Novelty class: 0\n",
      "Neuron: 400 - Fold 1 of 2 Folds -  Init 1 of 1 Inits\n",
      "Neuron: 400 - Fold 2 of 2 Folds -  Init 1 of 1 Inits\n"
     ]
    }
   ],
   "source": [
    "# Choose neurons topology\n",
    "ineuron = 400\n",
    "\n",
    "verbose = False\n",
    "\n",
    "print trn_params.get_params_str()\n",
    "\n",
    "start_time = time.time()\n",
    "for inovelty, novelty_class in enumerate(np.unique(trgt)):\n",
    "    trn_data = all_data[all_trgt!=novelty_class]\n",
    "    trn_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "\n",
    "    trn_trgt[trn_trgt>novelty_class] = trn_trgt[trn_trgt>novelty_class]-1\n",
    "    \n",
    "    if inovelty != 0:\n",
    "        print ''\n",
    "    print 'Novelty class: %i'%inovelty\n",
    "    \n",
    "    # Array with folds to be trained in parallel\n",
    "    folds = range(len(CVO[inovelty]))\n",
    "    \n",
    "    def trainFold(ifold):\n",
    "        n_folds = len(CVO[inovelty])\n",
    "        #print 'Neuron value: %i - fold %i'%(ineuron, ifold)\n",
    "        TrainFunctions.SAENoveltyTrainFunction(data=trn_data,\n",
    "                                                   trgt=trn_data,\n",
    "                                                   inovelty=inovelty,\n",
    "                                                   ifold=ifold,\n",
    "                                                   n_folds=n_folds,\n",
    "                                                   n_neurons=ineuron,\n",
    "                                                   trn_params=trn_params,\n",
    "                                                   save_path=results_path,\n",
    "                                                   layer = 1, # Choose the layer to be trained\n",
    "                                                   verbose=verbose,\n",
    "                                                   dev=development_flag)\n",
    "    \n",
    "    \n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "    \n",
    "    # To train on multiple cores sweeping the number of neurons\n",
    "    results = p.map(trainFold, folds)\n",
    "            \n",
    "    p.close()\n",
    "    p.join()        \n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reconstruction of Known Classes vs Reconstruction of Novelty - novelty detection for neural network\n",
    "%matplotlib inline \n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "verbose = False\n",
    "\n",
    "#os.remove(analysis_file_name)\n",
    "\n",
    "ineuron = 400\n",
    "\n",
    "# Choose class to plot\n",
    "iclass = 0\n",
    "# Choose model\n",
    "inovelty = 0\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "# Check if the analysis has already been performed\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NNNoveltyDetectionTrnParams(n_inits=2,\n",
    "                                                       hidden_activation='tanh',\n",
    "                                                       output_activation='linear',\n",
    "                                                       n_epochs=500,\n",
    "                                                       patience=30,\n",
    "                                                       batch_size=256,\n",
    "                                                       verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NNNoveltyDetectionTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "n_folds = 2\n",
    "CVO = trnparams.NoveltyDetectionFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\n",
    "\n",
    "novelty_class = float(inovelty)\n",
    "\n",
    "models = {}\n",
    "outputs = {}\n",
    "mean = {}\n",
    "indexes = {}\n",
    "\n",
    "n_folds = len(CVO[inovelty])\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    train_id, test_id = CVO[inovelty][ifold]\n",
    "    \n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data)\n",
    "   \n",
    "    trgt_data = all_trgt\n",
    "    trn_data = norm_data[trgt_data==iclass]\n",
    "     \n",
    "    if ifold == 0:\n",
    "        diffSquared = np.zeros([len(CVO),trn_data.shape[0],trn_data.shape[1]])\n",
    "    \n",
    "    print 'Novelty class: %01.0f - neuron: %i - fold %i'%(novelty_class, ineuron, ifold)\n",
    "    \n",
    "    neurons_str = str(data.shape[1]) + 'x' + str(ineuron)\n",
    "    model_str = '%s/%s/%s_%i_novelty_%i_folds_%s_%s_neurons'%(results_path,analysis_str,\n",
    "                                                               model_prefix_str,inovelty,\n",
    "                                                               n_folds,params_str,\n",
    "                                                               neurons_str)\n",
    "    \n",
    "    file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    \n",
    "    if verbose: \n",
    "        print file_name\n",
    "    \n",
    "    if not os.path.exists(file_name):\n",
    "        print 'File %s does not exist'%file_name\n",
    "        break\n",
    "    models[ifold] = load_model(file_name)\n",
    "    \n",
    "    outputs[ifold] = models[ifold].predict(trn_data)\n",
    "    diffSquared[ifold] = np.power((trn_data - outputs[ifold]), 2) \n",
    "\n",
    "mean = np.mean(np.mean(diffSquared, axis=0), axis=0)\n",
    "indexes = np.argsort(mean)[::-1]\n",
    "\n",
    "for ifold in range(len(CVO[inovelty])):\n",
    "    train_id, test_id = CVO[inovelty][ifold]\n",
    "    \n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data[test_id,:])\n",
    "   \n",
    "    trgt_data = all_trgt\n",
    "    trn_data = norm_data[trgt_data[test_id]==iclass]\n",
    "    points = trn_data.shape[0]\n",
    "    # Number of dimensions to analyse (even number is better!)\n",
    "    num_dim = 4\n",
    "    fig, m_ax = plt.subplots(figsize=(20,20),nrows=2, ncols=2)\n",
    "    for choose_index in range(num_dim):  \n",
    "        ax = plt.subplot(2,2,choose_index+1)\n",
    "        ax.plot(trn_data[:,indexes[choose_index]][:points], outputs[ifold][:,indexes[choose_index]][:points],\n",
    "                m_colors[iclass]+\".\")\n",
    "        plt.tight_layout()\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "        ax.set_title('Input x Output - Dim %i'%(indexes[choose_index]),fontsize=18, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid()    \n",
    "#         #Save the figure\n",
    "#         file_name = pict_results_path+'/'+current_analysis+'_first_layer_%i_neurons_%i_fold_'%(ineuron,ifold)+trn_params.get_params_str()+'.pdf'\n",
    "#         plt.savefig(file_name)\n",
    "print \"Topology (%s)\"%trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = all_data[all_trgt!=novelty_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento da 2º Camada - Número de Neurônios definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose neurons topology\n",
    "\n",
    "# hidden_neurons = [first_layer_dim, second_layer_dim]\n",
    "hidden_neurons = [400, 250]\n",
    "\n",
    "verbose = False\n",
    "\n",
    "print trn_params.get_params_str()\n",
    "\n",
    "start_time = time.time()\n",
    "for inovelty, novelty_class in enumerate(np.unique(trgt)):\n",
    "    trn_data = all_data[all_trgt!=novelty_class]\n",
    "    trn_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "\n",
    "    trn_trgt[trn_trgt>novelty_class] = trn_trgt[trn_trgt>novelty_class]-1\n",
    "    \n",
    "    if inovelty != 0:\n",
    "        print ''\n",
    "    print 'Novelty class: %i'%inovelty\n",
    "    \n",
    "    # Array with folds to be trained in parallel\n",
    "    folds = range(len(CVO[inovelty]))\n",
    "    \n",
    "    def trainFold(ifold):\n",
    "        n_folds = len(CVO[inovelty])\n",
    "        #print 'Neuron value: %i - fold %i'%(ineuron, ifold)\n",
    "        TrainFunctions.SAENoveltyTrainFunction(data=trn_data,\n",
    "                                                   trgt=trn_data,\n",
    "                                                   inovelty=inovelty,\n",
    "                                                   ifold=ifold,\n",
    "                                                   n_folds=n_folds,\n",
    "                                                   n_neurons=ineuron,\n",
    "                                                   trn_params=trn_params,\n",
    "                                                   save_path=results_path,\n",
    "                                                   layer = 2, # Choose the layer to be trained\n",
    "                                                   hidden_neurons = hidden_neurons,\n",
    "                                                   verbose=verbose,\n",
    "                                                   dev=development_flag)\n",
    "    \n",
    "    \n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "    \n",
    "    # To train on multiple cores sweeping the number of neurons\n",
    "    results = p.map(trainFold, folds)\n",
    "            \n",
    "    p.close()\n",
    "    p.join()        \n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento da 3ª Camada - Número de Neurônios definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose neurons topology\n",
    "\n",
    "# hidden_neurons = [first_layer_dim, second_layer_dim]\n",
    "hidden_neurons = [400, 250, 125]\n",
    "\n",
    "verbose = False\n",
    "\n",
    "print trn_params.get_params_str()\n",
    "\n",
    "start_time = time.time()\n",
    "for inovelty, novelty_class in enumerate(np.unique(trgt)):\n",
    "    trn_data = all_data[all_trgt!=novelty_class]\n",
    "    trn_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "\n",
    "    trn_trgt[trn_trgt>novelty_class] = trn_trgt[trn_trgt>novelty_class]-1\n",
    "    \n",
    "    if inovelty != 0:\n",
    "        print ''\n",
    "    print 'Novelty class: %i'%inovelty\n",
    "    \n",
    "    # Array with folds to be trained in parallel\n",
    "    folds = range(len(CVO[inovelty]))\n",
    "    \n",
    "    def trainFold(ifold):\n",
    "        n_folds = len(CVO[inovelty])\n",
    "        #print 'Neuron value: %i - fold %i'%(ineuron, ifold)\n",
    "        TrainFunctions.SAENoveltyTrainFunction(data=trn_data,\n",
    "                                                   trgt=trn_data,\n",
    "                                                   inovelty=inovelty,\n",
    "                                                   ifold=ifold,\n",
    "                                                   n_folds=n_folds,\n",
    "                                                   n_neurons=ineuron,\n",
    "                                                   trn_params=trn_params,\n",
    "                                                   save_path=results_path,\n",
    "                                                   layer = 3, # Choose the layer to be trained\n",
    "                                                   hidden_neurons = hidden_neurons,\n",
    "                                                   verbose=verbose,\n",
    "                                                   dev=development_flag)\n",
    "    \n",
    "    \n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "    \n",
    "    # To train on multiple cores sweeping the number of neurons\n",
    "    results = p.map(trainFold, folds)\n",
    "            \n",
    "    p.close()\n",
    "    p.join()        \n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
