{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de Classificação para Marinha do Brasil\n",
    "\n",
    "## Autor: Vinícius dos Santos Mello (viniciusdsmello@poli.ufrj.br)\n",
    "\n",
    "Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "Laboratório de Tecnologia Sonar\n",
    "\n",
    "Instituto de Pesquisas da Marinha - IPqM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/vinicius.mello/.virtualenvs/sonarenvAnnecy/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 2.40802764893e-05 seconds\n",
      "Time to read data file: 1.02640604973 seconds\n",
      "Qtd event of 0 is 12939\n",
      "Qtd event of 1 is 29352\n",
      "Qtd event of 2 is 11510\n",
      "Qtd event of 3 is 23760\n",
      "\n",
      "Biggest class is 1 with 29352 events\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (12939, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (29352, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (11510, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (23760, 400)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import matplotlib.pyploy as plt\n",
    "\n",
    "from Functions import TrainParameters as trnparams\n",
    "from Functions import TrainFunctions\n",
    "\n",
    "import multiprocessing \n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "analysis_name = 'StackedAutoEncoder'\n",
    "\n",
    "# Enviroment variables\n",
    "data_path = os.getenv('OUTPUTDATAPATH')\n",
    "results_path = os.getenv('PACKAGE_NAME')\n",
    "\n",
    "# paths to export results\n",
    "base_results_path = '%s/%s'%(results_path,analysis_name)\n",
    "pict_results_path = '%s/pictures_files'%(base_results_path)\n",
    "files_results_path = '%s/output_files'%(base_results_path)\n",
    "\n",
    "# For multiprocessing purpose\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# Read data\n",
    "m_time = time.time()\n",
    "\n",
    "# Database caracteristics\n",
    "database = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "spectrum_bins_left = 400\n",
    "development_flag = False\n",
    "development_events = 400\n",
    "\n",
    "# Check if LofarData has created...\n",
    "if not os.path.exists('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                      (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left)):\n",
    "    print 'No Files in %s/%s\\n'%(data_path,database)\n",
    "else:\n",
    "    #Read lofar data\n",
    "    [data,trgt,class_labels] = joblib.load('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                                           (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left))\n",
    "\n",
    "\n",
    "    m_time = time.time()-m_time\n",
    "    print 'Time to read data file: '+str(m_time)+' seconds'\n",
    "\n",
    "    # correct format\n",
    "    all_data = data\n",
    "    all_trgt = trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))\n",
    "    \n",
    "    # Process data\n",
    "    # unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "    # Same number of events in each class\n",
    "    qtd_events_biggest_class = 0\n",
    "    biggest_class_label = ''\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "            qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "            biggest_class_label = class_label\n",
    "        print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "    print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "    balanced_data = {}\n",
    "    balanced_trgt = {}\n",
    "\n",
    "    from Functions import DataHandler as dh\n",
    "    m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if development_flag:\n",
    "            class_events = all_data[all_trgt==iclass,:]\n",
    "            if len(balanced_data) == 0:\n",
    "                balanced_data = class_events[0:development_events,:]\n",
    "                balanced_trgt = (iclass)*np.ones(development_events)\n",
    "            else:\n",
    "                balanced_data = np.append(balanced_data,\n",
    "                                          class_events[0:development_events,:], \n",
    "                                          axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,(iclass)*np.ones(development_events))\n",
    "        else:\n",
    "            if len(balanced_data) == 0:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                balanced_data = m_datahandler.CreateEventsForClass(\n",
    "                    class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "                balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "            else:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                created_events = (m_datahandler.CreateEventsForClass(all_data[all_trgt==iclass,:],\n",
    "                                                                     qtd_events_biggest_class-\n",
    "                                                                     (len(class_events))))\n",
    "                balanced_data = np.append(balanced_data,created_events,axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,\n",
    "                                          (iclass)*np.ones(created_events.shape[0]),axis=0)\n",
    "        \n",
    "    all_data = balanced_data\n",
    "    all_trgt = balanced_trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "2_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "# Load train parameters\n",
    "\n",
    "analysis_str = 'StackedAutoEncoder'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "#os.remove(trn_params_folder)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.SAENoveltyDetectionTrnParams(n_inits=2,\n",
    "                                                       hidden_activation='tanh', # others tanh, relu, sigmoid, linear \n",
    "                                                       output_activation='linear',\n",
    "                                                       n_epochs=500,\n",
    "                                                       patience=30,\n",
    "                                                       batch_size=256,\n",
    "                                                       verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.SAENoveltyDetectionTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "    \n",
    "# Choose how many fold to be used in Cross Validation\n",
    "n_folds = 2\n",
    "CVO = trnparams.NoveltyDetectionFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\n",
    "print trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento com Processamento Pararalelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_inits_mapstd_norm_500_epochs_256_batch_size_tanh_hidden_activation_linear_output_activation\n",
      "Novelty class: 0\n",
      "\n",
      "Novelty class: 1\n",
      "\n",
      "Novelty class: 2\n",
      "Neuron: 250 - Fold 1 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 100 - Fold 1 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 1 - Fold 1 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 400 - Fold 1 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 300 - Fold 1 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 200 - Fold 1 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 350 - Fold 1 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 450 - Fold 1 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 1 - Fold 1 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 1 - Fold 2 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 1 - Fold 2 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 100 - Fold 1 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 400 - Fold 1 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 200 - Fold 1 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 100 - Fold 2 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 250 - Fold 1 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 450 - Fold 1 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 350 - Fold 1 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 300 - Fold 1 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 100 - Fold 2 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 400 - Fold 2 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 200 - Fold 2 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 350 - Fold 2 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 450 - Fold 2 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 300 - Fold 2 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 250 - Fold 2 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 400 - Fold 2 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 200 - Fold 2 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 250 - Fold 2 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 300 - Fold 2 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 450 - Fold 2 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 350 - Fold 2 of 2 Folds -  Init 2 of 2 Inits\n",
      "\n",
      "Novelty class: 3\n",
      "Neuron: 350 - Fold 2 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 450 - Fold 2 of 2 Folds -  Init 1 of 2 Inits\n",
      "Neuron: 350 - Fold 2 of 2 Folds -  Init 2 of 2 Inits\n",
      "Neuron: 450 - Fold 2 of 2 Folds -  Init 2 of 2 Inits\n",
      "It took 47680.702 seconds to perform the training\n"
     ]
    }
   ],
   "source": [
    "# Train example\n",
    "\n",
    "# Choose neurons topology\n",
    "max_n_neurons = 475\n",
    "min_n_neurons = 0\n",
    "neurons_step = 75\n",
    "\n",
    "verbose = False\n",
    "\n",
    "# Create neurons vector to be used in multiprocessing.Pool()\n",
    "neurons_mat = [1,100,200,250,300,350,400,450]  #range(min_n_neurons, max_n_neurons, neurons_step)\n",
    "print trn_params.get_params_str()\n",
    "start_time = time.time()\n",
    "for inovelty, novelty_class in enumerate(np.unique(trgt)):\n",
    "    trn_data = all_data[all_trgt!=novelty_class]\n",
    "    trn_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "\n",
    "    trn_trgt[trn_trgt>novelty_class] = trn_trgt[trn_trgt>novelty_class]-1\n",
    "    \n",
    "    if inovelty != 0:\n",
    "        print ''\n",
    "    print 'Novelty class: %i'%inovelty\n",
    "    \n",
    "    def trainNeuron(ineuron):\n",
    "        n_folds = len(CVO[inovelty])\n",
    "        for ifold in range(n_folds):\n",
    "            #print 'Neuron value: %i - fold %i'%(ineuron, ifold)\n",
    "            TrainFunctions.SAENoveltyTrainFunction(data=trn_data,\n",
    "                                                       trgt=trn_data,\n",
    "                                                       inovelty=inovelty,\n",
    "                                                       ifold=ifold,\n",
    "                                                       n_folds=n_folds,\n",
    "                                                       n_neurons=ineuron,\n",
    "                                                       trn_params=trn_params,\n",
    "                                                       save_path=results_path,\n",
    "                                                       layer = 1,\n",
    "                                                       verbose=verbose,\n",
    "                                                       dev=development_flag)\n",
    "    \n",
    "    \n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "    \n",
    "    # To train on multiple cores sweeping the number of neurons\n",
    "    results = p.map(trainNeuron, neurons_mat)\n",
    "            \n",
    "    p.close()\n",
    "    p.join()        \n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Check if the analysis has already been performed\\nif not os.path.exists(analysis_file_name):\\n    trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\\n    \\n    if not os.path.exists(trn_params_folder):\\n        trn_params = trnparams.NNNoveltyDetectionTrnParams(n_inits=2,\\n                                                           hidden_activation='tanh',\\n                                                           output_activation='linear',\\n                                                           n_epochs=500,\\n                                                           patience=30,\\n                                                           batch_size=256,\\n                                                           verbose=False)\\n        trn_params.save(trn_params_folder)\\n    else:\\n        trn_params = trnparams.NNNoveltyDetectionTrnParams()\\n        trn_params.load(trn_params_folder)\\n\\n    params_str = trn_params.get_params_str()\\n    n_folds = 2\\n    CVO = trnparams.NoveltyDetectionFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\\n    \\n    n_classes = len(np.unique(all_trgt))\\n    len_thr_mat = len(thr_mat)\\n    \\n    class_error_mat = np.zeros([n_folds,n_classes,n_classes,len_thr_mat])\\n    novelty_error_mat = np.zeros([n_folds,n_classes,len_thr_mat])\\n    \\n    for inovelty, novelty_class in enumerate(np.unique(trgt)):\\n        trn_data = all_data[all_trgt!=novelty_class]\\n        trn_trgt = all_trgt[all_trgt!=novelty_class]\\n        \\n        trn_trgt[trn_trgt>novelty_class] = trn_trgt[trn_trgt>novelty_class]-1\\n        \\n        for ifold in range(len(CVO[inovelty])):\\n            print 'Novelty class: %01.0f - neuron: %i - fold %i'%(novelty_class, ineuron, ifold)\\n            [classifier,trn_desc] = TrainFunctions.SAENoveltyTrainFunction(data=trn_data,\\n                                                                          trgt=trn_trgt,\\n                                                                          inovelty=inovelty,\\n                                                                          ifold=ifold,\\n                                                                          n_folds=len(CVO[inovelty]),\\n                                                                          n_neurons=ineuron, \\n                                                                          trn_params=trn_params,\\n                                                                          save_path=results_path,\\n                                                                          dev=development_flag)\\n            output = classifier.predict(trn_data)\\n            novelty_output = classifier.predict(all_data[all_trgt==novelty_class])\\n            \\n            for ithr,thr_value in enumerate(thr_mat): \\n                buff = np.zeros([len(np.unique(trgt))-1])\\n                for iclass, class_id in enumerate(np.unique(trgt)):\\n                    if iclass == inovelty:\\n                        continue\\n                    class_eff_mat[ifold, inovelty, iclass, ithr] = float(sum(output[np.argmax(output,axis=1)==iclass-(iclass>inovelty),iclass-(iclass>inovelty)]>thr_value))/float(len(output))\\n                    buff[iclass-(iclass>inovelty)] = float(sum(output[np.argmax(output,axis=1)==iclass-(iclass>inovelty),iclass-(iclass>inovelty)]>thr_value))/float(len(output))\\n                novelty_eff_mat[ifold, inovelty, ithr] = float(sum(1-(novelty_output>thr_value).any(axis=1)))/float(len(novelty_output))\\n                known_acc_mat[ifold, inovelty, ithr] = np.mean(buff,axis=0)\\n                known_sp_mat[ifold, inovelty, ithr]= (np.sqrt(np.mean(buff,axis=0)\\n                                                              *np.power(np.prod(buff),1./float(len(buff)))))\\n    joblib.dump([class_error_mat, novelty_error_mat, thr_mat],\\n                analysis_file_name,compress=9)\\nelse:\\n    [class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, thr_mat] = joblib.load(analysis_file_name) \\n\\n# plot analysis\\nimport matplotlib.pyplot as plt\\n%matplotlib inline  \\n\\nfig = plt.subplots(figsize=(20,15))\\n\\nplt.rcParams['xtick.labelsize'] = 15\\nplt.rcParams['ytick.labelsize'] = 15\\nplt.rcParams['legend.numpoints'] = 1\\nplt.rc('legend',**{'fontsize':15})\\nplt.rc('font', weight='bold')\\n\\nm_colors = ['b', 'r', 'g', 'y']\\n\\nfor inovelty, novelty_class in enumerate(np.unique(all_trgt)):\\n    ax = plt.subplot(2,2,inovelty+1)\\n    for iclass, m_class in enumerate(np.unique(all_trgt)):\\n        if novelty_class == m_class:\\n            #a = 0\\n            ax.errorbar(thr_mat,np.mean(novelty_eff_mat[:,int(novelty_class),:],axis=0),\\n                        np.std(novelty_eff_mat[:,int(novelty_class),:],axis=0),fmt='o-',\\n                        color='k',alpha=0.7,linewidth=2.5,\\n                        label='Novelty. Det.')\\n            ax.errorbar(thr_mat,np.mean(known_acc_mat[:,int(novelty_class),:],axis=0),\\n                        np.std(known_acc_mat[:,int(novelty_class),:],axis=0),fmt='o--',\\n                        color='k',alpha=0.7,linewidth=2.5,\\n                        label='Known Acc.')\\n            ax.errorbar(thr_mat,np.mean(known_sp_mat[:,int(novelty_class),:],axis=0),\\n                        np.std(known_sp_mat[:,int(novelty_class),:],axis=0),fmt='o:',\\n                        color='k',alpha=0.7,linewidth=2.5,\\n                        label='Known SP')\\n        else:\\n            ax.errorbar(thr_mat,np.mean(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),\\n                        np.std(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),fmt='o-',\\n                        color=m_colors[int(m_class)],alpha=0.7,linewidth=2.5,\\n                       label='C%i Eff.'%(int(m_class)+1))\\n    ax.set_xticks(thr_mat)\\n    ax.set_xticklabels(thr_mat,rotation=45)\\n    ax.set_title('Eff per Known Class',fontsize=18,weight='bold')\\n    ax.set_xlim([np.min(thr_mat), np.max(thr_mat)])\\n    ax.set_ylim([-0.5,1.2])\\n    ax.grid()\\n    \\n    if inovelty > 1:\\n        ax.set_xlabel('Threshold',fontsize=18,weight='bold')\\n    if inovelty == 0 or inovelty == 2:\\n        ax.set_ylabel('Eff.',fontsize=18,weight='bold')\\n        \\n    handles, labels = ax.get_legend_handles_labels()\\n    # sort both labels and handles by labels\\n    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\\n    ax.legend(handles, labels, ncol=1)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstruction of Known Classes vs Reconstruction of Novelty - novelty detection for neural network\n",
    "# threshold sweep\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'StackedAutoEncoders'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_novelty_detection_thr_sweep.jbl'%(results_path,analysis_str,analysis_name)\n",
    "\n",
    "#os.remove(analysis_file_name)\n",
    "\n",
    "ineuron = 400\n",
    "\n",
    "thr_mat = np.arange(0.0,1.05,0.05)\n",
    "\"\"\"\n",
    "# Check if the analysis has already been performed\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "    \n",
    "    if not os.path.exists(trn_params_folder):\n",
    "        trn_params = trnparams.NNNoveltyDetectionTrnParams(n_inits=2,\n",
    "                                                           hidden_activation='tanh',\n",
    "                                                           output_activation='linear',\n",
    "                                                           n_epochs=500,\n",
    "                                                           patience=30,\n",
    "                                                           batch_size=256,\n",
    "                                                           verbose=False)\n",
    "        trn_params.save(trn_params_folder)\n",
    "    else:\n",
    "        trn_params = trnparams.NNNoveltyDetectionTrnParams()\n",
    "        trn_params.load(trn_params_folder)\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "    n_folds = 2\n",
    "    CVO = trnparams.NoveltyDetectionFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\n",
    "    \n",
    "    n_classes = len(np.unique(all_trgt))\n",
    "    len_thr_mat = len(thr_mat)\n",
    "    \n",
    "    class_error_mat = np.zeros([n_folds,n_classes,n_classes,len_thr_mat])\n",
    "    novelty_error_mat = np.zeros([n_folds,n_classes,len_thr_mat])\n",
    "    \n",
    "    for inovelty, novelty_class in enumerate(np.unique(trgt)):\n",
    "        trn_data = all_data[all_trgt!=novelty_class]\n",
    "        trn_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "        \n",
    "        trn_trgt[trn_trgt>novelty_class] = trn_trgt[trn_trgt>novelty_class]-1\n",
    "        \n",
    "        for ifold in range(len(CVO[inovelty])):\n",
    "            print 'Novelty class: %01.0f - neuron: %i - fold %i'%(novelty_class, ineuron, ifold)\n",
    "            [classifier,trn_desc] = TrainFunctions.SAENoveltyTrainFunction(data=trn_data,\n",
    "                                                                          trgt=trn_trgt,\n",
    "                                                                          inovelty=inovelty,\n",
    "                                                                          ifold=ifold,\n",
    "                                                                          n_folds=len(CVO[inovelty]),\n",
    "                                                                          n_neurons=ineuron, \n",
    "                                                                          trn_params=trn_params,\n",
    "                                                                          save_path=results_path,\n",
    "                                                                          dev=development_flag)\n",
    "            output = classifier.predict(trn_data)\n",
    "            novelty_output = classifier.predict(all_data[all_trgt==novelty_class])\n",
    "            \n",
    "            for ithr,thr_value in enumerate(thr_mat): \n",
    "                buff = np.zeros([len(np.unique(trgt))-1])\n",
    "                for iclass, class_id in enumerate(np.unique(trgt)):\n",
    "                    if iclass == inovelty:\n",
    "                        continue\n",
    "                    class_eff_mat[ifold, inovelty, iclass, ithr] = float(sum(output[np.argmax(output,axis=1)==iclass-(iclass>inovelty),iclass-(iclass>inovelty)]>thr_value))/float(len(output))\n",
    "                    buff[iclass-(iclass>inovelty)] = float(sum(output[np.argmax(output,axis=1)==iclass-(iclass>inovelty),iclass-(iclass>inovelty)]>thr_value))/float(len(output))\n",
    "                novelty_eff_mat[ifold, inovelty, ithr] = float(sum(1-(novelty_output>thr_value).any(axis=1)))/float(len(novelty_output))\n",
    "                known_acc_mat[ifold, inovelty, ithr] = np.mean(buff,axis=0)\n",
    "                known_sp_mat[ifold, inovelty, ithr]= (np.sqrt(np.mean(buff,axis=0)\n",
    "                                                              *np.power(np.prod(buff),1./float(len(buff)))))\n",
    "    joblib.dump([class_error_mat, novelty_error_mat, thr_mat],\n",
    "                analysis_file_name,compress=9)\n",
    "else:\n",
    "    [class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, thr_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.subplots(figsize=(20,15))\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "for inovelty, novelty_class in enumerate(np.unique(all_trgt)):\n",
    "    ax = plt.subplot(2,2,inovelty+1)\n",
    "    for iclass, m_class in enumerate(np.unique(all_trgt)):\n",
    "        if novelty_class == m_class:\n",
    "            #a = 0\n",
    "            ax.errorbar(thr_mat,np.mean(novelty_eff_mat[:,int(novelty_class),:],axis=0),\n",
    "                        np.std(novelty_eff_mat[:,int(novelty_class),:],axis=0),fmt='o-',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Novelty. Det.')\n",
    "            ax.errorbar(thr_mat,np.mean(known_acc_mat[:,int(novelty_class),:],axis=0),\n",
    "                        np.std(known_acc_mat[:,int(novelty_class),:],axis=0),fmt='o--',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Known Acc.')\n",
    "            ax.errorbar(thr_mat,np.mean(known_sp_mat[:,int(novelty_class),:],axis=0),\n",
    "                        np.std(known_sp_mat[:,int(novelty_class),:],axis=0),fmt='o:',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Known SP')\n",
    "        else:\n",
    "            ax.errorbar(thr_mat,np.mean(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),\n",
    "                        np.std(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),fmt='o-',\n",
    "                        color=m_colors[int(m_class)],alpha=0.7,linewidth=2.5,\n",
    "                       label='C%i Eff.'%(int(m_class)+1))\n",
    "    ax.set_xticks(thr_mat)\n",
    "    ax.set_xticklabels(thr_mat,rotation=45)\n",
    "    ax.set_title('Eff per Known Class',fontsize=18,weight='bold')\n",
    "    ax.set_xlim([np.min(thr_mat), np.max(thr_mat)])\n",
    "    ax.set_ylim([-0.5,1.2])\n",
    "    ax.grid()\n",
    "    \n",
    "    if inovelty > 1:\n",
    "        ax.set_xlabel('Threshold',fontsize=18,weight='bold')\n",
    "    if inovelty == 0 or inovelty == 2:\n",
    "        ax.set_ylabel('Eff.',fontsize=18,weight='bold')\n",
    "        \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # sort both labels and handles by labels\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "    ax.legend(handles, labels, ncol=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
