{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de Classificação para Marinha do Brasil\n",
    "\n",
    "## Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "\n",
    "Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "Laboratório de Tecnologia Sonar\n",
    "\n",
    "Instituto de Pesquisas da Marinha - IPqM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas e leitura dos dados\n",
    "As bibliotecas necessárias para a inclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 4.73550 seconds\n",
      "/Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/LofarData_4classes_old_1024_fft_pts_3_decimation_rate.jbl doesnt exist...please create it\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/LofarData_4classes_old_1024_fft_pts_3_decimation_rate.jbl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3fe07407bd11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#Read lofar data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m [data,class_labels] = joblib.load(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n\u001b[0;32m---> 38\u001b[0;31m             subfolder,n_pts_fft,decimation_rate))\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mm_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mm_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natmourajr/.virtualenvs/sonarenv/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0mwas\u001b[0m \u001b[0msaved\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmemmaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \"\"\"\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0;31m# We are careful to open the file handle early and keep it open to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# avoid race-conditions on renames. That said, if data are stored in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/LofarData_4classes_old_1024_fft_pts_3_decimation_rate.jbl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: %1.5f seconds'%(m_time-init_time)\n",
    "\n",
    "outputpath = os.environ['OUTPUTDATAPATH']\n",
    "main_analysis_path = os.environ['SONAR_WORKSPACE']\n",
    "log_analysis_path = os.environ['PACKAGE_OUTPUT']\n",
    "result_analysis_path = os.environ['PACKAGE_OUTPUT']+'/PCASingleClassSVM'\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "\n",
    "subfolder = '4classes_old'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "\n",
    "if(not os.path.exists(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))):\n",
    "    print outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "        subfolder,n_pts_fft,decimation_rate)+' doesnt exist...please create it'\n",
    "    exit()\n",
    "    \n",
    "#Read lofar data\n",
    "[data,class_labels] = joblib.load(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))\n",
    "\n",
    "m_time = time.time()-m_time\n",
    "print 'Time to read data file: %1.5f seconds'%m_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento dos dados\n",
    "Os dados encontram-se no formato do matlab, para isso precisam ser processados para o formato de python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process data...\n",
    "# create a full data vector\n",
    "all_data = {};\n",
    "all_trgt = {};\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    for irun in range(len(data[iclass])):\n",
    "        if len(all_data) == 0:\n",
    "            all_data = data[iclass][irun]['Signal']\n",
    "            all_trgt = (iclass)*np.ones(data[iclass][irun]['Signal'].shape[1])\n",
    "        else:\n",
    "            all_data = np.append(all_data,data[iclass][irun]['Signal'],axis=1)\n",
    "            all_trgt = np.append(all_trgt,(iclass)*np.ones(data[iclass][irun]\n",
    "                                                           ['Signal'].shape[1]),axis=0)\n",
    "            \n",
    "all_data = all_data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanceamento de Classes\n",
    "Os dados encontram-se desbalanceados. Com isso, os classificadores podem se especializar em uma classe (gerando mais SVs para a mesma) e não se especializar em outras\n",
    "\n",
    "Acessados em 21/12/2016\n",
    "\n",
    "https://svds.com/learning-imbalanced-classes/\n",
    "\n",
    "http://www.cs.utah.edu/~piyush/teaching/ImbalancedLearning.pdf\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n",
    "\n",
    "Para solucionar isso, a primeira solução é \"criar\" dados das classes com menos eventos de maneira aleatória. Outras soluções podem ser propostas posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process data\n",
    "# unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "# Same number of events in each class\n",
    "qtd_events_biggest_class = 0\n",
    "biggest_class_label = ''\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "        qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "        biggest_class_label = class_label\n",
    "    print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "\n",
    "balanced_data = {}\n",
    "balanced_trgt = {}\n",
    "\n",
    "from Functions import DataHandler as dh\n",
    "m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if len(balanced_data) == 0:\n",
    "        class_events = all_data[all_trgt==iclass,:]\n",
    "        balanced_data = m_datahandler.CreateEventsForClass(\n",
    "            class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "        balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "    else:\n",
    "        balanced_data = np.append(balanced_data,\n",
    "                                  (m_datahandler.CreateEventsForClass(\n",
    "                    all_data[all_trgt==iclass,:],\n",
    "                    qtd_events_biggest_class-sum(all_trgt==iclass))),\n",
    "                                  axis=0)\n",
    "        balanced_trgt = np.append(balanced_trgt,\n",
    "                                  (iclass)*np.ones(qtd_events_biggest_class),axis=0)\n",
    "        \n",
    "all_data = balanced_data\n",
    "all_trgt = balanced_trgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições do treinamento\n",
    "Nessa célula temos os parâmetros do treinamento a ser realizado. No log, deve ficar armazenada a data do treinamento para a reconstrução dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(\"NoveltyDetection\",'PCASingleClassSVM')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_pcas = 270\n",
    "norm = 'mapstd'\n",
    "#nu_values = np.array([0.7, 0.8, 0.9])\n",
    "#nu_values = np.array([0.001, 0.1, 0.2])\n",
    "nu_values = np.array([0.001, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.999])\n",
    "gamma_value = 0.0001\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_pcas'] = n_pcas\n",
    "train_info['norm'] = norm\n",
    "train_info['nu_values'] = nu_values\n",
    "train_info['gamma_value'] = gamma_value\n",
    "\n",
    "# divide data in train and test for novelty detection\n",
    "for novelty_class, novelty_label in enumerate(class_labels):\n",
    "    print 'Dividing data in trn and tst for novelty class: %s'%(novelty_label)\n",
    "    CVO = cross_validation.StratifiedKFold(all_trgt[all_trgt!=novelty_class], n_folds)\n",
    "    CVO = list(CVO)\n",
    "    train_info['CVO_novelty_%s'%(novelty_label)] = CVO\n",
    "\n",
    "train_info['preprocessing_done'] = False\n",
    "train_info['preprocessing_analysis'] = False\n",
    "train_info['train_done'] = False\n",
    "train_info['results_done'] = False\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+date+'_train_info.jbl'\n",
    "joblib.dump([train_info],train_info_name,compress=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read log files\n",
    "from Functions import LogFunctions as log\n",
    "mlog = log.LogInformation()\n",
    "log_entries = mlog.RecoverLogEntries(package_name=\"NoveltyDetection\")\n",
    "print log_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Information of Train Info File\n",
    "choose_date = '2017_05_09_00_09_35'\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCASingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'Analysing train performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    print 'PCASingleClassSVM Train Info File'\n",
    "    print 'Date: %s'%(choose_date)\n",
    "    print 'Number of Folds: %i'%(train_info['n_folds'])\n",
    "    print 'Number of Used PCAs: %i'%(train_info['n_pcas'])\n",
    "    print 'Normalization Method: %s'%(train_info['norm'])\n",
    "    print 'Gamma Value: %1.7f'%(train_info['gamma_value'])\n",
    "    print 'Nu Value(s): '\n",
    "    print train_info['nu_values']\n",
    "    if train_info['preprocessing_done']:\n",
    "        print 'Preprocessing Done: True'\n",
    "    else:\n",
    "        print 'Preprocessing Done: False'\n",
    "    if train_info['preprocessing_analysis']:\n",
    "        print 'Preprocessing Analysis Done: True'\n",
    "    else:\n",
    "        print 'Preprocessing Analysis Done: False'\n",
    "    \n",
    "    if train_info['train_done']:\n",
    "        print 'Train Done: True'\n",
    "    else:\n",
    "        print 'Train Done: False'\n",
    "    if train_info['results_done']:\n",
    "        print 'Extract Results: True'\n",
    "    else:\n",
    "        print 'Extract Results: False'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessamento - PCA\n",
    "Como a dimensionalidade dos dados é alta (400 dimensões), um pré-processamento se faz necessário para reduzir as dimensões das entradas e tornar o modelo menos complexo. Aqui, o pré-processamento utilizado é a PCA (análise de componentes principais) linear.\n",
    "\n",
    "## Arquivos gerados\n",
    "Uma vez que uma das classes seja removida do processo de treinamento (considerada novidade), podemos fazer a extração dos PCAs para as classes restantes. Para isso, devemos definir um conjunto de treinamento e de testes. No conjunto de treinamento, os parâmetros (PCAs) serão extraídos, enquanto no conjunto de teste, o poder de generalização do processo de extração será testado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA extraction\n",
    "%time\n",
    "\n",
    "pcas = {}\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCASingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA extraction performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if train_info['preprocessing_done']:\n",
    "        print 'Preprocessing done, just analyse it'\n",
    "        continue\n",
    "\n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        pcas[novelty_class] = {}\n",
    "        print 'Extracting PCA for novelty %s'%(novelty_label)\n",
    "        \n",
    "        known_data = all_data[all_trgt!=novelty_class,:]\n",
    "        \n",
    "        for ifold in range(len(train_info['CVO_novelty_%s'%(novelty_label)])):\n",
    "            # split data in trn set, tst set\n",
    "            train_id, test_id = train_info['CVO_novelty_%s'%(novelty_label)][ifold]\n",
    "\n",
    "            # normalize data based in train set\n",
    "            if train_info['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "                \n",
    "            norm_known_data = scaler.transform(known_data)\n",
    "            \n",
    "            pca = PCA(n_components=known_data.shape[1])\n",
    "            \n",
    "            pca.fit(norm_known_data[train_id,:])\n",
    "            pcas[novelty_class][ifold] = pca\n",
    "        \n",
    "    # saving file\n",
    "    pca_file_path = result_analysis_path+'/result_files'+'/'+choose_date+'_pca_file.jbl'\n",
    "\n",
    "    if pcas != {}:\n",
    "        joblib.dump([pcas],pca_file_path,compress=9)\n",
    "\n",
    "    train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "    train_info['preprocessing_done'] = True\n",
    "    joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PCA energy analysis - energy curve\n",
    "%time\n",
    "\n",
    "pcas_energy = {}\n",
    "novelty_error = {}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCASingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if not train_info['preprocessing_done']:\n",
    "        print 'Preprocessing is not done, just do it'\n",
    "        continue\n",
    "    \n",
    "    [pcas] = joblib.load(result_analysis_path+'/result_files'+'/'+choose_date+'_pca_file.jbl')\n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels): \n",
    "        print 'Novelty: %s'%(novelty_label)\n",
    "        pcas_energy[novelty_class] = np.zeros([train_info['n_folds'],all_data.shape[1]])\n",
    "        novelty_error[novelty_class] = np.zeros([train_info['n_folds'],all_data.shape[1]])\n",
    "        \n",
    "        novelty_data = all_data[all_trgt==novelty_class,:]\n",
    "        known_data = all_data[all_trgt!=novelty_class,:]\n",
    "        \n",
    "        for ifold in range(len(train_info['CVO_novelty_%s'%(novelty_label)])):\n",
    "            pcas_energy[novelty_class][ifold,:] = (pcas[novelty_class][ifold].explained_variance_/\n",
    "                                                   np.sum(pcas[novelty_class][ifold].explained_variance_))\n",
    "            \n",
    "            # split data in trn set, tst set\n",
    "            train_id, test_id = train_info['CVO_novelty_%s'%(novelty_label)][ifold]\n",
    "\n",
    "            # normalize data based in train set\n",
    "            if train_info['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "                \n",
    "            norm_known_data = scaler.transform(known_data)\n",
    "            \n",
    "            for ipca in range(all_data.shape[1]):\n",
    "                pca = PCA(n_components=ipca)\n",
    "                pca.fit(norm_known_data[train_id,:])\n",
    "                tran_novelty_data = pca.transform(novelty_data)\n",
    "                reco_novelty_data = pca.inverse_transform(tran_novelty_data)\n",
    "                novelty_error[novelty_class][ifold,ipca] = np.mean((novelty_data-reco_novelty_data)**2)\n",
    "                \n",
    "            \n",
    "    \n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    plt.rcParams['xtick.labelsize'] = 13\n",
    "    plt.rcParams['ytick.labelsize'] = 13\n",
    "    \n",
    "    n_pcas = all_data.shape[1]\n",
    "    bins = np.linspace(0,n_pcas-1,n_pcas)\n",
    "    \n",
    "    n_columns = 1\n",
    "    icolumn = 0\n",
    "    \n",
    "    n_rows = 4\n",
    "    irow = 0\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=n_rows, ncols=n_columns,figsize=(10,20))\n",
    "    \n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        #ax[irow,icolumn].plot(bins+0.5,100.*np.cumsum(np.mean(pcas_energy[novelty_class].T,1)),'r--')\n",
    "        ax[irow].errorbar(bins+0.5,\n",
    "                                 100.*np.cumsum(np.mean(pcas_energy[novelty_class].T,1)),\n",
    "                                 np.std(pcas_energy[novelty_class].T,1),fmt='-o')\n",
    "        m_leg = []\n",
    "        \n",
    "        m_leg.append('PCA Energy')\n",
    "        \n",
    "        ax[irow].errorbar(bins+0.5,\n",
    "                                 1000.*np.mean(novelty_error[novelty_class].T,axis=1),\n",
    "                                 np.std(novelty_error[novelty_class].T,axis=1),fmt='-o')\n",
    "        m_leg.append('Novelty Error (MSE)')\n",
    "        ax[irow].set_ylim([0,100])\n",
    "        \n",
    "        \n",
    "        m_fontsize = 13\n",
    "        ax[irow].set_title('PCA Extraction for %s novelty'%(novelty_label), \n",
    "                                   fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow].set_ylabel('% of Cum. Energy/100.0*MSE', fontsize= m_fontsize, fontweight='bold')\n",
    "        if novelty_class == 3:\n",
    "            ax[irow].set_xlabel('Number of Components', fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow].grid()\n",
    "        \n",
    "        ax[irow].set_yticks([x for x in range(0,101,5)])\n",
    "        ax[irow].set_xticks([x for x in range(0,401,20)])\n",
    "        ax[irow].set_xticklabels([x for x in range(0,401,20)],rotation=45)\n",
    "        ax[irow].legend(m_leg, loc='upper left',ncol=1,fontsize=m_fontsize-3)\n",
    "        \n",
    "        for ipca in range(0,n_pcas,1):\n",
    "            if np.sum(np.mean(pcas_energy[novelty_class].T,1)[0:ipca]) > 0.9:\n",
    "                break\n",
    "        ax[irow].plot(bins[ipca],100.*np.sum(np.mean(pcas_energy[novelty_class].T,1)[0:ipca]),\n",
    "                              'ro',markersize=16)\n",
    "        \n",
    "                \n",
    "        if icolumn+1 == n_columns:\n",
    "            icolumn = 0\n",
    "            irow = irow+1\n",
    "        else:\n",
    "            icolumn = icolumn+1\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(result_analysis_path+'/picts/'+choose_date+'_pca_cumulative_energy.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_analysis_path+'/picts/'+choose_date+'_pca_cumulative_energy.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PCA energy analysis - amount of energy per pca\n",
    "%time\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCASingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if not train_info['preprocessing_done']:\n",
    "        print 'Preprocessing is not done, just do it'\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    [pcas] = joblib.load(result_analysis_path+'/result_files'+'/'+choose_date+'_pca_file.jbl')\n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels): \n",
    "        pcas_energy[novelty_class] = np.zeros([train_info['n_folds'],all_data.shape[1]])\n",
    "        \n",
    "        for ifold in range(len(train_info['CVO_novelty_%s'%(novelty_label)])):\n",
    "            pcas_energy[novelty_class][ifold,:] = (pcas[novelty_class][ifold].explained_variance_/\n",
    "                                                   np.sum(pcas[novelty_class][ifold].explained_variance_))\n",
    "            \n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    \n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    plt.rcParams['xtick.labelsize'] = 18\n",
    "    plt.rcParams['ytick.labelsize'] = 18\n",
    "    \n",
    "    n_pcas = all_data.shape[1]\n",
    "    bins = np.linspace(0,n_pcas-1,n_pcas)\n",
    "    \n",
    "    n_columns = 2\n",
    "    icolumn = 0\n",
    "    \n",
    "    n_rows = 2\n",
    "    irow = 0\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=n_rows, ncols=n_columns,figsize=(25,20))\n",
    "    \n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        \n",
    "        \n",
    "        plot_data = np.mean(pcas_energy[novelty_class].T,1)\n",
    "        err_plot_data = np.var(pcas_energy[novelty_class].T,1)\n",
    "        ax[irow,icolumn].bar(bins,plot_data,yerr=err_plot_data, width=1,facecolor='b',alpha=0.75)\n",
    "        ax[irow,icolumn].errorbar(bins+0.5, plot_data,yerr=err_plot_data,ecolor='r',fmt='o')\n",
    "        \n",
    "        #ax[irow,icolumn].set_ylim([0,100])\n",
    "        \n",
    "        m_fontsize = 25\n",
    "        ax[irow,icolumn].set_title('PCA Extraction for %s novelty'%(novelty_label), \n",
    "                                   fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow,icolumn].set_ylabel('% of Energy', fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow,icolumn].set_xlabel('#PCA', fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow,icolumn].grid()\n",
    "        \n",
    "        #ax[irow,icolumn].set_yticks([x for x in range(0,101,5)])\n",
    "        ax[irow,icolumn].set_xticks([x for x in range(0,401,20)])\n",
    "        ax[irow,icolumn].set_xticklabels([x for x in range(0,401,20)],rotation=45)\n",
    "                \n",
    "        \n",
    "        if icolumn+1 == n_columns:\n",
    "            icolumn = 0\n",
    "            irow = irow+1\n",
    "        else:\n",
    "            icolumn = icolumn+1\n",
    "    \n",
    "    fig.savefig(result_analysis_path+'/picts/'+choose_date+'_pca_amount_energy.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA energy analysis - correlation before PCA\n",
    "%time\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCASingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    \n",
    "            \n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    \n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    plt.rcParams['xtick.labelsize'] = 18\n",
    "    plt.rcParams['ytick.labelsize'] = 18\n",
    "    \n",
    "    n_pcas = all_data.shape[1]\n",
    "    bins = np.linspace(0,n_pcas-1,n_pcas)\n",
    "    \n",
    "    n_columns = 2\n",
    "    icolumn = 0\n",
    "    \n",
    "    n_rows = 2\n",
    "    irow = 0\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=n_rows, ncols=n_columns,figsize=(25,20))\n",
    "    \n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        known_data = all_data[all_trgt!=novelty_class,:]\n",
    "        known_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "        \n",
    "        corr_mat = np.corrcoef(known_data[:,:],rowvar=False)\n",
    "        \n",
    "        im = ax[irow,icolumn].imshow(corr_mat, interpolation='nearest',clim=(-1.0, 1.0))\n",
    "        plt.colorbar(im,ax=ax[irow,icolumn])\n",
    "        \n",
    "        m_fontsize = 25\n",
    "        ax[irow,icolumn].set_title('Correlation Coeficient for %s novelty'%(novelty_label), \n",
    "                                   fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow,icolumn].set_ylabel('# Frequency bin', fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow,icolumn].set_xlabel('# Frequency bin', fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow,icolumn].grid()\n",
    "        \n",
    "        ax[irow,icolumn].set_yticks([x for x in range(0,401,20)])\n",
    "        ax[irow,icolumn].set_yticklabels([x for x in range(0,401,20)],rotation=45)\n",
    "        ax[irow,icolumn].set_xticks([x for x in range(0,401,20)])\n",
    "        ax[irow,icolumn].set_xticklabels([x for x in range(0,401,20)],rotation=45)\n",
    "        \n",
    "        \n",
    "        if icolumn+1 == n_columns:\n",
    "            icolumn = 0\n",
    "            irow = irow+1\n",
    "        else:\n",
    "            icolumn = icolumn+1\n",
    "    \n",
    "    fig.savefig(result_analysis_path+'/picts/'+choose_date+'_before_pca_corr_coef.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA energy analysis - correlation after PCA\n",
    "%time\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCASingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if not train_info['preprocessing_done']:\n",
    "        print 'Preprocessing is not done, just do it'\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    [pcas] = joblib.load(result_analysis_path+'/result_files'+'/'+choose_date+'_pca_file.jbl')\n",
    "    \n",
    "    choose_fold = 0\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    \n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    plt.rcParams['xtick.labelsize'] = 18\n",
    "    plt.rcParams['ytick.labelsize'] = 18\n",
    "    \n",
    "    n_pcas = all_data.shape[1]\n",
    "    bins = np.linspace(0,n_pcas-1,n_pcas)\n",
    "    \n",
    "    n_columns = 2\n",
    "    icolumn = 0\n",
    "    \n",
    "    n_rows = 2\n",
    "    irow = 0\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=n_rows, ncols=n_columns,figsize=(25,20))\n",
    "    \n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        \n",
    "        my_pca = pcas[novelty_class][choose_fold]\n",
    "        \n",
    "        known_data = all_data[all_trgt!=novelty_class,:]\n",
    "        known_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "        \n",
    "        pca_proj_data = my_pca.transform(known_data)\n",
    "        \n",
    "        corr_mat = np.corrcoef(pca_proj_data[:,:],rowvar=False)\n",
    "        \n",
    "        im = ax[irow,icolumn].imshow(corr_mat, interpolation='nearest',clim=(-1.0, 1.0))\n",
    "        plt.colorbar(im,ax=ax[irow,icolumn])\n",
    "        \n",
    "        m_fontsize = 25\n",
    "        ax[irow,icolumn].set_title('Correlation Coeficient for %s novelty'%(novelty_label), \n",
    "                                   fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow,icolumn].set_ylabel('# Frequency bin', fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow,icolumn].set_xlabel('# Frequency bin', fontsize= m_fontsize, fontweight='bold')\n",
    "        ax[irow,icolumn].grid()\n",
    "        \n",
    "        ax[irow,icolumn].set_yticks([x for x in range(0,401,20)])\n",
    "        ax[irow,icolumn].set_yticklabels([x for x in range(0,401,20)],rotation=45)\n",
    "        ax[irow,icolumn].set_xticks([x for x in range(0,401,20)])\n",
    "        ax[irow,icolumn].set_xticklabels([x for x in range(0,401,20)],rotation=45)\n",
    "        \n",
    "        \n",
    "        if icolumn+1 == n_columns:\n",
    "            icolumn = 0\n",
    "            irow = irow+1\n",
    "        else:\n",
    "            icolumn = icolumn+1\n",
    "    \n",
    "    fig.savefig(result_analysis_path+'/picts/'+choose_date+'_after_pca_corr_coef.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento - Single Class SVM\n",
    "Adicionar informações\n",
    "## Arquivos gerados\n",
    "Adicionar informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treinamento do detector de novidade\n",
    "\n",
    "%time\n",
    "\n",
    "# Classifiers\n",
    "classifiers = {}\n",
    "\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCASingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    \n",
    "    # saving time\n",
    "    if not train_info['preprocessing_done']:\n",
    "        print 'Pre-processing is not done'\n",
    "        continue\n",
    "    \n",
    "    # saving time\n",
    "    if train_info['train_done']:\n",
    "        print 'Train done, just analyse it'\n",
    "        continue\n",
    "    \n",
    "    [pcas] = joblib.load(result_analysis_path+'/result_files'+'/'+choose_date+'_pca_file.jbl')\n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        classifiers[novelty_class] = {}\n",
    "        \n",
    "        known_data = all_data[all_trgt!=novelty_class,:]\n",
    "        known_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "        \n",
    "        # for: folds\n",
    "        for ifold in range(len(train_info['CVO_novelty_%s'%(novelty_label)])):\n",
    "            classifiers[novelty_class][ifold] = {}\n",
    "            \n",
    "            # split data in trn set, tst set\n",
    "            train_id, test_id = train_info['CVO_novelty_%s'%(novelty_label)][ifold]\n",
    "            \n",
    "            # normalize data based in train set\n",
    "            if train_info['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "        \n",
    "            norm_known_data = scaler.transform(known_data)\n",
    "        \n",
    "            pca_norm_known_data = pcas[novelty_class][ifold].transform(norm_known_data)\n",
    "            \n",
    "            #cut some pcas\n",
    "            pca_norm_known_data = pca_norm_known_data[:,0:train_info['n_pcas']]\n",
    "        \n",
    "            for nu_id, nu_value in enumerate(train_info['nu_values']):\n",
    "                classifiers[novelty_class][ifold][nu_value] = {}\n",
    "                print 'Training Classifiers: Novelty Class: %s, %i of %i folds, %i of %i nu_values'%(\n",
    "                    novelty_label, ifold+1, len(train_info['CVO_novelty_%s'%(novelty_label)]),\n",
    "                    nu_id+1, len(train_info['nu_values'])\n",
    "                )\n",
    "                \n",
    "                # novelty detector\n",
    "                novelty_detector = svm.OneClassSVM(nu=nu_value, \n",
    "                                                   kernel=\"rbf\", \n",
    "                                                   gamma=train_info['gamma_value'])\n",
    "                novelty_detector.fit(pca_norm_known_data[train_id,:])\n",
    "                \n",
    "                classifiers[novelty_class][ifold][nu_value]['NoveltyDetector'] = novelty_detector\n",
    "                \n",
    "                # class specialist\n",
    "                for known_class, known_label in enumerate(class_labels):\n",
    "                    if known_class == novelty_class: continue\n",
    "                    class_idx = np.nonzero(known_trgt == known_class)[0]\n",
    "                    idx = np.intersect1d(class_idx,train_id)\n",
    "                    classifier = svm.OneClassSVM(nu=nu_value, kernel=\"rbf\", \n",
    "                                                 gamma=train_info['gamma_value'])\n",
    "                    classifier.fit(pca_norm_known_data[idx,:])\n",
    "                    classifiers[novelty_class][ifold][nu_value][known_label] = classifier\n",
    "    \n",
    "        classifier_info_name = result_analysis_path+'/classifiers_files'+'/'+choose_date+'_'+novelty_label+'_classifiers.jbl'\n",
    "        joblib.dump([classifiers[novelty_class]],classifier_info_name,compress=9)\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "train_info['train_done'] = True\n",
    "joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extração dos resultados\n",
    "%time\n",
    "\n",
    "import scipy.stats as sp_stats\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCASingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # checking preprocessing\n",
    "    if not train_info['preprocessing_done']:\n",
    "        print 'Extract Preprocessing!!!'\n",
    "        continue\n",
    "        \n",
    "    # checking training\n",
    "    #if not train_info['train_done']:\n",
    "    #    print 'Perform Train!!!'\n",
    "    #    continue\n",
    "        \n",
    "    # checking results extraction\n",
    "    if train_info['results_done']:\n",
    "        print 'Extraction done, just analyse it'\n",
    "        continue\n",
    "        \n",
    "    [pcas] = joblib.load(result_analysis_path+'/result_files'+'/'+choose_date+'_pca_file.jbl')\n",
    "    \n",
    "    #classifier_info_name = result_analysis_path+'/classifiers_files'+'/'+choose_date+'_classifiers.jbl'\n",
    "    #[classifiers] = joblib.load(classifier_info_name)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        results[novelty_class] = {}\n",
    "        \n",
    "        # Class Specialist Efficiency\n",
    "        for known_class, known_label in enumerate(class_labels):\n",
    "            if known_class == novelty_class: continue\n",
    "            results[novelty_class][known_label] = np.zeros(\n",
    "                [len(train_info['nu_values']), \n",
    "                 len(train_info['CVO_novelty_%s'%(novelty_label)])])\n",
    "        \n",
    "        # Accuracy\n",
    "        results[novelty_class]['Accuracy'] = np.zeros(\n",
    "            [len(train_info['nu_values']), \n",
    "             len(train_info['CVO_novelty_%s'%(novelty_label)])]) \n",
    "        \n",
    "        # Known SP\n",
    "        results[novelty_class]['Known SP'] = np.zeros(\n",
    "            [len(train_info['nu_values']), \n",
    "             len(train_info['CVO_novelty_%s'%(novelty_label)])])\n",
    "        \n",
    "        # Novelty Detection\n",
    "        results[novelty_class]['Novelty Detection'] = np.zeros(\n",
    "            [len(train_info['nu_values']), \n",
    "             len(train_info['CVO_novelty_%s'%(novelty_label)])])\n",
    "        \n",
    "        # Trigger\n",
    "        results[novelty_class]['Trigger'] = np.zeros(\n",
    "            [len(train_info['nu_values']), \n",
    "             len(train_info['CVO_novelty_%s'%(novelty_label)])])\n",
    "       \n",
    "        # for: folds\n",
    "        for ifold in range(len(train_info['CVO_novelty_%s'%(novelty_label)])):\n",
    "            # split data in trn set, tst set\n",
    "            train_id, test_id = train_info['CVO_novelty_%s'%(novelty_label)][ifold]\n",
    "            \n",
    "            # normalize data based in train set\n",
    "            if train_info['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "        \n",
    "            norm_all_data = scaler.transform(all_data)\n",
    "        \n",
    "            pca_norm_all_data = pcas[novelty_class][ifold].transform(norm_all_data)\n",
    "            \n",
    "            #cut some pcas\n",
    "            pca_norm_all_data = pca_norm_all_data[:,0:train_info['n_pcas']]\n",
    "            \n",
    "            for nu_id, nu_value in enumerate(train_info['nu_values']):\n",
    "                print 'Analysing Classifiers: Novelty Class: %s, %i of %i folds, %i of %i nu_values'%(\n",
    "                    novelty_label, ifold+1, len(train_info['CVO_novelty_%s'%(novelty_label)]),\n",
    "                    nu_id+1, len(train_info['nu_values'])\n",
    "                )\n",
    "                \n",
    "                # class specialist\n",
    "                for known_class, known_label in enumerate(class_labels):\n",
    "                    if known_class == novelty_class: continue\n",
    "                    output = (classifiers[novelty_class]\n",
    "                              [ifold][nu_value]\n",
    "                              [known_label].predict(pca_norm_all_data[all_trgt==known_class]))\n",
    "                    \n",
    "                    results[novelty_class][known_label][nu_id,ifold] = (\n",
    "                    float(sum(output==1))/float(output.shape[0]))\n",
    "                \n",
    "                # accuracy\n",
    "                buff = np.zeros([len(class_labels)-1,1])\n",
    "                for known_class, known_label in enumerate(class_labels):\n",
    "                    if known_class == novelty_class: continue\n",
    "                    buff[known_class-(known_class>novelty_class)] = (\n",
    "                        results[novelty_class][known_label][nu_id,ifold])\n",
    "                results[novelty_class]['Accuracy'][nu_id,ifold] = np.mean(buff,axis=0)\n",
    "                \n",
    "                # known sp\n",
    "                results[novelty_class]['Known SP'][nu_id,ifold] = (np.sqrt(np.mean(buff,axis=0)\n",
    "                                                                           *sp_stats.gmean(buff,axis=0)))\n",
    "                \n",
    "                # novelty detection\n",
    "                output = (classifiers[novelty_class]\n",
    "                          [ifold][nu_value]\n",
    "                          ['NoveltyDetector'].predict(pca_norm_all_data\n",
    "                                                [all_trgt==novelty_class]))\n",
    "                results[novelty_class]['Novelty Detection'][nu_id,ifold] = (\n",
    "                float(sum(output==-1))/float(output.shape[0]))\n",
    "                \n",
    "                # trigger\n",
    "                output = (classifiers[novelty_class]\n",
    "                          [ifold][nu_value]\n",
    "                          ['NoveltyDetector'].predict(pca_norm_all_data\n",
    "                                                [all_trgt!=novelty_class]))\n",
    "                results[novelty_class]['Trigger'][nu_id,ifold] = (\n",
    "                float(sum(output==1))/float(output.shape[0]))\n",
    "                \n",
    "    result_file_path = result_analysis_path+'/result_files'+'/'+choose_date+'_results.jbl'\n",
    "    joblib.dump([results],result_file_path,compress=9)\n",
    "\n",
    "    train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "    train_info['results_done'] = True\n",
    "    joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analise dos resultados\n",
    "%time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCASingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # checking preprocessing\n",
    "    if not train_info['preprocessing_done']:\n",
    "        print 'Extract Preprocessing!!!'\n",
    "        continue\n",
    "        \n",
    "    # checking training\n",
    "    #if not train_info['train_done']:\n",
    "    #    print 'Perform Train!!!'\n",
    "    #    continue\n",
    "        \n",
    "    # checking results extraction\n",
    "    if not train_info['results_done']:\n",
    "        print 'Perform Extraction!!!'\n",
    "        continue\n",
    "        \n",
    "    [pcas] = joblib.load(result_analysis_path+'/result_files'+'/'+choose_date+'_pca_file.jbl')\n",
    "    \n",
    "    #classifier_info_name = result_analysis_path+'/classifiers_files'+'/'+choose_date+'_classifiers.jbl'\n",
    "    #[classifiers] = joblib.load(classifier_info_name)\n",
    "    \n",
    "    result_file_path = result_analysis_path+'/result_files'+'/'+choose_date+'_results.jbl'\n",
    "    [results] = joblib.load(result_file_path)\n",
    "    \n",
    "    # Plot Efficiency\n",
    "    fig, subplot_array = plt.subplots(nrows=2, ncols=2,figsize=(20,20))\n",
    "    m_colors = ['b', 'r', 'g', 'y']\n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        ax = plt.subplot(2,2,novelty_class+1)\n",
    "        m_fontsize = 18\n",
    "        plt.title('Classifier Eff. - Novelty: '+novelty_label, fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        if novelty_class > -1:\n",
    "            plt.xlabel(r'$\\nu$ values', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        plt.ylabel('Efficiency(%)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        m_leg = []\n",
    "        \n",
    "        line_width = 3.5\n",
    "        \n",
    "        # class specialist eff\n",
    "        for known_class, known_label in enumerate(class_labels):\n",
    "            if known_class == novelty_class: continue\n",
    "            plot_data = results[novelty_class][known_label]\n",
    "            ax.errorbar(train_info['nu_values'],\n",
    "                        100*np.mean(plot_data,axis=1),\n",
    "                        100*np.std(plot_data,axis=1),marker='o',\n",
    "                        color=m_colors[known_class],alpha=0.5,linewidth=line_width)\n",
    "            m_leg.append(known_label+' Eff.')\n",
    "        \n",
    "        # accuracy\n",
    "        plot_data = results[novelty_class]['Accuracy']\n",
    "        ax.errorbar(train_info['nu_values'],\n",
    "                    100*np.mean(plot_data,axis=1),\n",
    "                    100*np.std(plot_data,axis=1),marker='o',\n",
    "                    color='k',ls='-.',alpha=0.5, linewidth=line_width)\n",
    "        m_leg.append('Known Acc.')\n",
    "        \n",
    "        # known sp\n",
    "        plot_data = results[novelty_class]['Known SP']\n",
    "        ax.errorbar(train_info['nu_values'],\n",
    "                    100*np.mean(plot_data,axis=1),\n",
    "                    100*np.std(plot_data,axis=1),marker='o',\n",
    "                    color='k',ls='-.',alpha=0.5, linewidth=line_width)\n",
    "        m_leg.append('Known SP')\n",
    "        \n",
    "        # trigger\n",
    "        plot_data = results[novelty_class]['Trigger']\n",
    "        ax.errorbar(train_info['nu_values'],\n",
    "                    100*np.mean(plot_data,axis=1),\n",
    "                    100*np.std(plot_data,axis=1),marker='o',\n",
    "                    color='k',ls=':',alpha=0.5, linewidth=line_width)\n",
    "        m_leg.append('Trigger')\n",
    "        \n",
    "        # novelty detection\n",
    "        plot_data = results[novelty_class]['Novelty Detection']\n",
    "        ax.errorbar(train_info['nu_values'],\n",
    "                    100*np.mean(plot_data,axis=1),\n",
    "                    100*np.std(plot_data,axis=1),marker='o',\n",
    "                    color='k',ls='-',alpha=0.5, linewidth=line_width)\n",
    "        m_leg.append('Novelty Detection')\n",
    "        \n",
    "        # graphical assusts\n",
    "        ax.set_ylim([0.0, 115])\n",
    "        ax.set_yticks([x for x in range(0,101,5)])\n",
    "        \n",
    "        ax.set_xlim([np.min(train_info['nu_values']), np.max(train_info['nu_values'])])\n",
    "        ax.set_xticks(train_info['nu_values'])\n",
    "        ax.set_xticklabels(train_info['nu_values'],rotation=45)\n",
    "        \n",
    "        ax.grid()\n",
    "        ax.legend(m_leg, loc='upper right',ncol=3)\n",
    "    fig.savefig(result_analysis_path+'/picts/'+choose_date+'_'+\n",
    "                log_entries[log_id]['package']+'_novelty_detection.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print result_analysis_path+'/picts/'+choose_date+'_'+log_entries[log_id]['package']+'_novelty_detection.pdf'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
