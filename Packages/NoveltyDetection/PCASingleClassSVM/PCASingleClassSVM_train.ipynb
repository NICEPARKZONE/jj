{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "# Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "# Laboratorio de Processamento de Sinais - UFRJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natmourajr/.virtualenvs/sonarferney/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 1.21765494347 seconds\n",
      "Time to read data file: 1.61923003197 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "outputpath = os.environ['OUTPUTDATAPATH']\n",
    "main_analysis_path = os.environ['SONAR_WORKSPACE']\n",
    "log_analysis_path = os.environ['PACKAGE_OUTPUT']\n",
    "result_analysis_path = os.environ['PACKAGE_OUTPUT']+'/PCASingleClassSVM'\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "\n",
    "subfolder = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "\n",
    "if(not os.path.exists(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))):\n",
    "    print outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "        subfolder,n_pts_fft,decimation_rate)+' doesnt exist...please create it'\n",
    "    exit()\n",
    "    \n",
    "#Read lofar data\n",
    "[data,class_labels] = joblib.load(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))\n",
    "m_time = time.time()-m_time\n",
    "print 'Time to read data file: '+str(m_time)+' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train process\n",
    "## The train will modify one file and create three different files\n",
    "\n",
    "### Log File:\n",
    "This file will store basic information of all Package's trains and it will guide the analyses file to recognize which train information file should load. In each train this file should be appended with a new line contend the basic information to find the train information file (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n",
    "\n",
    "### Train Information File\n",
    "This file will store full information of the train performed (all parameters) in its name (each train information file will have a different name). And it will guide which train classifier file or which train result file should be open for analysis (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n",
    "\n",
    "### Train Classifier File\n",
    "This file will store the classifier after train, the folds information and the train evolution (when this exists) (NATIVE FORMAT) or (PYTHON FORMAT) - This file should not be access by all programs\n",
    "\n",
    "### Train Result File\n",
    "This file will store the classifier result for all data and classification target (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Novelty Classifier for ClassA\n",
      "Starting Training Processing for fold:  0\n",
      "Starting Training Processing for fold:  1\n",
      "Train Novelty Classifier for ClassB\n",
      "Starting Training Processing for fold:  0\n",
      "Starting Training Processing for fold:  1\n",
      "Train Novelty Classifier for ClassC\n",
      "Starting Training Processing for fold:  0\n",
      "Starting Training Processing for fold:  1\n",
      "Train Novelty Classifier for ClassD\n",
      "Starting Training Processing for fold:  0\n",
      "Starting Training Processing for fold:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/NoveltyDetection/PCASingleClassSVM/train_info_files/2016_12_01_19_09_08_train_info.jbl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Process\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(\"NoveltyDetection\",'PCASingleClassSVM')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_pcas = 50\n",
    "norm = 'mapstd'\n",
    "nu_values = np.array([0.001, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "gamma_value = 0.1\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_pcas'] = n_pcas\n",
    "train_info['norm'] = norm\n",
    "train_info['nu_values'] = nu_values\n",
    "train_info['gamma_value'] = gamma_value\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+date+'_train_info.jbl'\n",
    "class_info_name = result_analysis_path+'/classifiers_files'+'/'+date+'_classifiers.jbl'\n",
    "result_file_base_name = result_analysis_path+'/result_files'+'/'+date\n",
    "\n",
    "#joblib.dump([train_info],train_info_name,compress=9)\n",
    "\n",
    "# create a full data vector\n",
    "all_data = {};\n",
    "all_trgt = {};\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    for irun in range(len(data[iclass])):\n",
    "        if len(all_data) == 0:\n",
    "            all_data = data[iclass][irun]['Signal']\n",
    "            all_trgt = (iclass)*np.ones(data[iclass][irun]['Signal'].shape[1])\n",
    "        else:\n",
    "            all_data = np.append(all_data,data[iclass][irun]['Signal'],axis=1)\n",
    "            all_trgt = np.append(all_trgt,(iclass)*np.ones(data[iclass][irun]['Signal'].shape[1]),axis=0)\n",
    "\n",
    "all_data = all_data.transpose()\n",
    "\n",
    "# Train the Classifier\n",
    "\n",
    "# Classifiers\n",
    "classifiers = {}\n",
    "for novelty_class, novelty_label in enumerate(class_labels):\n",
    "    classifiers[novelty_class] = {}\n",
    "    print 'Train Novelty Classifier for %s'%(novelty_label)\n",
    "\n",
    "    #data vectors\n",
    "    known_data = {};\n",
    "    novelty_data = {};\n",
    "    \n",
    "    # target vector\n",
    "    known_trgt = {};\n",
    "\n",
    "    # known classes loop\n",
    "    for known_class, known_label in enumerate(class_labels):\n",
    "        if known_class == novelty_class:\n",
    "            continue\n",
    "        #print 'Create known data vector for %s' % known_label\n",
    "        \n",
    "        # known class run loop\n",
    "        for irun in range(len(data[known_class])):\n",
    "            if len(known_data) == 0:\n",
    "                known_data = data[known_class][irun]['Signal']\n",
    "                known_trgt = (known_class)*np.ones(data[known_class][irun]['Signal'].shape[1])\n",
    "            else:\n",
    "                known_data = np.append(known_data,data[known_class][irun]['Signal'],axis=1)\n",
    "                known_trgt = np.append(known_trgt,(known_class)*np.ones(data[known_class][irun]['Signal'].shape[1])\n",
    "                                       ,axis=0)\n",
    "    known_data = known_data.transpose()\n",
    "    \n",
    "    CVO = cross_validation.StratifiedKFold(known_trgt, n_folds)\n",
    "    CVO = list(CVO)\n",
    "    train_info['CVO_%s'%(novelty_label)] = CVO\n",
    "\n",
    "    # for: folds\n",
    "    for ifold in range(len(CVO)):\n",
    "        classifiers[novelty_class][ifold] = {}\n",
    "        #if not ifold == 0 :\n",
    "        #    break\n",
    "        print 'Starting Training Processing for fold: ', ifold\n",
    "        \n",
    "        # split data in trn set, tst set\n",
    "        train_id, test_id = CVO[ifold]\n",
    "\n",
    "        # normalize data based in train set\n",
    "        if train_info['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "        elif train_info['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "        elif train_info['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "        \n",
    "        norm_known_data = scaler.transform(known_data)\n",
    "        norm_all_data = scaler.transform(all_data)\n",
    "        \n",
    "        pca = PCA(n_components=train_info['n_pcas'])\n",
    "        norm_known_data = pca.fit(norm_known_data[train_id,:]).transform(norm_known_data)\n",
    "        norm_all_data = pca.transform(norm_all_data)\n",
    "        \n",
    "        \n",
    "        for nu_id, nu_value in enumerate(train_info['nu_values']):\n",
    "            classifiers[novelty_class][ifold][nu_value] = {}\n",
    "            # single class (known vs novelty)\n",
    "            \n",
    "            # Training Novelty Detector\n",
    "            novelty_detector = svm.OneClassSVM(nu=nu_value, kernel=\"rbf\", gamma=gamma_value)\n",
    "            novelty_detector.fit(norm_known_data[train_id,:])\n",
    "            classifiers[novelty_class][ifold][nu_value]['NoveltyDetector'] = novelty_detector\n",
    "            \n",
    "            # known classes loop (known vs other known)\n",
    "            known_classifier = {}\n",
    "            all_outputs = []\n",
    "            for known_class, known_label in enumerate(class_labels):\n",
    "                if known_class == novelty_class: continue\n",
    "                known_classifier[known_class-(known_class>novelty_class)] = []\n",
    "                class_idx = np.nonzero(known_trgt == known_class)[0]\n",
    "                idx = np.intersect1d(class_idx,train_id)\n",
    "                known_classifier[known_class-(known_class>novelty_class)] = (svm.OneClassSVM\n",
    "                                                                             (nu=nu_value, kernel=\"rbf\", \n",
    "                                                                              gamma=gamma_value))\n",
    "                known_classifier[known_class-(known_class>novelty_class)].fit(norm_known_data[idx,:])\n",
    "                classifiers[novelty_class][ifold][nu_value][known_label] = known_classifier[known_class-(\n",
    "                                                                                known_class>novelty_class)]\n",
    "                a = classifiers[novelty_class][ifold][nu_value][known_label].predict(norm_all_data)\n",
    "                if len(all_outputs) == 0:\n",
    "                    all_outputs = a[:,np.newaxis]\n",
    "                else:\n",
    "                    all_outputs = np.append(all_outputs,a[:,np.newaxis],axis=1)\n",
    "            a = classifiers[novelty_class][ifold][nu_value]['NoveltyDetector'].predict(norm_all_data)\n",
    "            all_outputs = np.append(all_outputs,a[:,np.newaxis],axis=1)\n",
    "            all_outputs = np.append(all_outputs,all_trgt[:,np.newaxis],axis=1)\n",
    "            np.savetxt('%s_%s_novelty_%i_fold_%f_nu.txt'%\n",
    "                                (result_file_base_name, novelty_label,ifold,nu_value),all_outputs)\n",
    "            \n",
    "            \n",
    "# Create a train classifier file\n",
    "joblib.dump([classifiers],class_info_name,compress=9)\n",
    "\n",
    "joblib.dump([train_info],train_info_name,compress=9)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
