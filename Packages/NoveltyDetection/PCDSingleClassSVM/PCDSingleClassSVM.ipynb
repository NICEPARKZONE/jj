{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de Classificação para Marinha do Brasil\n",
    "\n",
    "## Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "\n",
    "Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "Laboratório de Tecnologia Sonar\n",
    "\n",
    "Instituto de Pesquisas da Marinha - IPqM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas e leitura dos dados\n",
    "As bibliotecas necessárias para a inclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 0.00012 seconds\n",
      "Time to read data file: 1.58412 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: %1.5f seconds'%(m_time-init_time)\n",
    "\n",
    "outputpath = os.environ['OUTPUTDATAPATH']\n",
    "main_analysis_path = os.environ['SONAR_WORKSPACE']\n",
    "log_analysis_path = os.environ['PACKAGE_OUTPUT']\n",
    "result_analysis_path = os.environ['PACKAGE_OUTPUT']+'/PCDSingleClassSVM'\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "\n",
    "subfolder = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "\n",
    "if(not os.path.exists(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))):\n",
    "    print outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "        subfolder,n_pts_fft,decimation_rate)+' doesnt exist...please create it'\n",
    "    exit()\n",
    "    \n",
    "#Read lofar data\n",
    "[data,class_labels] = joblib.load(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))\n",
    "m_time = time.time()-m_time\n",
    "print 'Time to read data file: %1.5f seconds'%m_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento dos dados\n",
    "Os dados encontram-se no formato do matlab, para isso precisam ser processados para o formato de python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process data...\n",
    "# create a full data vector\n",
    "all_data = {};\n",
    "all_trgt = {};\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    for irun in range(len(data[iclass])):\n",
    "        if len(all_data) == 0:\n",
    "            all_data = data[iclass][irun]['Signal']\n",
    "            all_trgt = (iclass)*np.ones(data[iclass][irun]['Signal'].shape[1])\n",
    "        else:\n",
    "            all_data = np.append(all_data,data[iclass][irun]['Signal'],axis=1)\n",
    "            all_trgt = np.append(all_trgt,(iclass)*np.ones(data[iclass][irun]\n",
    "                                                           ['Signal'].shape[1]),axis=0)\n",
    "            \n",
    "all_data = all_data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanceamento de Classes\n",
    "Os dados encontram-se desbalanceados. Com isso, os classificadores podem se especializar em uma classe (gerando mais SVs para a mesma) e não se especializar em outras\n",
    "\n",
    "Acessados em 21/12/2016\n",
    "\n",
    "https://svds.com/learning-imbalanced-classes/\n",
    "\n",
    "http://www.cs.utah.edu/~piyush/teaching/ImbalancedLearning.pdf\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n",
    "\n",
    "Para solucionar isso, a primeira solução é \"criar\" dados das classes com menos eventos de maneira aleatória. Outras soluções podem ser propostas posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd event of ClassA is 4312\n",
      "Qtd event of ClassB is 9781\n",
      "Qtd event of ClassC is 3833\n",
      "Qtd event of ClassD is 7918\n",
      "\n",
      "Biggest class is ClassB with 9781 events\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (4312, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (9781, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (3833, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (7918, 400)\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "# unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "# Same number of events in each class\n",
    "qtd_events_biggest_class = 0\n",
    "biggest_class_label = ''\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "        qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "        biggest_class_label = class_label\n",
    "    print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "\n",
    "balanced_data = {}\n",
    "balanced_trgt = {}\n",
    "\n",
    "from Functions import DataHandler as dh\n",
    "m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if len(balanced_data) == 0:\n",
    "        class_events = all_data[all_trgt==iclass,:]\n",
    "        balanced_data = m_datahandler.CreateEventsForClass(\n",
    "            class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "        balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "    else:\n",
    "        balanced_data = np.append(balanced_data,\n",
    "                                  (m_datahandler.CreateEventsForClass(\n",
    "                    all_data[all_trgt==iclass,:],\n",
    "                    qtd_events_biggest_class-sum(all_trgt==iclass))),\n",
    "                                  axis=0)\n",
    "        balanced_trgt = np.append(balanced_trgt,\n",
    "                                  (iclass)*np.ones(qtd_events_biggest_class),axis=0)\n",
    "        \n",
    "all_data = balanced_data\n",
    "all_trgt = balanced_trgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições do treinamento\n",
    "Nessa célula temos os parâmetros do treinamento a ser realizado. No log, deve ficar armazenada a data do treinamento para a reconstrução dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.06 µs\n",
      "Dividing data in trn and tst for novelty class: ClassA\n",
      "Dividing data in trn and tst for novelty class: ClassB\n",
      "Dividing data in trn and tst for novelty class: ClassC\n",
      "Dividing data in trn and tst for novelty class: ClassD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/NoveltyDetection/PCDSingleClassSVM/train_info_files/2016_12_25_20_58_23_train_info.jbl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(\"NoveltyDetection\",'PCDSingleClassSVM')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_pcds = 2\n",
    "norm = 'mapstd'\n",
    "nu_values = np.array([0.001, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "gamma_value = 0.1\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_pcds'] = n_pcds\n",
    "train_info['norm'] = norm\n",
    "train_info['nu_values'] = nu_values\n",
    "train_info['gamma_value'] = gamma_value\n",
    "\n",
    "# divide data in train and test for novelty detection\n",
    "for novelty_class, novelty_label in enumerate(class_labels):\n",
    "    print 'Dividing data in trn and tst for novelty class: %s'%(novelty_label)\n",
    "    CVO = cross_validation.StratifiedKFold(all_trgt[all_trgt!=novelty_class], n_folds)\n",
    "    CVO = list(CVO)\n",
    "    train_info['CVO_novelty_%s'%(novelty_label)] = CVO\n",
    "\n",
    "train_info['preprocessing_done'] = False\n",
    "train_info['train_done'] = False\n",
    "train_info['results_done'] = False\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+date+'_train_info.jbl'\n",
    "joblib.dump([train_info],train_info_name,compress=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'date': '2016_12_01_17_27_33', 'package': 'PCASingleClassSVM'}, 1: {'date': '2016_12_01_17_51_48', 'package': 'PCASingleClassSVM'}, 2: {'date': '2016_12_01_18_09_34', 'package': 'PCASingleClassSVM'}, 3: {'date': '2016_12_01_18_25_15', 'package': 'PCASingleClassSVM'}, 4: {'date': '2016_12_01_19_09_08', 'package': 'PCASingleClassSVM'}, 5: {'date': '2016_12_21_15_11_02', 'package': 'PCASingleClassSVM'}, 6: {'date': '2016_12_21_15_14_37', 'package': 'PCASingleClassSVM'}, 7: {'date': '2016_12_22_14_02_05', 'package': 'PCASingleClassSVM'}, 8: {'date': '2016_12_22_14_03_23', 'package': 'PCASingleClassSVM'}, 9: {'date': '2016_12_22_14_13_41', 'package': 'PCASingleClassSVM'}, 10: {'date': '2016_12_22_14_14_45', 'package': 'PCASingleClassSVM'}, 11: {'date': '2016_12_22_15_27_30', 'package': 'PCASingleClassSVM'}, 12: {'date': '2016_12_22_16_33_54', 'package': 'PCASingleClassSVM'}, 13: {'date': '2016_12_22_17_02_55', 'package': 'PCASingleClassSVM'}, 14: {'date': '2016_12_22_17_13_21', 'package': 'PCASingleClassSVM'}, 15: {'date': '2016_12_24_19_23_04', 'package': 'PCASingleClassSVM'}, 16: {'date': '2016_12_24_19_50_34', 'package': 'PCASingleClassSVM'}, 17: {'date': '2016_12_24_19_50_59', 'package': 'PCASingleClassSVM'}, 18: {'date': '2016_12_24_19_54_20', 'package': 'PCASingleClassSVM'}, 19: {'date': '2016_12_24_20_00_43', 'package': 'PCASingleClassSVM'}, 20: {'date': '2016_12_24_21_47_32', 'package': 'PCASingleClassSVM'}, 21: {'date': '2016_12_24_23_03_14', 'package': 'PCASingleClassSVM'}, 22: {'date': '2016_12_24_23_25_49', 'package': 'PCASingleClassSVM'}, 23: {'date': '2016_12_25_20_24_25', 'package': 'PCDSingleClassSVM'}, 24: {'date': '2016_12_25_20_25_18', 'package': 'PCDSingleClassSVM'}, 25: {'date': '2016_12_25_20_58_23', 'package': 'PCDSingleClassSVM'}}\n"
     ]
    }
   ],
   "source": [
    "# Read log files\n",
    "from Functions import LogFunctions as log\n",
    "mlog = log.LogInformation()\n",
    "log_entries = mlog.RecoverLogEntries(package_name=\"NoveltyDetection\")\n",
    "print log_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing train performed in 2016_12_25_20_58_23 and for PCDSingleClassSVM analysis\n",
      "PCASingleClassSVM Train Info File\n",
      "Date: 2016_12_25_20_58_23\n",
      "Number of Folds: 2\n",
      "Number of Used PCDs: 2\n",
      "Normalization Method: mapstd\n",
      "Gamma Value: 0.100\n",
      "Nu Value(s): \n",
      "[ 0.001  0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9  ]\n",
      "Preprocessing Done: False\n",
      "Train Done: False\n",
      "Extract Results: False\n"
     ]
    }
   ],
   "source": [
    "# Read Information of Train Info File\n",
    "choose_date = '2016_12_25_20_58_23'\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCDSingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'Analysing train performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    print 'PCASingleClassSVM Train Info File'\n",
    "    print 'Date: %s'%(choose_date)\n",
    "    print 'Number of Folds: %i'%(train_info['n_folds'])\n",
    "    print 'Number of Used PCDs: %i'%(train_info['n_pcds'])\n",
    "    print 'Normalization Method: %s'%(train_info['norm'])\n",
    "    print 'Gamma Value: %1.3f'%(train_info['gamma_value'])\n",
    "    print 'Nu Value(s): '\n",
    "    print train_info['nu_values']\n",
    "    if train_info['preprocessing_done']:\n",
    "        print 'Preprocessing Done: True'\n",
    "    else:\n",
    "        print 'Preprocessing Done: False'\n",
    "    if train_info['train_done']:\n",
    "        print 'Train Done: True'\n",
    "    else:\n",
    "        print 'Train Done: False'\n",
    "    if train_info['results_done']:\n",
    "        print 'Extract Results: True'\n",
    "    else:\n",
    "        print 'Extract Results: False'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessamento - PCD\n",
    "Como a dimensionalidade dos dados é alta (400 dimensões), um pré-processamento se faz necessário para reduzir as dimensões das entradas e tornar o modelo menos complexo. Aqui, o pré-processamento utilizado é a PCD (análise de componentes principais de discriminação). Existem alguns tipos de extração de PCDs. Cada extração visa uma característica diferente (uma visa a extração de componentes necessariamente ortogonais entre si, por exemplo). \n",
    "\n",
    "## PCD por Deflação\n",
    "definir\n",
    "\n",
    "## PCD por Cooperativa\n",
    "definir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Merge\n",
    "\n",
    "\n",
    "def pcdc_extractor(inputdata, targetdata, trn_params=None):\n",
    "    ''' \n",
    "        This function extracts the Cooperative Principal Components of Discrimination of a Dataset\n",
    "        \n",
    "        Parameters:\n",
    "            inputdata: dataset with inputs\n",
    "            \n",
    "            targetdata: each class -> an integer\n",
    "            \n",
    "            trn_params: train parameters\n",
    "            \n",
    "            trn_params['n_folds'] = number of cross validation folds\n",
    "            trn_params['n_inits'] = number of initializations\n",
    "            trn_params['n_pcds'] = number of PCDs to be extracted\n",
    "            trn_params['norm'] = normalization\n",
    "            trn_params['learning_rate'] = learning rate\n",
    "            trn_params['learning_decay'] = learning rate decay\n",
    "            trn_params['momentum'] = momentum\n",
    "            trn_params['nesterov'] = nesterov momentum\n",
    "            trn_params['train_verbose'] = train verbose\n",
    "            trn_params['n_epochs'] = number of epochs\n",
    "            trn_params['batch_size'] = batch size\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    if trn_params == None:\n",
    "        trn_params = {}\n",
    "        trn_params['n_folds'] = 2\n",
    "        trn_params['n_inits'] = 2\n",
    "        trn_params['n_pcds'] = 2\n",
    "        trn_params['norm'] = 'none'\n",
    "        trn_params['learning_rate'] = 0.01\n",
    "        trn_params['learning_decay'] = 1e-6\n",
    "        trn_params['momentum'] = 0.3\n",
    "        trn_params['nesterov'] = True\n",
    "        trn_params['train_verbose'] = False\n",
    "        trn_params['n_epochs'] = 300\n",
    "        trn_params['batch_size'] = 8\n",
    "        trn_params['CVO'] = None\n",
    "\n",
    "    print 'PCD Cooperative Extractor'\n",
    "    print 'trn_params: ',trn_params\n",
    "    \n",
    "    # trained classifiers\n",
    "    classifiers = {}\n",
    "    trn_desc = {}\n",
    "    pcds = {}\n",
    "    \n",
    "    if trn_params['CVO'] == None:\n",
    "        CVO = cross_validation.StratifiedKFold(targetdata, trn_params['n_folds'])\n",
    "        CVO = list(CVO)\n",
    "    else:\n",
    "        CVO = trn_params['CVO']\n",
    "    \n",
    "    # from each class an integer -> target max sparse\n",
    "    targetdata_sparse = np_utils.to_categorical(targetdata)\n",
    "    \n",
    "    for ifold in range(trn_params['n_folds']):\n",
    "        train_id, test_id = CVO[ifold]\n",
    "\n",
    "        # normalize data based in train set\n",
    "        if trn_params['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(inputdata[train_id,:])\n",
    "        elif trn_params['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(inputdata[train_id,:])\n",
    "        elif trn_params['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(inputdata[train_id,:])\n",
    "        \n",
    "        if trn_params['norm'] != \"none\":\n",
    "            norm_inputdata = scaler.transform(inputdata)\n",
    "        else:\n",
    "            norm_inputdata = inputdata\n",
    "         \n",
    "        \n",
    "        classifiers[ifold] = {}\n",
    "        trn_desc[ifold] = {}\n",
    "        pcds[ifold] = {}\n",
    "        \n",
    "        for ipcd in range(trn_params['n_pcds']):\n",
    "            best_init = 0\n",
    "            best_loss = 999\n",
    "            if ipcd == 0:\n",
    "                # first pcd - random init\n",
    "                for i_init in range(trn_params['n_inits']):\n",
    "                    # create the model\n",
    "                    model = Sequential()\n",
    "                    \n",
    "                    # add a linear layer to isolate the input of NN model\n",
    "                    model.add(Dense(norm_inputdata.shape[1],\n",
    "                                input_dim=norm_inputdata.shape[1], \n",
    "                                init='identity',trainable=False))\n",
    "                    model.add(Activation('linear'))\n",
    "                    \n",
    "                    # add a non-linear single neuron layer to compress all information\n",
    "                    model.add(Dense(1, input_dim=norm_inputdata.shape[1], init='uniform'))\n",
    "                    model.add(Activation('tanh'))\n",
    "                    \n",
    "                    # add a non-linear output layer with max sparse target shape\n",
    "                    model.add(Dense(targetdata_sparse.shape[1], init='uniform')) \n",
    "                    model.add(Activation('tanh'))\n",
    "                    \n",
    "                    # creating a optimization function using steepest gradient\n",
    "                    sgd = SGD(lr=trn_params['learning_rate'],\n",
    "                              decay=trn_params['learning_decay'],\n",
    "                              momentum=trn_params['momentum'],\n",
    "                              nesterov=trn_params['nesterov'])\n",
    "                    \n",
    "                    # compile the model\n",
    "                    model.compile(loss='mean_squared_error', \n",
    "                                  optimizer=sgd,\n",
    "                                  metrics=['accuracy','mean_squared_error'])\n",
    "                    \n",
    "                    # early stopping to avoid overtraining\n",
    "                    earlyStopping = callbacks.EarlyStopping(\n",
    "                        monitor='val_loss', patience=25,\n",
    "                        verbose=0, mode='auto')\n",
    "                    \n",
    "                    # Train model\n",
    "                    init_trn_desc = model.fit(norm_inputdata[train_id], targetdata_sparse[train_id],\n",
    "                                              nb_epoch=trn_params['n_epochs'], \n",
    "                                              batch_size=trn_params['batch_size'],\n",
    "                                              callbacks=[earlyStopping], \n",
    "                                              verbose=trn_params['train_verbose'],\n",
    "                                              validation_data=(norm_inputdata[test_id],\n",
    "                                                               targetdata_sparse[test_id]),\n",
    "                                              shuffle=True)\n",
    "                    \n",
    "                    if np.min(init_trn_desc.history['val_loss']) < best_loss:\n",
    "                        best_init = i_init\n",
    "                        best_loss = np.min(init_trn_desc.history['val_loss'])\n",
    "                        classifiers[ifold][ipcd] = model\n",
    "                        trn_desc[ifold][ipcd] = init_trn_desc\n",
    "                        pcds[ifold][ipcd] = model.layers[2].get_weights()[0]\n",
    "                    \n",
    "                    print ('Fold: %i of %i - PCD: %i of %i - Init: %i of %i - finished with val cost: %1.3f'%\n",
    "                           (ifold+1,trn_params['n_folds'],\n",
    "                            ipcd+1,trn_params['n_pcds'],\n",
    "                            i_init+1,trn_params['n_inits'],\n",
    "                            best_loss\n",
    "                           ))\n",
    "                    \n",
    "            else: # ipcd != 0\n",
    "                # from second pcd to the end - freeze previous neurons and create a new neuron\n",
    "                for i_init in range(trn_params['n_inits']):\n",
    "                    # create the model\n",
    "                    model = Sequential()\n",
    "                    \n",
    "                    # I removed the linear layer to allow freeze!!!\n",
    "                    \n",
    "                    # add a non-linear freeze previous extracted pcd\n",
    "                    freeze_layer = Sequential()\n",
    "                    \n",
    "                    freeze_layer.add(Dense(ipcd, input_dim=norm_inputdata.shape[1],trainable=False))\n",
    "                    \n",
    "                    weights = freeze_layer.layers[0].get_weights()\n",
    "                    \n",
    "                    for i_old_pcd in range(ipcd):\n",
    "                        for idim in range(norm_inputdata.shape[1]):\n",
    "                            weights[0][idim,i_old_pcd] = pcds[ifold][i_old_pcd][idim]\n",
    "                    \n",
    "                    freeze_layer.layers[0].set_weights(weights)\n",
    "                    \n",
    "                    # add a non-linear no-freeze new neuron\n",
    "                    non_freeze_layer = Sequential()\n",
    "                    non_freeze_layer.add(Dense(1, input_dim=norm_inputdata.shape[1]))\n",
    "                 \n",
    "                    # merge everything\n",
    "                    merged = Merge([freeze_layer, non_freeze_layer], mode='concat')\n",
    "                    model.add(merged)\n",
    "                    \n",
    "                    # add a non-linear output layer with max sparse target shape\n",
    "                    model.add(Dense(targetdata_sparse.shape[1], init='uniform')) \n",
    "                    model.add(Activation('tanh'))\n",
    "                    \n",
    "                    # creating a optimization function using steepest gradient\n",
    "                    sgd = SGD(lr=trn_params['learning_rate'],\n",
    "                              decay=trn_params['learning_decay'],\n",
    "                              momentum=trn_params['momentum'],\n",
    "                              nesterov=trn_params['nesterov'])\n",
    "                    \n",
    "                    # compile the model\n",
    "                    model.compile(loss='mean_squared_error', \n",
    "                                  optimizer=sgd,\n",
    "                                  metrics=['accuracy','mean_squared_error'])\n",
    "                    \n",
    "                    # early stopping to avoid overtraining\n",
    "                    earlyStopping = callbacks.EarlyStopping(\n",
    "                        monitor='val_loss', patience=25,\n",
    "                        verbose=0, mode='auto')\n",
    "                    \n",
    "                    # Train model\n",
    "                    init_trn_desc = model.fit([norm_inputdata[train_id], \n",
    "                                               norm_inputdata[train_id]], \n",
    "                                              targetdata_sparse[train_id],\n",
    "                                              nb_epoch=trn_params['n_epochs'], \n",
    "                                              batch_size=trn_params['batch_size'],\n",
    "                                              callbacks=[earlyStopping], \n",
    "                                              verbose=trn_params['train_verbose'],\n",
    "                                              validation_data=([norm_inputdata[test_id], \n",
    "                                                                norm_inputdata[test_id]],\n",
    "                                                               targetdata_sparse[test_id]),\n",
    "                                              shuffle=True)\n",
    "                    \n",
    "                    if np.min(init_trn_desc.history['val_loss']) < best_loss:\n",
    "                        best_init = i_init\n",
    "                        best_loss = np.min(init_trn_desc.history['val_loss'])\n",
    "                        classifiers[ifold][ipcd] = model\n",
    "                        trn_desc[ifold][ipcd] = init_trn_desc\n",
    "                        pcds[ifold][ipcd] = model.layers[0].layers[1].get_weights()[0]\n",
    "                        \n",
    "                    print ('Fold: %i of %i - PCD: %i of %i - Init: %i of %i - finished with val cost: %1.3f'%\n",
    "                           (ifold+1,trn_params['n_folds'],\n",
    "                            ipcd+1,trn_params['n_pcds'],\n",
    "                            i_init+1,trn_params['n_inits'],\n",
    "                            best_loss\n",
    "                           ))\n",
    "                    \n",
    "    # add cross-validation information in train desc.\n",
    "    trn_desc['CVO'] = CVO                    \n",
    "                    \n",
    "    return [pcds,classifiers,trn_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.1 µs\n",
      "PCD extraction performed in 2016_12_25_20_58_23 and for PCDSingleClassSVM analysis\n",
      "Extracting PCD for novelty ClassA\n",
      "PCD Cooperative Extractor\n",
      "trn_params:  {'nesterov': True, 'learning_rate': 0.01, 'n_inits': 5, 'batch_size': 9, 'n_epochs': 1000, 'train_verbose': False, 'CVO': [(array([ 4891,  4892,  4893, ..., 29340, 29341, 29342]), array([    0,     1,     2, ..., 24450, 24451, 24452])), (array([    0,     1,     2, ..., 24450, 24451, 24452]), array([ 4891,  4892,  4893, ..., 29340, 29341, 29342]))], 'learning_decay': 0.0001, 'momentum': 0.9, 'n_folds': 2, 'norm': 'mapstd', 'n_pcds': 2}\n",
      "Fold: 1 of 2 - PCD: 1 of 2 - Init: 1 of 5 - finished with val cost: 0.132\n",
      "Fold: 1 of 2 - PCD: 1 of 2 - Init: 2 of 5 - finished with val cost: 0.131\n"
     ]
    }
   ],
   "source": [
    "# PCD extraction\n",
    "%time\n",
    "\n",
    "pcds = {}\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCDSingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCD extraction performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    #if train_info['preprocessing_done']:\n",
    "    #    print 'Preprocessing done, just analyse it'\n",
    "    #    continue\n",
    "\n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        pcds[novelty_class] = {}\n",
    "        print 'Extracting PCD for novelty %s'%(novelty_label)\n",
    "        \n",
    "        known_data = all_data[all_trgt!=novelty_class,:]\n",
    "        known_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "        known_trgt[known_trgt>novelty_class] = known_trgt[known_trgt>novelty_class]-1\n",
    "        \n",
    "        # Extract PCD Cooperative\n",
    "        trn_params = {}\n",
    "        trn_params['n_folds'] = train_info['n_folds']\n",
    "        trn_params['n_inits'] = 5\n",
    "        trn_params['n_pcds'] = train_info['n_pcds']\n",
    "        trn_params['norm'] = train_info['norm']\n",
    "        trn_params['learning_rate'] = 0.01\n",
    "        trn_params['learning_decay'] = 1e-4\n",
    "        trn_params['momentum'] = 0.9\n",
    "        trn_params['nesterov'] = True\n",
    "        trn_params['train_verbose'] = False\n",
    "        trn_params['n_epochs'] = 1000\n",
    "        trn_params['batch_size'] = 9\n",
    "        trn_params['CVO'] = train_info['CVO_novelty_%s'%(novelty_label)]\n",
    "            \n",
    "        [pcd,pcd_classifiers,pcd_trn_desc] = pcdc_extractor(known_data,known_trgt,trn_params)            \n",
    "        pcds[novelty_class] = pcd\n",
    "        \n",
    "# saving file\n",
    "pcd_file_path = result_analysis_path+'/result_files'+'/'+date+'_pcd_file.jbl'\n",
    "\n",
    "if pcds != {}:\n",
    "    joblib.dump([pcds],pcd_file_path,compress=9)\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "train_info['preprocessing_done'] = True\n",
    "joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
