{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de Classificação para Marinha do Brasil\n",
    "\n",
    "## Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "\n",
    "Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "Laboratório de Tecnologia Sonar\n",
    "\n",
    "Instituto de Pesquisas da Marinha - IPqM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas e leitura dos dados\n",
    "As bibliotecas necessárias para a inclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 0.55740 seconds\n",
      "Time to read data file: 1.64463 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: %1.5f seconds'%(m_time-init_time)\n",
    "\n",
    "outputpath = os.environ['OUTPUTDATAPATH']\n",
    "main_analysis_path = os.environ['SONAR_WORKSPACE']\n",
    "log_analysis_path = os.environ['PACKAGE_OUTPUT']\n",
    "result_analysis_path = os.environ['PACKAGE_OUTPUT']+'/PCDSingleClassSVM'\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "\n",
    "subfolder = '4classes_old'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "\n",
    "if(not os.path.exists(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))):\n",
    "    print outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "        subfolder,n_pts_fft,decimation_rate)+' doesnt exist...please create it'\n",
    "    exit()\n",
    "    \n",
    "#Read lofar data\n",
    "[data,class_labels] = joblib.load(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))\n",
    "m_time = time.time()-m_time\n",
    "print 'Time to read data file: %1.5f seconds'%m_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento dos dados\n",
    "Os dados encontram-se no formato do matlab, para isso precisam ser processados para o formato de python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process data...\n",
    "# create a full data vector\n",
    "all_data = {};\n",
    "all_trgt = {};\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    for irun in range(len(data[iclass])):\n",
    "        if len(all_data) == 0:\n",
    "            all_data = data[iclass][irun]['Signal']\n",
    "            all_trgt = (iclass)*np.ones(data[iclass][irun]['Signal'].shape[1])\n",
    "        else:\n",
    "            all_data = np.append(all_data,data[iclass][irun]['Signal'],axis=1)\n",
    "            all_trgt = np.append(all_trgt,(iclass)*np.ones(data[iclass][irun]\n",
    "                                                           ['Signal'].shape[1]),axis=0)\n",
    "            \n",
    "all_data = all_data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanceamento de Classes\n",
    "Os dados encontram-se desbalanceados. Com isso, os classificadores podem se especializar em uma classe (gerando mais SVs para a mesma) e não se especializar em outras\n",
    "\n",
    "Acessados em 21/12/2016\n",
    "\n",
    "https://svds.com/learning-imbalanced-classes/\n",
    "\n",
    "http://www.cs.utah.edu/~piyush/teaching/ImbalancedLearning.pdf\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n",
    "\n",
    "Para solucionar isso, a primeira solução é \"criar\" dados das classes com menos eventos de maneira aleatória. Outras soluções podem ser propostas posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd event of ClassA is 4312\n",
      "Qtd event of ClassB is 9781\n",
      "Qtd event of ClassC is 3833\n",
      "Qtd event of ClassD is 7918\n",
      "\n",
      "Biggest class is ClassB with 9781 events\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (4312, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (9781, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (3833, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (7918, 400)\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "# unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "# Same number of events in each class\n",
    "qtd_events_biggest_class = 0\n",
    "biggest_class_label = ''\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "        qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "        biggest_class_label = class_label\n",
    "    print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "\n",
    "balanced_data = {}\n",
    "balanced_trgt = {}\n",
    "\n",
    "from Functions import DataHandler as dh\n",
    "m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if len(balanced_data) == 0:\n",
    "        class_events = all_data[all_trgt==iclass,:]\n",
    "        balanced_data = m_datahandler.CreateEventsForClass(\n",
    "            class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "        balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "    else:\n",
    "        balanced_data = np.append(balanced_data,\n",
    "                                  (m_datahandler.CreateEventsForClass(\n",
    "                    all_data[all_trgt==iclass,:],\n",
    "                    qtd_events_biggest_class-sum(all_trgt==iclass))),\n",
    "                                  axis=0)\n",
    "        balanced_trgt = np.append(balanced_trgt,\n",
    "                                  (iclass)*np.ones(qtd_events_biggest_class),axis=0)\n",
    "        \n",
    "all_data = balanced_data\n",
    "all_trgt = balanced_trgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições do treinamento\n",
    "Nessa célula temos os parâmetros do treinamento a ser realizado. No log, deve ficar armazenada a data do treinamento para a reconstrução dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "Dividing data in trn and tst for novelty class: ClassA\n",
      "Dividing data in trn and tst for novelty class: ClassB\n",
      "Dividing data in trn and tst for novelty class: ClassC\n",
      "Dividing data in trn and tst for novelty class: ClassD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/NoveltyDetection/PCDSingleClassSVM/train_info_files/2017_03_18_18_26_56_train_info.jbl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(\"NoveltyDetection\",'PCDCooperativeSingleClassSVM')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_pcds = 100\n",
    "norm = 'mapstd'\n",
    "nu_values = np.array([0.001, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "gamma_value = 0.1\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_pcds'] = n_pcds\n",
    "train_info['norm'] = norm\n",
    "train_info['nu_values'] = nu_values\n",
    "train_info['gamma_value'] = gamma_value\n",
    "\n",
    "# divide data in train and test for novelty detection\n",
    "for novelty_class, novelty_label in enumerate(class_labels):\n",
    "    print 'Dividing data in trn and tst for novelty class: %s'%(novelty_label)\n",
    "    CVO = cross_validation.StratifiedKFold(all_trgt[all_trgt!=novelty_class], n_folds)\n",
    "    CVO = list(CVO)\n",
    "    train_info['CVO_novelty_%s'%(novelty_label)] = CVO\n",
    "\n",
    "train_info['preprocessing_done'] = False\n",
    "train_info['train_done'] = False\n",
    "train_info['results_done'] = False\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+date+'_train_info.jbl'\n",
    "joblib.dump([train_info],train_info_name,compress=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'date': '2016_12_01_17_27_33', 'package': 'PCASingleClassSVM'}, 1: {'date': '2016_12_01_17_51_48', 'package': 'PCASingleClassSVM'}, 2: {'date': '2016_12_01_18_09_34', 'package': 'PCASingleClassSVM'}, 3: {'date': '2016_12_01_18_25_15', 'package': 'PCASingleClassSVM'}, 4: {'date': '2016_12_01_19_09_08', 'package': 'PCASingleClassSVM'}, 5: {'date': '2016_12_21_15_11_02', 'package': 'PCASingleClassSVM'}, 6: {'date': '2016_12_21_15_14_37', 'package': 'PCASingleClassSVM'}, 7: {'date': '2016_12_22_14_02_05', 'package': 'PCASingleClassSVM'}, 8: {'date': '2016_12_22_14_03_23', 'package': 'PCASingleClassSVM'}, 9: {'date': '2016_12_22_14_13_41', 'package': 'PCASingleClassSVM'}, 10: {'date': '2016_12_22_14_14_45', 'package': 'PCASingleClassSVM'}, 11: {'date': '2016_12_22_15_27_30', 'package': 'PCASingleClassSVM'}, 12: {'date': '2016_12_22_16_33_54', 'package': 'PCASingleClassSVM'}, 13: {'date': '2016_12_22_17_02_55', 'package': 'PCASingleClassSVM'}, 14: {'date': '2016_12_22_17_13_21', 'package': 'PCASingleClassSVM'}, 15: {'date': '2016_12_24_19_23_04', 'package': 'PCASingleClassSVM'}, 16: {'date': '2016_12_24_19_50_34', 'package': 'PCASingleClassSVM'}, 17: {'date': '2016_12_24_19_50_59', 'package': 'PCASingleClassSVM'}, 18: {'date': '2016_12_24_19_54_20', 'package': 'PCASingleClassSVM'}, 19: {'date': '2016_12_24_20_00_43', 'package': 'PCASingleClassSVM'}, 20: {'date': '2016_12_24_21_47_32', 'package': 'PCASingleClassSVM'}, 21: {'date': '2016_12_24_23_03_14', 'package': 'PCASingleClassSVM'}, 22: {'date': '2016_12_24_23_25_49', 'package': 'PCASingleClassSVM'}, 23: {'date': '2016_12_25_20_24_25', 'package': 'PCDSingleClassSVM'}, 24: {'date': '2016_12_25_20_25_18', 'package': 'PCDSingleClassSVM'}, 25: {'date': '2016_12_25_20_58_23', 'package': 'PCDSingleClassSVM'}, 26: {'date': '2017_02_09_16_44_20', 'package': 'PCASingleClassSVM'}, 27: {'date': '2017_02_11_17_32_40', 'package': 'PCASingleClassSVM'}, 28: {'date': '2017_02_11_17_41_38', 'package': 'PCASingleClassSVM'}, 29: {'date': '2017_02_11_17_42_22', 'package': 'PCASingleClassSVM'}, 30: {'date': '2017_02_11_17_46_00', 'package': 'PCDSingleClassSVM'}, 31: {'date': '2017_02_11_20_21_36', 'package': 'PCDSingleClassSVM'}, 32: {'date': '2017_02_11_20_38_27', 'package': 'PCDSingleClassSVM'}, 33: {'date': '2017_02_11_23_00_31', 'package': 'kPCASingleClassSVM'}, 34: {'date': '2017_02_11_23_00_51', 'package': 'kPCASingleClassSVM'}, 35: {'date': '2017_02_11_23_01_16', 'package': 'kPCASingleClassSVM'}, 36: {'date': '2017_02_11_23_54_50', 'package': 'kPCASingleClassSVM'}, 37: {'date': '2017_02_12_03_31_51', 'package': 'kPCASingleClassSVM'}, 38: {'date': '2017_02_12_03_33_35', 'package': 'PCDSingleClassSVM'}, 39: {'date': '2017_02_16_15_20_38', 'package': 'NLPCASingleClassSVM'}, 40: {'date': '2017_02_16_15_20_45', 'package': 'NLPCASingleClassSVM'}, 41: {'date': '2017_02_16_15_20_59', 'package': 'NLPCASingleClassSVM'}, 42: {'date': '2017_02_17_16_53_57', 'package': 'NLPCASingleClassSVM'}, 43: {'date': '2017_03_17_20_48_09', 'package': 'PCASingleClassSVM'}, 44: {'date': '2017_03_17_20_48_17', 'package': 'PCASingleClassSVM'}, 45: {'date': '2017_03_17_21_16_24', 'package': 'PCDCooperativeSingleClassSVM'}, 46: {'date': '2017_03_17_21_17_20', 'package': 'PCDCooperativeSingleClassSVM'}, 47: {'date': '2017_03_17_21_20_55', 'package': 'PCDInpendentSingleClassSVM'}, 48: {'date': '2017_03_17_21_24_27', 'package': 'kPCASingleClassSVM'}, 49: {'date': '2017_03_17_21_24_47', 'package': 'kPCASingleClassSVM'}, 50: {'date': '2017_03_18_15_17_46', 'package': 'SingleClassSVM'}, 51: {'date': '2017_03_18_15_32_01', 'package': 'kPCASingleClassSVM'}, 52: {'date': '2017_03_18_18_26_56', 'package': 'PCDCooperativeSingleClassSVM'}}\n"
     ]
    }
   ],
   "source": [
    "# Read log files\n",
    "from Functions import LogFunctions as log\n",
    "mlog = log.LogInformation()\n",
    "log_entries = mlog.RecoverLogEntries(package_name=\"NoveltyDetection\")\n",
    "print log_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing train performed in 2017_03_18_18_26_56 and for PCDCooperativeSingleClassSVM analysis\n",
      "PCDIndependentSingleClassSVM Train Info File\n",
      "Date: 2017_03_18_18_26_56\n",
      "Number of Folds: 2\n",
      "Number of Used PCDs: 100\n",
      "Normalization Method: mapstd\n",
      "Gamma Value: 0.100\n",
      "Nu Value(s): \n",
      "[ 0.001  0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9  ]\n",
      "Preprocessing Done: False\n",
      "Train Done: False\n",
      "Extract Results: False\n"
     ]
    }
   ],
   "source": [
    "# Read Information of Train Info File\n",
    "choose_date = '2017_03_18_18_26_56'\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCDCooperativeSingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'Analysing train performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    print 'PCDIndependentSingleClassSVM Train Info File'\n",
    "    print 'Date: %s'%(choose_date)\n",
    "    print 'Number of Folds: %i'%(train_info['n_folds'])\n",
    "    print 'Number of Used PCDs: %i'%(train_info['n_pcds'])\n",
    "    print 'Normalization Method: %s'%(train_info['norm'])\n",
    "    print 'Gamma Value: %1.3f'%(train_info['gamma_value'])\n",
    "    print 'Nu Value(s): '\n",
    "    print train_info['nu_values']\n",
    "    if train_info['preprocessing_done']:\n",
    "        print 'Preprocessing Done: True'\n",
    "    else:\n",
    "        print 'Preprocessing Done: False'\n",
    "    if train_info['train_done']:\n",
    "        print 'Train Done: True'\n",
    "    else:\n",
    "        print 'Train Done: False'\n",
    "    if train_info['results_done']:\n",
    "        print 'Extract Results: True'\n",
    "    else:\n",
    "        print 'Extract Results: False'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessamento - PCD\n",
    "Como a dimensionalidade dos dados é alta (400 dimensões), um pré-processamento se faz necessário para reduzir as dimensões das entradas e tornar o modelo menos complexo. Aqui, o pré-processamento utilizado é a PCD (análise de componentes principais de discriminação). Existem alguns tipos de extração de PCDs. Cada extração visa uma característica diferente (uma visa a extração de componentes necessariamente ortogonais entre si, por exemplo). \n",
    "\n",
    "## PCD por Deflação\n",
    "definir\n",
    "\n",
    "## PCD por Cooperativa\n",
    "definir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 4.05 µs\n",
      "PCA extraction performed in 2017_03_18_18_26_56 and for PCDCooperativeSingleClassSVM analysis\n",
      "Extracting PCD: Novelty Class: ClassA, 1 of 2 folds\n",
      "PCD Cooperative fit function\n",
      "PCD Cooperative Model Struct: 400 - 1 - 3\n",
      "Training 1 PCD of 100 PCDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natmourajr/.virtualenvs/sonarcessy/lib/python2.7/site-packages/keras/utils/np_utils.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  Y = np.zeros((len(y), nb_classes))\n",
      "/home/natmourajr/.virtualenvs/sonarcessy/lib/python2.7/site-packages/keras/utils/np_utils.py:16: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  Y[i, y[i]] = 1.\n"
     ]
    }
   ],
   "source": [
    "# PCD extraction\n",
    "%time\n",
    "\n",
    "from Functions import PreProcessing as preproc\n",
    "from keras.utils import np_utils\n",
    "\n",
    "pcds = {}\n",
    "pcd_objs = {}\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCDCooperativeSingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA extraction performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if train_info['preprocessing_done']:\n",
    "        print 'Preprocessing done, just analyse it'\n",
    "        continue\n",
    "    \n",
    "    trn_params = preproc.TrnParams(learning_rate= 0.005,verbose=True,\n",
    "                                   train_verbose=False)\n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        pcds[novelty_class] = {}\n",
    "        pcd_objs[novelty_class] = {}\n",
    "        \n",
    "        known_data = all_data[all_trgt!=novelty_class,:]\n",
    "        known_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "        \n",
    "        # to put 3 dimensions in output\n",
    "        known_trgt[known_trgt > novelty_class] = known_trgt[known_trgt > novelty_class]-1\n",
    "        \n",
    "        known_trgt_sparse = np_utils.to_categorical(known_trgt)\n",
    "        \n",
    "        for ifold in range(len(train_info['CVO_novelty_%s'%(novelty_label)])):\n",
    "            \n",
    "            print 'Extracting PCD: Novelty Class: %s, %i of %i folds'%(\n",
    "                    novelty_label, ifold+1, len(train_info['CVO_novelty_%s'%(novelty_label)])\n",
    "                )\n",
    "            # split data in trn set, tst set\n",
    "            train_id, test_id = train_info['CVO_novelty_%s'%(novelty_label)][ifold]\n",
    "\n",
    "            # normalize data based in train set\n",
    "            if train_info['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "                \n",
    "            norm_known_data = scaler.transform(known_data)\n",
    "            \n",
    "            #pcd = preproc.PCDIndependent(n_components=train_info['n_pcds'])\n",
    "            pcd = preproc.PCDCooperative(n_components=train_info['n_pcds'])\n",
    "            \n",
    "            pcd.fit(norm_known_data, known_trgt_sparse, \n",
    "                    train_id, test_id, trn_params=trn_params)\n",
    "\n",
    "            pcd_objs[novelty_class][ifold] = pcd\n",
    "            pcds[novelty_class][ifold] = pcd.pcds\n",
    "\n",
    "    print 'Extraction done'\n",
    "    # saving file\n",
    "    pcd_file_path = result_analysis_path+'/result_files'+'/'+choose_date+'_pcd_file.jbl'\n",
    "\n",
    "    if pcds != {}:\n",
    "        joblib.dump([pcds],pcd_file_path,compress=9)\n",
    "\n",
    "    train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "    train_info['preprocessing_done'] = True\n",
    "    joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29343, 3)\n",
      "(29343, 400)\n",
      "(14670,)\n",
      "(14673,)\n"
     ]
    }
   ],
   "source": [
    "print known_trgt_sparse.shape\n",
    "print norm_known_data.shape\n",
    "print train_id.shape\n",
    "print test_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/NoveltyDetection/PCDSingleClassSVM/train_info_files/2017_02_11_17_46_00_train_info.jbl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_info['preprocessing_done'] = False\n",
    "joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analise da Extração!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Treinamento do detector de novidade\n",
    "\n",
    "%time\n",
    "\n",
    "# Classifiers\n",
    "classifiers = {}\n",
    "\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCDCooperativeSingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if train_info['train_done']:\n",
    "        print 'Train done, just analyse it'\n",
    "        continue\n",
    "    \n",
    "    [pcds] = joblib.load(result_analysis_path+'/result_files'+'/'+choose_date+'_pcd_file.jbl')\n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        classifiers[novelty_class] = {}\n",
    "        \n",
    "        known_data = all_data[all_trgt!=novelty_class,:]\n",
    "        known_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "        \n",
    "        # for: folds\n",
    "        for ifold in range(len(train_info['CVO_novelty_%s'%(novelty_label)])):\n",
    "            classifiers[novelty_class][ifold] = {}\n",
    "            \n",
    "            # split data in trn set, tst set\n",
    "            train_id, test_id = train_info['CVO_novelty_%s'%(novelty_label)][ifold]\n",
    "            \n",
    "            # normalize data based in train set\n",
    "            if train_info['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "        \n",
    "            norm_known_data = scaler.transform(known_data)\n",
    "            \n",
    "            mat_pcd = []\n",
    "            for ipcd in range(len(pcds[novelty_class][ifold])):\n",
    "                if ipcd==0:\n",
    "                    mat_pcd = pcds[novelty_class][ifold][ipcd]\n",
    "                else:\n",
    "                    mat_pcd = np.append(mat_pcd,pcds[novelty_class][ifold][ipcd],axis=1)\n",
    "                    \n",
    "            pcd_norm_known_data = ((np.inner(mat_pcd.T,norm_known_data).T)/\n",
    "                                   np.linalg.norm(mat_pcd))\n",
    "            \n",
    "            for nu_id, nu_value in enumerate(train_info['nu_values']):\n",
    "                classifiers[novelty_class][ifold][nu_value] = {}\n",
    "                print 'Training Classifiers: Novelty Class: %s, %i of %i folds, %i of %i nu_values'%(\n",
    "                    novelty_label, ifold+1, len(train_info['CVO_novelty_%s'%(novelty_label)]),\n",
    "                    nu_id+1, len(train_info['nu_values'])\n",
    "                )\n",
    "                \n",
    "                # novelty detector\n",
    "                novelty_detector = svm.OneClassSVM(nu=nu_value, \n",
    "                                                   kernel=\"rbf\", \n",
    "                                                   gamma=train_info['gamma_value'])\n",
    "                novelty_detector.fit(pcd_norm_known_data[train_id,:])\n",
    "                \n",
    "                classifiers[novelty_class][ifold][nu_value]['NoveltyDetector'] = novelty_detector\n",
    "                \n",
    "                # class specialist\n",
    "                for known_class, known_label in enumerate(class_labels):\n",
    "                    if known_class == novelty_class: continue\n",
    "                    class_idx = np.nonzero(known_trgt == known_class)[0]\n",
    "                    idx = np.intersect1d(class_idx,train_id)\n",
    "                    classifier = svm.OneClassSVM(nu=nu_value, kernel=\"rbf\", \n",
    "                                                 gamma=train_info['gamma_value'])\n",
    "                    classifier.fit(pcd_norm_known_data[idx,:])\n",
    "                    classifiers[novelty_class][ifold][nu_value][known_label] = classifier\n",
    "\n",
    "    classifier_info_name = result_analysis_path+'/classifiers_files'+'/'+choose_date+'_classifiers.jbl'\n",
    "    joblib.dump([classifiers],classifier_info_name,compress=9)\n",
    "\n",
    "    train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "    train_info['train_done'] = True\n",
    "    joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extração dos resultados\n",
    "%time\n",
    "\n",
    "import scipy.stats as sp_stats\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCDCooperativeSingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'PCA analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # checking preprocessing\n",
    "    if not train_info['train_done']:\n",
    "        print 'Extract Preprocessing!!!'\n",
    "        continue\n",
    "        \n",
    "    # checking training\n",
    "    if not train_info['train_done']:\n",
    "        print 'Perform Train!!!'\n",
    "        continue\n",
    "        \n",
    "    # checking results extraction\n",
    "    if train_info['results_done']:\n",
    "        print 'Extraction done, just analyse it'\n",
    "        continue\n",
    "        \n",
    "    [pcds] = joblib.load(result_analysis_path+'/result_files'+'/'+choose_date+'_pcd_file.jbl')\n",
    "    \n",
    "    classifier_info_name = result_analysis_path+'/classifiers_files'+'/'+choose_date+'_classifiers.jbl'\n",
    "    [classifiers] = joblib.load(classifier_info_name)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        results[novelty_class] = {}\n",
    "        \n",
    "        # Class Specialist Efficiency\n",
    "        for known_class, known_label in enumerate(class_labels):\n",
    "            if known_class == novelty_class: continue\n",
    "            results[novelty_class][known_label] = np.zeros(\n",
    "                [len(train_info['nu_values']), \n",
    "                 len(train_info['CVO_novelty_%s'%(novelty_label)])])\n",
    "        \n",
    "        # Accuracy\n",
    "        results[novelty_class]['Accuracy'] = np.zeros(\n",
    "            [len(train_info['nu_values']), \n",
    "             len(train_info['CVO_novelty_%s'%(novelty_label)])]) \n",
    "        \n",
    "        # Known SP\n",
    "        results[novelty_class]['Known SP'] = np.zeros(\n",
    "            [len(train_info['nu_values']), \n",
    "             len(train_info['CVO_novelty_%s'%(novelty_label)])])\n",
    "        \n",
    "        # Novelty Detection\n",
    "        results[novelty_class]['Novelty Detection'] = np.zeros(\n",
    "            [len(train_info['nu_values']), \n",
    "             len(train_info['CVO_novelty_%s'%(novelty_label)])])\n",
    "        \n",
    "        # Trigger\n",
    "        results[novelty_class]['Trigger'] = np.zeros(\n",
    "            [len(train_info['nu_values']), \n",
    "             len(train_info['CVO_novelty_%s'%(novelty_label)])])\n",
    "       \n",
    "        # for: folds\n",
    "        for ifold in range(len(train_info['CVO_novelty_%s'%(novelty_label)])):\n",
    "            # split data in trn set, tst set\n",
    "            train_id, test_id = train_info['CVO_novelty_%s'%(novelty_label)][ifold]\n",
    "            \n",
    "            # normalize data based in train set\n",
    "            if train_info['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(known_data[train_id,:])\n",
    "            elif train_info['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(known_data[train_id,:])\n",
    "        \n",
    "            norm_all_data = scaler.transform(all_data)\n",
    "        \n",
    "            mat_pcd = []\n",
    "            for ipcd in range(len(pcds[novelty_class][ifold])):\n",
    "                if ipcd==0:\n",
    "                    mat_pcd = pcds[novelty_class][ifold][ipcd]\n",
    "                else:\n",
    "                    mat_pcd = np.append(mat_pcd,pcds[novelty_class][ifold][ipcd],axis=1)\n",
    "                    \n",
    "            pcd_norm_all_data = ((np.inner(mat_pcd.T,norm_all_data).T)/\n",
    "                                   np.linalg.norm(mat_pcd))\n",
    "            \n",
    "            for nu_id, nu_value in enumerate(train_info['nu_values']):\n",
    "                print 'Analysing Classifiers: Novelty Class: %s, %i of %i folds, %i of %i nu_values'%(\n",
    "                    novelty_label, ifold+1, len(train_info['CVO_novelty_%s'%(novelty_label)]),\n",
    "                    nu_id+1, len(train_info['nu_values'])\n",
    "                )\n",
    "                \n",
    "                # class specialist\n",
    "                for known_class, known_label in enumerate(class_labels):\n",
    "                    if known_class == novelty_class: continue\n",
    "                    output = (classifiers[novelty_class]\n",
    "                              [ifold][nu_value]\n",
    "                              [known_label].predict(pcd_norm_all_data[all_trgt==known_class]))\n",
    "                    \n",
    "                    results[novelty_class][known_label][nu_id,ifold] = (\n",
    "                    float(sum(output==1))/float(output.shape[0]))\n",
    "                \n",
    "                # accuracy\n",
    "                buff = np.zeros([len(class_labels)-1,1])\n",
    "                for known_class, known_label in enumerate(class_labels):\n",
    "                    if known_class == novelty_class: continue\n",
    "                    buff[known_class-(known_class>novelty_class)] = (\n",
    "                        results[novelty_class][known_label][nu_id,ifold])\n",
    "                results[novelty_class]['Accuracy'][nu_id,ifold] = np.mean(buff,axis=0)\n",
    "                \n",
    "                # known sp\n",
    "                results[novelty_class]['Known SP'][nu_id,ifold] = (np.sqrt(np.mean(buff,axis=0)\n",
    "                                                                           *sp_stats.gmean(buff,axis=0)))\n",
    "                \n",
    "                # novelty detection\n",
    "                output = (classifiers[novelty_class]\n",
    "                          [ifold][nu_value]\n",
    "                          ['NoveltyDetector'].predict(pcd_norm_all_data\n",
    "                                                [all_trgt==novelty_class]))\n",
    "                results[novelty_class]['Novelty Detection'][nu_id,ifold] = (\n",
    "                float(sum(output==-1))/float(output.shape[0]))\n",
    "                \n",
    "                # trigger\n",
    "                output = (classifiers[novelty_class]\n",
    "                          [ifold][nu_value]\n",
    "                          ['NoveltyDetector'].predict(pcd_norm_all_data\n",
    "                                                [all_trgt!=novelty_class]))\n",
    "                results[novelty_class]['Trigger'][nu_id,ifold] = (\n",
    "                float(sum(output==1))/float(output.shape[0]))\n",
    "                \n",
    "    result_file_path = result_analysis_path+'/result_files'+'/'+choose_date+'_results.jbl'\n",
    "    joblib.dump([results],result_file_path,compress=9)\n",
    "\n",
    "    train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "    train_info['results_done'] = True\n",
    "    joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analise dos resultados\n",
    "%time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'PCDCooperativeSingleClassSVM':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'Novelty Detection analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # checking preprocessing\n",
    "    if not train_info['train_done']:\n",
    "        print 'Extract Preprocessing!!!'\n",
    "        continue\n",
    "        \n",
    "    # checking training\n",
    "    if not train_info['train_done']:\n",
    "        print 'Perform Train!!!'\n",
    "        continue\n",
    "        \n",
    "    # checking results extraction\n",
    "    if not train_info['results_done']:\n",
    "        print 'Perform Extraction!!!'\n",
    "        continue\n",
    "        \n",
    "    [pcDs] = joblib.load(result_analysis_path+'/result_files'+'/'+choose_date+'_pcd_file.jbl')\n",
    "    \n",
    "    classifier_info_name = result_analysis_path+'/classifiers_files'+'/'+choose_date+'_classifiers.jbl'\n",
    "    [classifiers] = joblib.load(classifier_info_name)\n",
    "    \n",
    "    result_file_path = result_analysis_path+'/result_files'+'/'+choose_date+'_results.jbl'\n",
    "    [results] = joblib.load(result_file_path)\n",
    "    \n",
    "    # Plot Efficiency\n",
    "    fig, subplot_array = plt.subplots(nrows=2, ncols=2,figsize=(20,20))\n",
    "    m_colors = ['b', 'r', 'g', 'y']\n",
    "    for novelty_class, novelty_label in enumerate(class_labels):\n",
    "        ax = plt.subplot(2,2,novelty_class+1)\n",
    "        m_fontsize = 18\n",
    "        plt.title('Classifier Eff. - Novelty: '+novelty_label, fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        if novelty_class > -1:\n",
    "            plt.xlabel(r'$\\nu$ values', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        plt.ylabel('Efficiency(%)', fontsize= m_fontsize, fontweight=\"bold\")\n",
    "        m_leg = []\n",
    "        \n",
    "        line_width = 3.5\n",
    "        \n",
    "        # class specialist eff\n",
    "        for known_class, known_label in enumerate(class_labels):\n",
    "            if known_class == novelty_class: continue\n",
    "            plot_data = results[novelty_class][known_label]\n",
    "            ax.errorbar(train_info['nu_values'],\n",
    "                        100*np.mean(plot_data,axis=1),\n",
    "                        100*np.std(plot_data,axis=1),marker='o',\n",
    "                        color=m_colors[known_class],alpha=0.5,linewidth=line_width)\n",
    "            m_leg.append(known_label+' Eff.')\n",
    "        \n",
    "        # accuracy\n",
    "        plot_data = results[novelty_class]['Accuracy']\n",
    "        ax.errorbar(train_info['nu_values'],\n",
    "                    100*np.mean(plot_data,axis=1),\n",
    "                    100*np.std(plot_data,axis=1),marker='o',\n",
    "                    color='k',ls='-.',alpha=0.5, linewidth=line_width)\n",
    "        m_leg.append('Known Acc.')\n",
    "        \n",
    "        # known sp\n",
    "        plot_data = results[novelty_class]['Known SP']\n",
    "        ax.errorbar(train_info['nu_values'],\n",
    "                    100*np.mean(plot_data,axis=1),\n",
    "                    100*np.std(plot_data,axis=1),marker='o',\n",
    "                    color='k',ls='-.',alpha=0.5, linewidth=line_width)\n",
    "        m_leg.append('Known SP')\n",
    "        \n",
    "        # trigger\n",
    "        plot_data = results[novelty_class]['Trigger']\n",
    "        ax.errorbar(train_info['nu_values'],\n",
    "                    100*np.mean(plot_data,axis=1),\n",
    "                    100*np.std(plot_data,axis=1),marker='o',\n",
    "                    color='k',ls=':',alpha=0.5, linewidth=line_width)\n",
    "        m_leg.append('Trigger')\n",
    "        \n",
    "        # novelty detection\n",
    "        plot_data = results[novelty_class]['Novelty Detection']\n",
    "        ax.errorbar(train_info['nu_values'],\n",
    "                    100*np.mean(plot_data,axis=1),\n",
    "                    100*np.std(plot_data,axis=1),marker='o',\n",
    "                    color='k',ls='-',alpha=0.5, linewidth=line_width)\n",
    "        m_leg.append('Novelty Detection')\n",
    "        \n",
    "        # graphical assusts\n",
    "        ax.set_ylim([0.0, 115])\n",
    "        ax.set_yticks([x for x in range(0,101,5)])\n",
    "        \n",
    "        ax.set_xlim([np.min(train_info['nu_values']), np.max(train_info['nu_values'])])\n",
    "        ax.set_xticks(train_info['nu_values'])\n",
    "        ax.set_xticklabels(train_info['nu_values'],rotation=45)\n",
    "        \n",
    "        ax.grid()\n",
    "        ax.legend(m_leg, loc='upper right',ncol=3)\n",
    "    fig.savefig(result_analysis_path+'/picts/'+choose_date+'_'+\n",
    "                log_entries[log_id]['package']+'_novelty_detection.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
