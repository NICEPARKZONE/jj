{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de Classificação para Marinha do Brasil\n",
    "\n",
    "## Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "\n",
    "Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "Laboratório de Tecnologia Sonar\n",
    "\n",
    "Instituto de Pesquisas da Marinha - IPqM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from Functions import TrainParameters as trnparams\n",
    "from Functions import TrainFunctions\n",
    "\n",
    "import multiprocessing \n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "analysis_name = 'NeuralNetwork'\n",
    "\n",
    "# Enviroment variables\n",
    "data_path = os.getenv('OUTPUTDATAPATH')\n",
    "results_path = os.getenv('PACKAGE_NAME')\n",
    "\n",
    "# paths to export results\n",
    "base_results_path = '%s/%s'%(results_path,analysis_name)\n",
    "pict_results_path = '%s/pictures_files'%(base_results_path)\n",
    "files_results_path = '%s/output_files'%(base_results_path)\n",
    "\n",
    "# For multiprocessing purpose\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# Read data\n",
    "m_time = time.time()\n",
    "\n",
    "# Database caracteristics\n",
    "database = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "spectrum_bins_left = 400\n",
    "development_flag = False\n",
    "development_events = 100\n",
    "\n",
    "# Check if LofarData has created...\n",
    "if not os.path.exists('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                      (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left)):\n",
    "    print 'No Files in %s/%s\\n'%(data_path,database)\n",
    "else:\n",
    "    #Read lofar data\n",
    "    [data,trgt,class_labels] = joblib.load('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                                           (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left))\n",
    "\n",
    "\n",
    "    m_time = time.time()-m_time\n",
    "    print 'Time to read data file: '+str(m_time)+' seconds'\n",
    "\n",
    "    # correct format\n",
    "    all_data = data\n",
    "    all_trgt = trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))\n",
    "    \n",
    "    # Process data\n",
    "    # unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "    # Same number of events in each class\n",
    "    qtd_events_biggest_class = 0\n",
    "    biggest_class_label = ''\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "            qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "            biggest_class_label = class_label\n",
    "        print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "    print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "    balanced_data = {}\n",
    "    balanced_trgt = {}\n",
    "\n",
    "    from Functions import DataHandler as dh\n",
    "    m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if development_flag:\n",
    "            class_events = all_data[all_trgt==iclass,:]\n",
    "            if len(balanced_data) == 0:\n",
    "                balanced_data = class_events[0:development_events,:]\n",
    "                balanced_trgt = (iclass)*np.ones(development_events)\n",
    "            else:\n",
    "                balanced_data = np.append(balanced_data,\n",
    "                                          class_events[0:development_events,:], \n",
    "                                          axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,(iclass)*np.ones(development_events))\n",
    "        else:\n",
    "            if len(balanced_data) == 0:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                balanced_data = m_datahandler.CreateEventsForClass(\n",
    "                    class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "                balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "            else:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                created_events = (m_datahandler.CreateEventsForClass(all_data[all_trgt==iclass,:],\n",
    "                                                                     qtd_events_biggest_class-\n",
    "                                                                     (len(class_events))))\n",
    "                balanced_data = np.append(balanced_data,created_events,axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,\n",
    "                                          (iclass)*np.ones(created_events.shape[0]),axis=0)\n",
    "        \n",
    "    all_data = balanced_data\n",
    "    all_trgt = balanced_trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "# Load train parameters\n",
    "\n",
    "analysis_str = 'NeuralNetwork'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "os.remove(trn_params_folder)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NNNoveltyDetectionTrnParams(n_inits=1,\n",
    "                                                       hidden_activation='tanh',\n",
    "                                                       output_activation='softmax',\n",
    "                                                       n_epochs=500,\n",
    "                                                       patience=30,\n",
    "                                                       batch_size=256,\n",
    "                                                       verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NNNoveltyDetectionTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "    \n",
    "# Choose how many fold to be used in Cross Validation\n",
    "n_folds = 2\n",
    "CVO = trnparams.NoveltyDetectionFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento com Processamento Pararalelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train range of neurons\n",
    "\n",
    "# Choose neurons topology\n",
    "max_n_neurons = 60\n",
    "min_n_neurons = 0\n",
    "neurons_step = 20\n",
    "\n",
    "verbose = False\n",
    "\n",
    "# Create neurons vector to be used in multiprocessing.Pool()\n",
    "neurons_mat = range(min_n_neurons, max_n_neurons, neurons_step)\n",
    "start_time = time.time()\n",
    "for inovelty, novelty_class in enumerate(np.unique(trgt)):\n",
    "    trn_data = all_data[all_trgt!=novelty_class]\n",
    "    trn_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "\n",
    "    trn_trgt[trn_trgt>novelty_class] = trn_trgt[trn_trgt>novelty_class]-1\n",
    "    \n",
    "    if inovelty != 0:\n",
    "        print ''\n",
    "    print 'Novelty class: %i'%inovelty\n",
    "    \n",
    "    def trainNeuron(ineuron):\n",
    "        n_folds = len(CVO[inovelty])\n",
    "        for ifold in range(n_folds):\n",
    "            #print 'Neuron value: %i - fold %i'%(ineuron, ifold)\n",
    "            TrainFunctions.NNNoveltyTrainFunction(data=trn_data,\n",
    "                                                       trgt=trn_trgt,\n",
    "                                                       inovelty=inovelty,\n",
    "                                                       ifold=ifold,\n",
    "                                                       n_folds=n_folds,\n",
    "                                                       n_neurons=ineuron,\n",
    "                                                       trn_params=trn_params,\n",
    "                                                       save_path=results_path,\n",
    "                                                       verbose=verbose,\n",
    "                                                       dev=development_flag)\n",
    "    \n",
    "    \n",
    "    # Start Parallel processing\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "    \n",
    "    # To train on multiple cores sweeping the number of neurons\n",
    "    results = p.map(trainNeuron, neurons_mat)\n",
    "            \n",
    "    p.close()\n",
    "    p.join()        \n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# analysis example - novelty detection for neural network\n",
    "# threshold sweep\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'NeuralNetwork'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_novelty_detection_thr_sweep.jbl'%(results_path,analysis_str,analysis_name)\n",
    "os.remove(analysis_file_name)\n",
    "\n",
    "ineuron = 40\n",
    "\n",
    "thr_mat = np.arange(0.0,1.05,0.05)\n",
    "\n",
    "# Check if the analysis has already been performed\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "    \n",
    "    if not os.path.exists(trn_params_folder):\n",
    "        trn_params = trnparams.NNNoveltyDetectionTrnParams(n_inits=2,\n",
    "                                                           hidden_activation='softplus',\n",
    "                                                           output_activation='softmax',\n",
    "                                                           n_epochs=500,\n",
    "                                                           patience=30,\n",
    "                                                           batch_size=256,\n",
    "                                                           verbose=False)\n",
    "        trn_params.save(trn_params_folder)\n",
    "    else:\n",
    "        trn_params = trnparams.NNNoveltyDetectionTrnParams()\n",
    "        trn_params.load(trn_params_folder)\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "    n_folds = 2\n",
    "    CVO = trnparams.NoveltyDetectionFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\n",
    "    \n",
    "    n_classes = len(np.unique(all_trgt))\n",
    "    len_thr_mat = len(thr_mat)\n",
    "    \n",
    "    class_eff_mat = np.zeros([n_folds,n_classes,n_classes,len_thr_mat])\n",
    "    novelty_eff_mat = np.zeros([n_folds,n_classes,len_thr_mat])\n",
    "    known_acc_mat = np.zeros([n_folds,n_classes,len_thr_mat])\n",
    "    known_sp_mat = np.zeros([n_folds,n_classes,len_thr_mat])\n",
    "    \n",
    "    for inovelty, novelty_class in enumerate(np.unique(trgt)):\n",
    "        trn_data = all_data[all_trgt!=novelty_class]\n",
    "        trn_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "        \n",
    "        trn_trgt[trn_trgt>novelty_class] = trn_trgt[trn_trgt>novelty_class]-1\n",
    "        \n",
    "        for ifold in range(len(CVO[inovelty])):\n",
    "            print 'Novelty class: %01.0f - neuron: %i - fold %i'%(novelty_class, ineuron, ifold)\n",
    "            [classifier,trn_desc] = TrainFunctions.NNNoveltyTrainFunction(data=trn_data,\n",
    "                                                                          trgt=trn_trgt,\n",
    "                                                                          inovelty=inovelty,\n",
    "                                                                          ifold=ifold,\n",
    "                                                                          n_folds=len(CVO[inovelty]),\n",
    "                                                                          n_neurons=ineuron, \n",
    "                                                                          trn_params=trn_params,\n",
    "                                                                          save_path=results_path,\n",
    "                                                                          dev=development_flag)\n",
    "            output = classifier.predict(trn_data)\n",
    "            novelty_output = classifier.predict(all_data[all_trgt==novelty_class])\n",
    "            \n",
    "            for ithr,thr_value in enumerate(thr_mat): \n",
    "                buff = np.zeros([len(np.unique(trgt))-1])\n",
    "                for iclass, class_id in enumerate(np.unique(trgt)):\n",
    "                    if iclass == inovelty:\n",
    "                        continue\n",
    "                    class_eff_mat[ifold, inovelty, iclass, ithr] = float(sum(output[np.argmax(output,axis=1)==iclass-(iclass>inovelty),iclass-(iclass>inovelty)]>thr_value))/float(len(output))\n",
    "                    buff[iclass-(iclass>inovelty)] = float(sum(output[np.argmax(output,axis=1)==iclass-(iclass>inovelty),iclass-(iclass>inovelty)]>thr_value))/float(len(output))\n",
    "                novelty_eff_mat[ifold, inovelty, ithr] = float(sum(1-(novelty_output>thr_value).any(axis=1)))/float(len(novelty_output))\n",
    "                known_acc_mat[ifold, inovelty, ithr] = np.mean(buff,axis=0)\n",
    "                known_sp_mat[ifold, inovelty, ithr]= (np.sqrt(np.mean(buff,axis=0)\n",
    "                                                              *np.power(np.prod(buff),1./float(len(buff)))))\n",
    "    joblib.dump([class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, thr_mat],\n",
    "                analysis_file_name,compress=9)\n",
    "else:\n",
    "    [class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, thr_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.subplots(figsize=(20,15))\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "for inovelty, novelty_class in enumerate(np.unique(all_trgt)):\n",
    "    ax = plt.subplot(2,2,inovelty+1)\n",
    "    for iclass, m_class in enumerate(np.unique(all_trgt)):\n",
    "        if novelty_class == m_class:\n",
    "            #a = 0\n",
    "            ax.errorbar(thr_mat,np.mean(novelty_eff_mat[:,int(novelty_class),:],axis=0),\n",
    "                        np.std(novelty_eff_mat[:,int(novelty_class),:],axis=0),fmt='o-',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Novelty. Det.')\n",
    "            ax.errorbar(thr_mat,np.mean(known_acc_mat[:,int(novelty_class),:],axis=0),\n",
    "                        np.std(known_acc_mat[:,int(novelty_class),:],axis=0),fmt='o--',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Known Acc.')\n",
    "            ax.errorbar(thr_mat,np.mean(known_sp_mat[:,int(novelty_class),:],axis=0),\n",
    "                        np.std(known_sp_mat[:,int(novelty_class),:],axis=0),fmt='o:',\n",
    "                        color='k',alpha=0.7,linewidth=2.5,\n",
    "                        label='Known SP')\n",
    "        else:\n",
    "            ax.errorbar(thr_mat,np.mean(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),\n",
    "                        np.std(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),fmt='o-',\n",
    "                        color=m_colors[int(m_class)],alpha=0.7,linewidth=2.5,\n",
    "                       label='C%i Eff.'%(int(m_class)+1))\n",
    "    ax.set_xticks(thr_mat)\n",
    "    ax.set_xticklabels(thr_mat,rotation=45)\n",
    "    ax.set_title('Eff per Known Class',fontsize=18,weight='bold')\n",
    "    ax.set_xlim([np.min(thr_mat), np.max(thr_mat)])\n",
    "    ax.set_ylim([-0.5,1.2])\n",
    "    ax.grid()\n",
    "    \n",
    "    if inovelty > 1:\n",
    "        ax.set_xlabel('Threshold',fontsize=18,weight='bold')\n",
    "    if inovelty == 0 or inovelty == 2:\n",
    "        ax.set_ylabel('Eff.',fontsize=18,weight='bold')\n",
    "        \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # sort both labels and handles by labels\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "    ax.legend(handles, labels, ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
